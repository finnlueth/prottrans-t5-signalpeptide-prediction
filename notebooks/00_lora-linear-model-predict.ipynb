{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import types\n",
    "import yaml\n",
    "import gc\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import (\n",
    "    CrossEntropyLoss,\n",
    "    MSELoss\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoConfig,\n",
    "    T5EncoderModel,\n",
    "    T5Tokenizer,\n",
    "    T5PreTrainedModel,\n",
    "    T5ForConditionalGeneration,\n",
    "    pipeline,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    "    )\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    prepare_model_for_kbit_training\n",
    "    )\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import src.config as config\n",
    "\n",
    "from src.model import (\n",
    "    get_prottrans_tokenizer_model,\n",
    "    df_to_dataset,\n",
    "    inject_linear_layer,\n",
    "    compute_metrics_full,\n",
    "    compute_metrics_fast\n",
    "    )\n",
    "from src.utils import get_project_root_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = config.base_model_name\n",
    "print(\"Base Model:\\t\", base_model_name)\n",
    "print(\"MPS:\\t\\t\", torch.backends.mps.is_available())\n",
    "ROOT = get_project_root_path()\n",
    "print(\"Path:\\t\\t\", ROOT)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f\"Using device:\\t {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = config.lr\n",
    "batch_size = config.batch_size\n",
    "num_epochs = config.num_epochs\n",
    "dropout_rate = config.dropout_rate\n",
    "\n",
    "label_encoding = config.label_encoding\n",
    "label_list = config.label_decoding\n",
    "\n",
    "compute_metrics = compute_metrics_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Tokenizer and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture = T5EncoderModel\n",
    "t5_tokenizer, t5_base_model = get_prottrans_tokenizer_model(base_model_name, model_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_location = '/models/linear_model_v3'\n",
    "t5_lora_model_config = PeftConfig.from_pretrained(ROOT + adapter_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_base_model = PeftModel.from_pretrained(\n",
    "    model=t5_base_model,\n",
    "    model_id=ROOT+adapter_location,\n",
    "    is_trainable=False,\n",
    "    )\n",
    "# del t5_base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model = inject_linear_layer(\n",
    "    t5_lora_model=t5_base_model,\n",
    "    num_labels=config.label_decoding.__len__(),\n",
    "    dropout_rate=config.dropout_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_lora_model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0195,  0.0400, -0.0119,  ...,  0.0237, -0.0318, -0.0082],\n",
       "        [-0.0312, -0.0007,  0.0204,  ..., -0.0371, -0.0603,  0.0122],\n",
       "        [-0.0065, -0.0192,  0.0001,  ..., -0.0070, -0.0275, -0.0091],\n",
       "        ...,\n",
       "        [-0.0391, -0.0258,  0.0162,  ..., -0.0222, -0.0073, -0.0247],\n",
       "        [ 0.0190, -0.0202,  0.0442,  ...,  0.0069, -0.0004, -0.0178],\n",
       "        [ 0.0224, -0.0318, -0.0377,  ..., -0.0311,  0.0143,  0.0004]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_lora_model.encoder.block[4].layer[0].SelfAttention.v.lora_A.default.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New parameter model.shared.weight | 131072 parameters\n",
      "New parameter model.encoder.block.0.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.0.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.0.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.0.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.0.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.0.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.0.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.0.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight |  1024 parameters\n",
      "New parameter model.encoder.block.0.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.0.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.0.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.0.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.1.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.1.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.1.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.1.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.1.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.1.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.1.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.1.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.1.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.1.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.1.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.1.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.2.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.2.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.2.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.2.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.2.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.2.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.2.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.2.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.2.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.2.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.2.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.2.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.3.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.3.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.3.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.3.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.3.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.3.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.3.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.3.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.3.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.3.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.3.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.3.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.4.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.4.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.4.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.4.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.4.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.4.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.4.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.4.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.4.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.4.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.4.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.4.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.5.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.5.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.5.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.5.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.5.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.5.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.5.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.5.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.5.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.5.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.5.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.5.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.6.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.6.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.6.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.6.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.6.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.6.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.6.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.6.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.6.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.6.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.6.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.6.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.7.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.7.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.7.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.7.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.7.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.7.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.7.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.7.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.7.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.7.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.7.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.7.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.8.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.8.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.8.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.8.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.8.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.8.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.8.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.8.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.8.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.8.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.8.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.8.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.8.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.8.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.9.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.9.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.9.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.9.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.9.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.9.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.9.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.9.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.9.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.9.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.9.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.9.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.9.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.9.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.10.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.10.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.10.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.10.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.10.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.10.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.10.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.10.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.10.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.10.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.10.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.10.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.10.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.10.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.11.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.11.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.11.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.11.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.11.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.11.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.11.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.11.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.11.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.11.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.11.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.11.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.11.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.11.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.12.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.12.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.12.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.12.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.12.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.12.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.12.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.12.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.12.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.12.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.12.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.12.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.12.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.12.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.12.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.12.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.13.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.13.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.13.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.13.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.13.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.13.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.13.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.13.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.13.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.13.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.13.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.13.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.13.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.13.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.13.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.13.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.14.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.14.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.14.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.14.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.14.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.14.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.14.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.14.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.14.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.14.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.14.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.14.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.14.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.14.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.14.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.14.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.15.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.15.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.15.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.15.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.15.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.15.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.15.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.15.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.15.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.15.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.15.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.15.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.15.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.15.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.15.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.15.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.16.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.16.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.16.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.16.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.16.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.16.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.16.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.16.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.16.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.16.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.16.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.16.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.16.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.16.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.16.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.16.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.17.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.17.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.17.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.17.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.17.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.17.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.17.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.17.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.17.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.17.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.17.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.17.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.17.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.17.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.17.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.17.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.18.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.18.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.18.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.18.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.18.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.18.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.18.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.18.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.18.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.18.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.18.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.18.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.18.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.18.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.18.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.18.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.19.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.19.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.19.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.19.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.19.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.19.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.19.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.19.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.19.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.19.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.19.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.19.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.19.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.19.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.19.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.19.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.20.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.20.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.20.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.20.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.20.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.20.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.20.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.20.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.20.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.20.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.20.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.20.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.20.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.20.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.20.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.20.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.21.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.21.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.21.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.21.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.21.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.21.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.21.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.21.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.21.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.21.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.21.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.21.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.21.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.21.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.21.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.21.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.22.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.22.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.22.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.22.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.22.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.22.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.22.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.22.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.22.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.22.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.22.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.22.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.22.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.22.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.22.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.22.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.23.layer.0.SelfAttention.q.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.23.layer.0.SelfAttention.q.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.23.layer.0.SelfAttention.q.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.23.layer.0.SelfAttention.k.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.23.layer.0.SelfAttention.k.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.23.layer.0.SelfAttention.k.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.23.layer.0.SelfAttention.v.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.23.layer.0.SelfAttention.v.lora_A.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.23.layer.0.SelfAttention.v.lora_B.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.23.layer.0.SelfAttention.o.weight | 4194304 parameters\n",
      "New parameter model.encoder.block.23.layer.0.SelfAttention.o.lora_A.default.weight | 32768 parameters | updated\n",
      "New parameter model.encoder.block.23.layer.0.SelfAttention.o.lora_B.default.weight |  8192 parameters | updated\n",
      "New parameter model.encoder.block.23.layer.0.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.block.23.layer.1.DenseReluDense.wi.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.23.layer.1.DenseReluDense.wo.weight | 16777216 parameters\n",
      "New parameter model.encoder.block.23.layer.1.layer_norm.weight |  1024 parameters\n",
      "New parameter model.encoder.final_layer_norm.weight |  1024 parameters\n"
     ]
    }
   ],
   "source": [
    " for name, param in t5_lora_model.base_model.named_parameters():\n",
    "    if \"lora\" not in name:\n",
    "        print(f\"New parameter {name:<13} | {param.numel():>5} parameters\")\n",
    "        continue\n",
    "    if param.isnan().any():\n",
    "        print(f\"New parameter {name:<13} | {param.numel():>5} parameters | not updated\")\n",
    "    else:\n",
    "        print(f\"New parameter {name:<13} | {param.numel():>5} parameters | updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_parquet(ROOT + '/data/processed/5.0_train.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Use entire test set\n",
    "ds_test = df_data[df_data.Split.isin([4])].head(config.dataset_size)\n",
    "ds_test = df_to_dataset(\n",
    "    t5_tokenizer,\n",
    "    ds_test.Sequence.to_list(),\n",
    "    ds_test.Label.to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*ds_test['input_ids'][0])\n",
    "print(*ds_test['attention_mask'][0])\n",
    "print(*ds_test['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = t5_tokenizer.decode(ds_test['input_ids'][0][:-1])\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = t5_tokenizer(input_str)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inid = torch.tensor(ds_test['input_ids']).to(device)\n",
    "print(inid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "results = []\n",
    "for index, _ in enumerate(inid):\n",
    "    if index == 10:\n",
    "        break\n",
    "    if index % 100 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "    results += t5_lora_model(input_ids=inid[index:index+1]).logits#.argmax(dim=-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = [[config.label_decoding[y] for y in x] for x in ds_test['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "for index, item in enumerate(results):\n",
    "    truth = ground_truth[index]\n",
    "    prediction = [config.label_decoding[x] for x in item[:len(ground_truth[index])]]\n",
    "    \n",
    "    # if index % 50 == 0:\n",
    "    print('T: ', *truth, sep='')\n",
    "    print('P: ', *prediction, sep='')\n",
    "    print()\n",
    "    \n",
    "    for t, p in zip(truth, prediction):\n",
    "        if t == p:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"Correct\", correct)\n",
    "print(\"Incorrect\", incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correct/(correct+incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Measure Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "## Save Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q02742|EUKARYA|NO_SP|4\\\n",
    "MLRTLLRRRLFSYPTKYYFMVLVLSLITFSVLRIHQKPEFVSVRHLELAGENPSSDINCTKVLQGDVNEI\\\n",
    "IIIIIIIIIMMMMMMMMMMMMMMMMMMMMMMMOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n",
    "\n",
    "Q9NQR9|EUKARYA|NO_SP|4\\\n",
    "MDFLHRNGVLIIQHLQKDYRAYYTFLNFMSNVGDPRNIFFIYFPLCFQFNQTVGTKMIWVAVIGDWLNLI\\\n",
    "OOOOOOOOOOOOOOOOOOOOOOOOMMMMMMMMMMMMMMMMMMMMMIIIIIIIIIIIMMMMMMMMMMMMMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "t5_lora_model.to(device)\n",
    "\n",
    "test_set = ds_test.with_format(\"torch\", device=device)\n",
    "\n",
    "# For token classification we need a data collator here to pad correctly\n",
    "data_collator = DataCollatorForTokenClassification(t5_tokenizer) \n",
    "\n",
    "# Create a dataloader for the test dataset\n",
    "test_dataloader = DataLoader(test_set, batch_size=16, shuffle = False, collate_fn = data_collator)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "t5_lora_model.eval()\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = []\n",
    "# We need to collect the batch[\"labels\"] as well, this allows us to filter out all positions with a -100 afterwards\n",
    "padded_labels = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        print(counter)\n",
    "        counter += 1\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        padded_labels += batch['labels'].tolist()\n",
    "        # Add batch results(logits) to predictions, we take the argmax here to get the predicted class\n",
    "        prediction = t5_lora_model(input_ids=input_ids).logits.argmax(dim=-1).tolist()\n",
    "        print(prediction)\n",
    "        predictions += prediction#.argmax(dim=-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_item = 0\n",
    "\n",
    "actual = [config.label_decoding[x] for x in test_set['labels'][index_item].tolist()]\n",
    "print(actual.__len__())\n",
    "print(*actual)\n",
    "pred = [config.label_decoding[x] for x in predictions[index_item]]\n",
    "print(pred.__len__())\n",
    "print(*pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
