{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/.venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import types\n",
    "import yaml\n",
    "import gc\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import (\n",
    "    CrossEntropyLoss,\n",
    "    MSELoss\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoConfig,\n",
    "    T5EncoderModel,\n",
    "    T5Tokenizer,\n",
    "    T5PreTrainedModel,\n",
    "    T5ForConditionalGeneration,\n",
    "    pipeline,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    "    )\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    prepare_model_for_kbit_training\n",
    "    )\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import src.config as config\n",
    "\n",
    "from src.model import (\n",
    "    get_prottrans_tokenizer_model,\n",
    "    df_to_dataset,\n",
    "    inject_linear_layer,\n",
    "    compute_metrics_full,\n",
    "    compute_metrics_fast\n",
    "    )\n",
    "from src.utils import get_project_root_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model:\t Rostlab/prot_t5_xl_uniref50\n",
      "MPS:\t\t True\n",
      "Path:\t\t /Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction\n",
      "Using device:\t mps\n"
     ]
    }
   ],
   "source": [
    "base_model_name = config.base_model_name\n",
    "print(\"Base Model:\\t\", base_model_name)\n",
    "print(\"MPS:\\t\\t\", torch.backends.mps.is_available())\n",
    "ROOT = get_project_root_path()\n",
    "print(\"Path:\\t\\t\", ROOT)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f\"Using device:\\t {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = config.lr\n",
    "batch_size = config.batch_size\n",
    "num_epochs = config.num_epochs\n",
    "dropout_rate = config.dropout_rate\n",
    "\n",
    "label_encoding = config.label_encoding\n",
    "label_list = config.label_decoding\n",
    "\n",
    "compute_metrics = compute_metrics_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Tokenizer and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture = T5EncoderModel\n",
    "t5_tokenizer, t5_base_model = get_prottrans_tokenizer_model(base_model_name, model_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_location = '/models/linear_model_v3'\n",
    "t5_lora_model_config = PeftConfig.from_pretrained(ROOT + adapter_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_base_model = PeftModel.from_pretrained(\n",
    "    model=t5_base_model,\n",
    "    model_id=ROOT+adapter_location,\n",
    "    is_trainable=False,\n",
    "    )\n",
    "# del t5_base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model = inject_linear_layer(\n",
    "    t5_lora_model=t5_base_model,\n",
    "    num_labels=config.label_decoding.__len__(),\n",
    "    dropout_rate=config.dropout_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method injected_forward of PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): T5EncoderModel(\n",
       "      (shared): Embedding(128, 1024)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(128, 1024)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (v): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(\n",
       "                    in_features=4096, out_features=1024, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (relative_attention_bias): Embedding(32, 32)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-23): 23 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (v): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(\n",
       "                    in_features=4096, out_features=1024, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=6, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_lora_model.forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_parquet(ROOT + '/data/processed/5.0_train.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M A P T L F Q K L F S K R T G L G A P G R D A ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M D F T S L E T T T F E E V V I A L G S N V G ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M D D I S G R Q T L P R I N R L L E H V G N P ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M L G T V K M E G H E T S D W N S Y Y A D T Q ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M L G A V K M E G H E P S D W S S Y Y A E P E ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence  \\\n",
       "0  M A P T L F Q K L F S K R T G L G A P G R D A ...   \n",
       "1  M D F T S L E T T T F E E V V I A L G S N V G ...   \n",
       "2  M D D I S G R Q T L P R I N R L L E H V G N P ...   \n",
       "3  M L G T V K M E G H E T S D W N S Y Y A D T Q ...   \n",
       "4  M L G A V K M E G H E P S D W S S Y Y A E P E ...   \n",
       "\n",
       "                                               Label  Split  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      4  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      4  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/.venv/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# ToDo: Use entire test set\n",
    "ds_test = df_data[df_data.Split.isin([4])].head(config.dataset_size)\n",
    "ds_test = df_to_dataset(\n",
    "    t5_tokenizer,\n",
    "    ds_test.Sequence.to_list(),\n",
    "    ds_test.Label.to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 4 5 11 6 14 19 9 5 20 9 11 7 10 21 17 7 18 18 3 10 11 16 9 3 18 7 7 6 13 6 7 17 19 17 7 5 4 5 7 19 17 7 19 17 11 18 19 11 19 17 11 19 11 11 7 5 17 19 11 13 3 7 15 17 19 7 18 3 17 1\n",
      "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "print(*ds_test['input_ids'][0])\n",
    "print(*ds_test['attention_mask'][0])\n",
    "print(*ds_test['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M L G T V K M E G H E T S D W N S Y Y A D T Q E A Y S S V P V S N M N S G L G S M N S M N T Y M T M N T M T T S G N M T P A S F N M S Y A N\n"
     ]
    }
   ],
   "source": [
    "input_str = t5_tokenizer.decode(ds_test['input_ids'][0][:-1])\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [19, 4, 5, 11, 6, 14, 19, 9, 5, 20, 9, 11, 7, 10, 21, 17, 7, 18, 18, 3, 10, 11, 16, 9, 3, 18, 7, 7, 6, 13, 6, 7, 17, 19, 17, 7, 5, 4, 5, 7, 19, 17, 7, 19, 17, 11, 18, 19, 11, 19, 17, 11, 19, 11, 11, 7, 5, 17, 19, 11, 13, 3, 7, 15, 17, 19, 7, 18, 3, 17, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "inputs = t5_tokenizer(input_str)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 71])\n"
     ]
    }
   ],
   "source": [
    "inid = torch.tensor(ds_test['input_ids']).to(device)\n",
    "print(inid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): T5EncoderModel(\n",
       "      (shared): Embedding(128, 1024)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(128, 1024)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (v): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(\n",
       "                    in_features=4096, out_features=1024, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (relative_attention_bias): Embedding(32, 32)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-23): 23 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (v): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(\n",
       "                    in_features=4096, out_features=1024, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_lora_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n",
      "True\n",
      "\n",
      "<class 'peft.peft_model.PeftModel'>\n",
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): T5EncoderModel(\n",
      "      (shared): Embedding(128, 1024)\n",
      "      (encoder): T5Stack(\n",
      "        (embed_tokens): Embedding(128, 1024)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (v): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear(\n",
      "                    in_features=4096, out_features=1024, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (relative_attention_bias): Embedding(32, 32)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-23): 23 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (v): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear(\n",
      "                    in_features=4096, out_features=1024, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=1024, out_features=6, bias=True)\n",
      ")\n",
      "Linear(in_features=1024, out_features=6, bias=True)\n",
      "\n",
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions'>\n",
      "\n",
      "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[ 0.3986,  0.4020,  0.1518,  ..., -0.6233, -0.0546,  0.1950],\n",
      "         [ 0.4440,  0.4131,  0.3412,  ..., -0.6688,  0.0080,  0.2285],\n",
      "         [ 0.3544,  0.3966,  0.4409,  ..., -0.5744,  0.1129,  0.2647],\n",
      "         ...,\n",
      "         [ 0.2416,  0.3098,  0.5643,  ..., -0.6341,  0.0972,  0.2391],\n",
      "         [ 0.2725,  0.2376,  0.4465,  ..., -0.7689, -0.1138,  0.0649],\n",
      "         [-0.0215,  0.0928, -0.1247,  ..., -0.1639, -0.1693,  0.2096]]],\n",
      "       device='mps:0'), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n",
      "\n",
      "torch.Size([1, 71, 1024])\n",
      "\n",
      "tensor([[[ 0.3986,  0.4020,  0.1518,  ..., -0.6233, -0.0546,  0.1950],\n",
      "         [ 0.4440,  0.4131,  0.3412,  ..., -0.6688,  0.0080,  0.2285],\n",
      "         [ 0.3544,  0.3966,  0.4409,  ..., -0.5744,  0.1129,  0.2647],\n",
      "         ...,\n",
      "         [ 0.2416,  0.3098,  0.5643,  ..., -0.6341,  0.0972,  0.2391],\n",
      "         [ 0.2725,  0.2376,  0.4465,  ..., -0.7689, -0.1138,  0.0649],\n",
      "         [-0.0215,  0.0928, -0.1247,  ..., -0.1639, -0.1693,  0.2096]]],\n",
      "       device='mps:0')\n",
      "\n",
      "torch.Size([1, 71, 1024])\n",
      "abc\n",
      "True\n",
      "\n",
      "<class 'peft.peft_model.PeftModel'>\n",
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): T5EncoderModel(\n",
      "      (shared): Embedding(128, 1024)\n",
      "      (encoder): T5Stack(\n",
      "        (embed_tokens): Embedding(128, 1024)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (v): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear(\n",
      "                    in_features=4096, out_features=1024, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (relative_attention_bias): Embedding(32, 32)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-23): 23 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (v): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear(\n",
      "                    in_features=4096, out_features=1024, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=1024, out_features=6, bias=True)\n",
      ")\n",
      "Linear(in_features=1024, out_features=6, bias=True)\n",
      "\n",
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions'>\n",
      "\n",
      "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[ 0.3867,  0.2021,  0.2374,  ..., -0.5695, -0.1318,  0.1963],\n",
      "         [ 0.3736,  0.3793,  0.5073,  ..., -0.5873, -0.2257,  0.2610],\n",
      "         [ 0.1934,  0.3257,  0.6132,  ..., -0.4410, -0.0156,  0.3933],\n",
      "         ...,\n",
      "         [ 0.2850,  0.2980,  0.8054,  ..., -0.7320, -0.2024,  0.0637],\n",
      "         [ 0.1840,  0.1985,  0.8079,  ..., -0.7358, -0.0960,  0.0746],\n",
      "         [-0.0098,  0.0717, -0.1639,  ..., -0.1672, -0.1561,  0.1760]]],\n",
      "       device='mps:0'), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n",
      "\n",
      "torch.Size([1, 71, 1024])\n",
      "\n",
      "tensor([[[ 0.3867,  0.2021,  0.2374,  ..., -0.5695, -0.1318,  0.1963],\n",
      "         [ 0.3736,  0.3793,  0.5073,  ..., -0.5873, -0.2257,  0.2610],\n",
      "         [ 0.1934,  0.3257,  0.6132,  ..., -0.4410, -0.0156,  0.3933],\n",
      "         ...,\n",
      "         [ 0.2850,  0.2980,  0.8054,  ..., -0.7320, -0.2024,  0.0637],\n",
      "         [ 0.1840,  0.1985,  0.8079,  ..., -0.7358, -0.0960,  0.0746],\n",
      "         [-0.0098,  0.0717, -0.1639,  ..., -0.1672, -0.1561,  0.1760]]],\n",
      "       device='mps:0')\n",
      "\n",
      "torch.Size([1, 71, 1024])\n",
      "abc\n",
      "True\n",
      "\n",
      "<class 'peft.peft_model.PeftModel'>\n",
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): T5EncoderModel(\n",
      "      (shared): Embedding(128, 1024)\n",
      "      (encoder): T5Stack(\n",
      "        (embed_tokens): Embedding(128, 1024)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (v): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear(\n",
      "                    in_features=4096, out_features=1024, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (relative_attention_bias): Embedding(32, 32)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-23): 23 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (v): Linear(\n",
      "                    in_features=1024, out_features=4096, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear(\n",
      "                    in_features=4096, out_features=1024, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=1024, out_features=6, bias=True)\n",
      ")\n",
      "Linear(in_features=1024, out_features=6, bias=True)\n",
      "\n",
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions'>\n",
      "\n",
      "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[ 0.5429,  0.3183,  0.6369,  ..., -0.6179,  0.2019,  0.2065],\n",
      "         [ 0.4436,  0.1924,  0.5930,  ..., -0.6083,  0.1690,  0.2073],\n",
      "         [ 0.5019,  0.2260,  0.4837,  ..., -0.5922,  0.1983,  0.1765],\n",
      "         ...,\n",
      "         [ 0.3451,  0.2010,  0.7742,  ..., -0.7121,  0.2556,  0.1545],\n",
      "         [ 0.3192,  0.1417,  0.6659,  ..., -0.6050,  0.2229,  0.4106],\n",
      "         [-0.0316,  0.0834, -0.1469,  ..., -0.1118, -0.1918,  0.2299]]],\n",
      "       device='mps:0'), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n",
      "\n",
      "torch.Size([1, 71, 1024])\n",
      "\n",
      "tensor([[[ 0.5429,  0.3183,  0.6369,  ..., -0.6179,  0.2019,  0.2065],\n",
      "         [ 0.4436,  0.1924,  0.5930,  ..., -0.6083,  0.1690,  0.2073],\n",
      "         [ 0.5019,  0.2260,  0.4837,  ..., -0.5922,  0.1983,  0.1765],\n",
      "         ...,\n",
      "         [ 0.3451,  0.2010,  0.7742,  ..., -0.7121,  0.2556,  0.1545],\n",
      "         [ 0.3192,  0.1417,  0.6659,  ..., -0.6050,  0.2229,  0.4106],\n",
      "         [-0.0316,  0.0834, -0.1469,  ..., -0.1118, -0.1918,  0.2299]]],\n",
      "       device='mps:0')\n",
      "\n",
      "torch.Size([1, 71, 1024])\n"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad():\n",
    "results = []\n",
    "for index, _ in enumerate(inid):\n",
    "    if index == 10:\n",
    "        break\n",
    "    if index % 100 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "    results += t5_lora_model(input_ids=inid[index:index+1]).logits#.argmax(dim=-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1002e-01, -1.3651e-01, -2.6129e-01, -1.2518e-02, -1.1174e-01,\n",
       "          6.1251e-02],\n",
       "        [ 1.1470e-01,  4.1029e-02, -2.5060e-01,  7.3243e-02, -2.8825e-02,\n",
       "          2.0297e-01],\n",
       "        [-7.5371e-03, -1.1698e-01, -1.8258e-01, -3.7678e-02, -4.3240e-02,\n",
       "          1.2707e-01],\n",
       "        [-9.1174e-02,  1.8264e-02, -1.9967e-01,  6.4711e-02,  2.7490e-02,\n",
       "          4.7841e-02],\n",
       "        [-1.0222e-01, -1.0567e-01, -1.1503e-01,  4.3114e-03, -1.7900e-02,\n",
       "          3.5852e-03],\n",
       "        [ 2.5795e-01, -4.5228e-01,  9.7303e-03,  1.5691e-01,  2.3870e-02,\n",
       "          1.2575e-01],\n",
       "        [-5.2036e-04, -5.6591e-02,  3.6631e-02,  1.3374e-01, -1.2680e-02,\n",
       "          1.0854e-01],\n",
       "        [-5.4624e-02, -2.2156e-02, -7.2359e-02, -3.2889e-02, -1.3086e-01,\n",
       "          2.9044e-01],\n",
       "        [ 6.1361e-03, -3.0071e-01, -7.0684e-02, -1.1380e-01, -1.8691e-01,\n",
       "          9.5446e-02],\n",
       "        [-1.1758e-01,  4.7077e-02, -2.4958e-01,  9.2215e-02, -1.1208e-01,\n",
       "          1.3471e-01],\n",
       "        [ 8.6320e-04,  1.4906e-01, -2.4398e-02, -1.3085e-01, -1.5584e-01,\n",
       "          2.9684e-01],\n",
       "        [ 7.2170e-02, -9.3930e-02, -1.0273e-02,  8.3064e-03,  1.3599e-01,\n",
       "          2.5080e-01],\n",
       "        [-1.7469e-01, -3.1111e-02,  1.3982e-02,  1.2288e-01,  6.4725e-02,\n",
       "          3.0167e-01],\n",
       "        [ 5.0911e-03, -1.5849e-01, -3.0709e-02, -2.1202e-01, -1.0949e-01,\n",
       "          1.4383e-01],\n",
       "        [-1.9612e-01, -6.1821e-02, -9.8409e-02,  4.7344e-02, -7.2484e-02,\n",
       "          3.4481e-01],\n",
       "        [-2.7999e-01, -2.0628e-01, -2.2518e-01,  9.8332e-04,  4.6152e-02,\n",
       "          3.1009e-01],\n",
       "        [-1.1137e-01, -2.4365e-02, -3.2112e-02, -1.2743e-01, -4.3272e-02,\n",
       "          1.6999e-01],\n",
       "        [-3.0659e-02, -5.6486e-03, -2.8369e-02, -3.2152e-02,  6.8018e-02,\n",
       "          2.5925e-01],\n",
       "        [-1.0293e-01, -2.3919e-01, -1.3862e-01, -5.9980e-02, -8.2664e-02,\n",
       "          2.3632e-01],\n",
       "        [-3.0353e-02, -2.0149e-01, -9.7972e-02,  6.2936e-02, -2.1748e-02,\n",
       "          1.7360e-01],\n",
       "        [-9.2499e-02, -6.6371e-02, -1.2840e-01, -2.1038e-01,  2.4192e-04,\n",
       "          1.9628e-01],\n",
       "        [-5.1296e-02, -2.7381e-01, -1.3985e-02,  1.4631e-01,  9.7373e-02,\n",
       "          1.6526e-01],\n",
       "        [-1.7644e-01, -6.4096e-02, -1.0969e-01,  2.9667e-01, -1.1486e-01,\n",
       "          2.2515e-01],\n",
       "        [-2.4445e-02,  6.5760e-02,  6.3829e-03,  8.7603e-02, -4.7975e-02,\n",
       "          2.9922e-01],\n",
       "        [ 6.1829e-02, -8.5223e-02, -1.0580e-01,  1.3216e-02, -8.8284e-02,\n",
       "          1.9350e-01],\n",
       "        [-9.0105e-03, -3.1799e-01, -1.9970e-01, -4.7230e-02,  9.4657e-02,\n",
       "          2.8203e-01],\n",
       "        [-2.1730e-02, -2.4957e-01, -2.4964e-01,  6.9047e-02,  8.8944e-02,\n",
       "          2.5387e-01],\n",
       "        [-1.5386e-01,  5.1645e-02, -1.3424e-01,  1.0150e-01,  3.0639e-04,\n",
       "          2.9245e-01],\n",
       "        [ 1.3083e-01,  1.8659e-02, -1.6612e-01, -3.3959e-02, -3.6919e-02,\n",
       "          2.9229e-01],\n",
       "        [-1.4600e-01, -1.3433e-02, -3.6655e-02,  1.0298e-01, -2.8357e-02,\n",
       "          1.7046e-01],\n",
       "        [ 9.5291e-02, -1.2094e-01, -7.4617e-02,  7.5411e-02,  5.1115e-02,\n",
       "          1.3757e-01],\n",
       "        [-1.2747e-01, -4.6255e-03, -6.5303e-02, -2.7671e-04, -2.4227e-02,\n",
       "          2.2769e-01],\n",
       "        [-7.9748e-02, -1.9093e-01, -2.1588e-01, -5.0495e-02, -2.0477e-01,\n",
       "         -1.0200e-01],\n",
       "        [-1.4948e-02, -1.4748e-01,  1.7294e-02, -6.6687e-02, -2.0161e-02,\n",
       "          1.9021e-01],\n",
       "        [-2.5380e-01, -9.8415e-02, -2.7990e-01, -2.7400e-02, -7.8144e-02,\n",
       "          2.1476e-01],\n",
       "        [-1.2636e-01, -1.3905e-01,  3.6654e-02,  2.3567e-02, -1.4044e-01,\n",
       "          2.2627e-01],\n",
       "        [-6.9030e-02, -2.2501e-01, -1.2576e-01, -1.3567e-01, -3.3147e-01,\n",
       "          7.1093e-02],\n",
       "        [ 2.7987e-01, -4.2152e-02, -2.8687e-01,  2.7302e-02, -1.5995e-01,\n",
       "          1.4794e-01],\n",
       "        [-5.7384e-02, -1.8855e-01, -2.8042e-02, -5.3466e-02, -3.5694e-01,\n",
       "          1.9730e-01],\n",
       "        [-3.7347e-02, -4.5651e-02, -1.3560e-01, -2.2708e-01, -9.2368e-02,\n",
       "          1.2934e-01],\n",
       "        [ 4.7924e-02, -1.5121e-01,  5.5771e-02,  8.0771e-02, -8.8812e-02,\n",
       "          9.5891e-03],\n",
       "        [ 4.5516e-02, -1.5170e-01, -5.1025e-02, -3.8331e-02, -2.0318e-01,\n",
       "          2.4567e-01],\n",
       "        [-2.0483e-02, -1.9137e-01, -6.8416e-02, -1.6765e-01, -1.2455e-01,\n",
       "          2.5285e-01],\n",
       "        [ 1.2672e-01, -2.7768e-01, -1.4193e-03,  2.6959e-02, -1.3104e-01,\n",
       "          1.4039e-01],\n",
       "        [-1.2720e-01, -1.0622e-01, -1.6616e-01,  4.8171e-02, -1.2894e-01,\n",
       "          2.7148e-01],\n",
       "        [-9.5036e-02, -9.9405e-02,  9.5822e-03, -9.3391e-02, -7.1425e-02,\n",
       "          5.1361e-02],\n",
       "        [-1.1326e-01, -2.7461e-01, -2.1452e-01, -2.5369e-01, -8.1278e-02,\n",
       "          1.3126e-01],\n",
       "        [-1.4180e-01, -6.1280e-02,  3.1295e-02,  4.6026e-02, -1.4106e-01,\n",
       "          1.7975e-01],\n",
       "        [ 5.1654e-02, -1.2514e-01, -5.5499e-02, -8.0687e-02, -6.9634e-03,\n",
       "          1.0253e-01],\n",
       "        [-3.2259e-02, -3.1591e-01, -1.0419e-01, -1.2466e-01, -1.5357e-01,\n",
       "          1.2188e-01],\n",
       "        [-5.3263e-02, -1.2517e-01, -1.4449e-01, -6.9557e-02, -2.3180e-01,\n",
       "          9.2787e-02],\n",
       "        [ 2.9477e-02, -2.0882e-01,  9.0146e-02, -1.0393e-01, -1.2098e-01,\n",
       "          1.7181e-01],\n",
       "        [ 4.5403e-02, -2.5732e-01,  6.9266e-02, -5.3753e-02,  9.2976e-03,\n",
       "          2.2773e-03],\n",
       "        [-1.7977e-01,  5.4548e-02,  1.6143e-02, -1.6036e-01,  3.8730e-02,\n",
       "          2.2166e-02],\n",
       "        [ 1.2090e-02, -1.6155e-01, -9.5578e-03, -7.7371e-02, -3.3799e-02,\n",
       "          1.0795e-01],\n",
       "        [ 1.6411e-02, -2.5260e-01, -1.4046e-01, -3.2303e-01, -1.9564e-01,\n",
       "          8.4634e-02],\n",
       "        [-1.5074e-01, -3.7899e-01, -7.4510e-02, -1.5832e-01, -4.6102e-02,\n",
       "          1.0906e-01],\n",
       "        [-1.4794e-01, -1.2153e-01, -1.5366e-01,  3.7817e-02, -1.5742e-01,\n",
       "          3.7730e-01],\n",
       "        [-6.6255e-02, -3.3704e-01,  5.0687e-02, -1.7957e-01,  1.0297e-01,\n",
       "          1.8685e-01],\n",
       "        [-3.3401e-02, -8.7639e-02,  1.0641e-01, -6.3225e-02, -4.5327e-02,\n",
       "          1.2929e-01],\n",
       "        [-1.3601e-01, -2.3730e-01, -2.3301e-01, -6.0315e-02, -7.1830e-02,\n",
       "          1.3355e-01],\n",
       "        [-4.0105e-02, -3.4433e-01,  6.7470e-03, -1.5548e-01, -1.1603e-01,\n",
       "          2.3947e-01],\n",
       "        [-2.2476e-01, -5.8497e-02, -5.8244e-02, -2.1812e-01, -5.4127e-02,\n",
       "          2.6170e-01],\n",
       "        [-6.4862e-02, -1.1460e-01, -2.3610e-01, -1.9944e-01, -1.2877e-02,\n",
       "          1.9549e-01],\n",
       "        [-2.0880e-01, -6.9501e-02, -1.2297e-01,  6.3446e-02, -2.5072e-02,\n",
       "          1.3879e-01],\n",
       "        [ 5.6134e-02, -6.3419e-02, -3.6803e-02, -6.1194e-02,  1.7430e-01,\n",
       "          8.2522e-02],\n",
       "        [-4.5633e-02, -4.2672e-02, -5.4635e-02, -1.0332e-01, -3.6364e-03,\n",
       "          8.2511e-02],\n",
       "        [-1.5052e-01, -1.0925e-01, -1.2809e-01, -2.1230e-01,  4.1360e-02,\n",
       "          1.4880e-01],\n",
       "        [ 7.8563e-02, -1.3492e-01, -1.7942e-02, -4.0189e-02,  6.3574e-04,\n",
       "          2.4470e-01],\n",
       "        [-1.9065e-01, -1.6644e-01, -2.6574e-01, -1.1540e-01,  4.3987e-02,\n",
       "          1.3473e-01],\n",
       "        [ 2.3041e-03,  7.0762e-03,  1.0212e-01,  3.7592e-02, -3.3510e-02,\n",
       "         -9.4928e-02]], device='mps:0', grad_fn=<UnbindBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19,  4,  5, 11,  6, 14, 19,  9,  5, 20,  9, 11,  7, 10, 21, 17,  7, 18,\n",
       "        18,  3, 10, 11, 16,  9,  3, 18,  7,  7,  6, 13,  6,  7, 17, 19, 17,  7,\n",
       "         5,  4,  5,  7, 19, 17,  7, 19, 17, 11, 18, 19, 11, 19, 17, 11, 19, 11,\n",
       "        11,  7,  5, 17, 19, 11, 13,  3,  7, 15, 17, 19,  7, 18,  3, 17,  1],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1071, -0.1378, -0.1933,  0.1902, -0.0305,  0.1059],\n",
       "        [ 0.1011,  0.0519, -0.1658,  0.0611,  0.0029,  0.1912],\n",
       "        [-0.1178, -0.2082, -0.1434,  0.0763, -0.0423,  0.1246],\n",
       "        [-0.1257, -0.0359, -0.1733, -0.0491, -0.0415, -0.0466],\n",
       "        [ 0.0497, -0.2000, -0.2202, -0.0820,  0.0453,  0.1234],\n",
       "        [ 0.2108, -0.3067,  0.0053,  0.1441, -0.1169,  0.0552],\n",
       "        [-0.1084, -0.1360,  0.0363,  0.0255,  0.0093,  0.1930],\n",
       "        [-0.0580, -0.0603, -0.1644,  0.0522, -0.2061,  0.2905],\n",
       "        [-0.1383, -0.4175, -0.1037,  0.1006, -0.0871,  0.0957],\n",
       "        [-0.1784, -0.0133, -0.2317,  0.0383, -0.1960,  0.1178],\n",
       "        [-0.0051,  0.2334,  0.0213, -0.0300, -0.0167,  0.1881],\n",
       "        [-0.0714, -0.1151, -0.2211,  0.1327,  0.1008,  0.1833],\n",
       "        [-0.1677,  0.0007, -0.0345, -0.0171, -0.1235,  0.2255],\n",
       "        [ 0.0793, -0.0709, -0.1627, -0.2155, -0.1119,  0.2311],\n",
       "        [-0.2228, -0.1180, -0.0904,  0.0278, -0.0210,  0.3222],\n",
       "        [-0.0925, -0.0783, -0.1517,  0.0034, -0.0043,  0.1244],\n",
       "        [-0.0166,  0.0368, -0.0875, -0.0824, -0.0436,  0.1699],\n",
       "        [-0.0575, -0.1344, -0.1328, -0.1913, -0.0498,  0.2197],\n",
       "        [-0.0753, -0.1192, -0.1268,  0.0085, -0.0259,  0.2329],\n",
       "        [-0.0262, -0.1349,  0.0639, -0.0073, -0.1418,  0.2652],\n",
       "        [-0.0207,  0.0186, -0.0075, -0.1606, -0.0850,  0.3267],\n",
       "        [-0.1286, -0.3447, -0.1042, -0.0616, -0.0490,  0.3087],\n",
       "        [-0.1408, -0.1712, -0.2026,  0.1792,  0.1756,  0.1939],\n",
       "        [-0.0939,  0.1042,  0.0465, -0.0974, -0.1580,  0.2679],\n",
       "        [ 0.1144, -0.1765, -0.0721,  0.0515, -0.0005,  0.1643],\n",
       "        [ 0.0016, -0.2970, -0.1908, -0.0531, -0.0181,  0.1934],\n",
       "        [-0.0908, -0.0600, -0.1354,  0.0789, -0.0487,  0.0921],\n",
       "        [ 0.1029, -0.2071, -0.0007,  0.0740,  0.1191,  0.2018],\n",
       "        [ 0.1465, -0.0448, -0.1212,  0.0521, -0.0807,  0.2725],\n",
       "        [-0.0638, -0.1861,  0.0061,  0.0890, -0.2487,  0.1842],\n",
       "        [ 0.1056, -0.2488, -0.1768, -0.0336,  0.0142,  0.1901],\n",
       "        [-0.0138, -0.0568, -0.1768,  0.0028, -0.0221,  0.2579],\n",
       "        [ 0.0193, -0.1891, -0.3875, -0.0553, -0.1422,  0.1117],\n",
       "        [-0.0675, -0.2378, -0.0618,  0.1173,  0.0240,  0.1490],\n",
       "        [-0.1262, -0.0367, -0.2236,  0.0973, -0.1230,  0.1683],\n",
       "        [ 0.0253,  0.0262, -0.0156,  0.0722,  0.0156,  0.1955],\n",
       "        [ 0.0148, -0.2236, -0.1370, -0.1188, -0.3573,  0.0786],\n",
       "        [ 0.0904, -0.0941, -0.0613, -0.0613, -0.1451,  0.3257],\n",
       "        [ 0.0383, -0.2469,  0.1212, -0.0336, -0.3519,  0.2375],\n",
       "        [ 0.0875, -0.0320, -0.0481,  0.0019, -0.1843,  0.1014],\n",
       "        [ 0.1343, -0.2653,  0.1754, -0.0213,  0.0260,  0.1111],\n",
       "        [-0.0666, -0.1552, -0.1205, -0.0046, -0.1154,  0.2490],\n",
       "        [-0.0042, -0.2067, -0.0485, -0.2047, -0.1980,  0.2096],\n",
       "        [-0.0162, -0.1929,  0.0074, -0.0018, -0.0795,  0.1612],\n",
       "        [-0.0306, -0.1917, -0.1727,  0.1053, -0.2056,  0.1925],\n",
       "        [ 0.0714, -0.1638,  0.0803, -0.2057,  0.0300, -0.0699],\n",
       "        [-0.1731, -0.1834, -0.2782, -0.1656, -0.0292,  0.1020],\n",
       "        [-0.0889, -0.0648, -0.0859,  0.0404, -0.1816,  0.3403],\n",
       "        [ 0.1493, -0.1075,  0.1876, -0.1785, -0.1780,  0.1783],\n",
       "        [-0.0856, -0.2169,  0.0230, -0.0918,  0.0054, -0.0273],\n",
       "        [-0.1056, -0.1157, -0.0750, -0.0274, -0.2353,  0.2270],\n",
       "        [ 0.0426, -0.0778, -0.0344, -0.0658, -0.0437,  0.0778],\n",
       "        [-0.0240, -0.2087,  0.0654, -0.0385, -0.0750,  0.0384],\n",
       "        [-0.0498, -0.0136, -0.1631, -0.0965, -0.0197,  0.1721],\n",
       "        [-0.0780, -0.3506, -0.0748, -0.1332, -0.1346,  0.3402],\n",
       "        [-0.0520, -0.2149, -0.0066, -0.1264, -0.1189,  0.1234],\n",
       "        [-0.1129, -0.3452, -0.0667, -0.1858, -0.0916,  0.0548],\n",
       "        [-0.0863, -0.1676, -0.2343,  0.0585, -0.0650,  0.2209],\n",
       "        [-0.0462, -0.1602, -0.0655, -0.1217,  0.0727,  0.2760],\n",
       "        [-0.0551, -0.0876,  0.0940, -0.2257,  0.0072,  0.0727],\n",
       "        [-0.2145, -0.0954, -0.1366,  0.0108, -0.2208,  0.1578],\n",
       "        [ 0.0272, -0.2047,  0.0059, -0.1149, -0.0820,  0.3712],\n",
       "        [-0.0608,  0.0408, -0.0704, -0.1451, -0.2033,  0.4077],\n",
       "        [-0.0474, -0.1361, -0.1041, -0.3393,  0.1266,  0.2225],\n",
       "        [-0.3029, -0.0501, -0.1854,  0.1867, -0.0291,  0.2859],\n",
       "        [-0.0854, -0.0450, -0.0105, -0.0618,  0.1388,  0.1346],\n",
       "        [-0.0421, -0.0214, -0.0473, -0.1902, -0.1212,  0.2484],\n",
       "        [-0.2049, -0.0464, -0.1240, -0.1616,  0.0027,  0.0785],\n",
       "        [-0.0449, -0.2095, -0.1729, -0.1197, -0.0152,  0.1894],\n",
       "        [-0.1493, -0.0659, -0.2053,  0.0692, -0.0330,  0.1050],\n",
       "        [ 0.0254,  0.0313,  0.1000,  0.0603, -0.0581, -0.1046]],\n",
       "       device='mps:0', grad_fn=<UnbindBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = [[config.label_decoding[y] for y in x] for x in ds_test['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4146\n"
     ]
    }
   ],
   "source": [
    "print(len(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T: IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n",
      "P: TLSLLTSLSLLOOLLSLLLLLLLLLLLLLLLLLIMLLLOLISLLSILIIILIISOOOLILLOLLSLLLLL\n",
      "\n",
      "T: IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n",
      "P: LLLLLTOLSSLOLOLLLLLLLSSLOLLLLTISLOLLSLSISSSISOSIILOSLLOOOSSLISOSISLLLL\n",
      "\n",
      "T: IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n",
      "P: OLLLSLOLOLOLLOLLLLLLLLTLLLISLSLLLLLLSSLLLOLLOLOOOLILMOOIIIILILLILLLOLL\n",
      "\n",
      "T: IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n",
      "P: SSSSLLLLLLLSSLLMLLLMSSMLLLSLOSLLSTLOLLSSOSLLLLSSLMOMOSOOOOMMOLOOLOLLOL\n",
      "\n",
      "T: IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n",
      "P: LMLSSLLLLLLLLLLLLLSMTILSLILLOOLOLSSLLSTLLOSLTOLTOOOSSOOOOLTSOLMOLLLLLL\n",
      "\n",
      "T: IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n",
      "P: LLLLLLSOLSLLOLLLMLLLOLLSLLLLLLTSOILLLLLLLLLLLLLSLLLIOLLLLLILLLLLLLLLLL\n",
      "\n",
      "T: IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n",
      "P: MSSSMSSLSSLLSSSSSSSSLSSLSSSISSSLSSLSSSSISSSSSSSSSSLSSSSSSSSSSILSLISLSI\n",
      "\n",
      "T: IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n",
      "P: TLSLLILLLTLLLLSLLLTSTSLSLLLLLLLLLLLLSSLLLLLLLSSLLLLLLLLOLLLLLILLLLLLLL\n",
      "\n",
      "T: IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n",
      "P: OOTLMILLLOLLMLSLMSOSSLOSSSIISOISSTSSSLOLTOSSSOOSOTSIISSLTLTSLLLIOOIOLL\n",
      "\n",
      "T: IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n",
      "P: LLSIMOLLSLLILLLILOIISLLLLILLLLILIIIILLLILLILILIIIILILLLSOLLLLLSSLIILLL\n",
      "\n",
      "Correct 65\n",
      "Incorrect 635\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "for index, item in enumerate(results):\n",
    "    truth = ground_truth[index]\n",
    "    prediction = [config.label_decoding[x] for x in item[:len(ground_truth[index])]]\n",
    "    \n",
    "    # if index % 50 == 0:\n",
    "    print('T: ', *truth, sep='')\n",
    "    print('P: ', *prediction, sep='')\n",
    "    print()\n",
    "    \n",
    "    for t, p in zip(truth, prediction):\n",
    "        if t == p:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"Correct\", correct)\n",
    "print(\"Incorrect\", incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09\n"
     ]
    }
   ],
   "source": [
    "print(correct/(correct+incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "797"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Measure Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "## Save Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q02742|EUKARYA|NO_SP|4\\\n",
    "MLRTLLRRRLFSYPTKYYFMVLVLSLITFSVLRIHQKPEFVSVRHLELAGENPSSDINCTKVLQGDVNEI\\\n",
    "IIIIIIIIIMMMMMMMMMMMMMMMMMMMMMMMOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n",
    "\n",
    "Q9NQR9|EUKARYA|NO_SP|4\\\n",
    "MDFLHRNGVLIIQHLQKDYRAYYTFLNFMSNVGDPRNIFFIYFPLCFQFNQTVGTKMIWVAVIGDWLNLI\\\n",
    "OOOOOOOOOOOOOOOOOOOOOOOOMMMMMMMMMMMMMMMMMMMMMIIIIIIIIIIIMMMMMMMMMMMMMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "t5_lora_model.to(device)\n",
    "\n",
    "test_set = ds_test.with_format(\"torch\", device=device)\n",
    "\n",
    "# For token classification we need a data collator here to pad correctly\n",
    "data_collator = DataCollatorForTokenClassification(t5_tokenizer) \n",
    "\n",
    "# Create a dataloader for the test dataset\n",
    "test_dataloader = DataLoader(test_set, batch_size=16, shuffle = False, collate_fn = data_collator)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "t5_lora_model.eval()\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = []\n",
    "# We need to collect the batch[\"labels\"] as well, this allows us to filter out all positions with a -100 afterwards\n",
    "padded_labels = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        print(counter)\n",
    "        counter += 1\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        padded_labels += batch['labels'].tolist()\n",
    "        # Add batch results(logits) to predictions, we take the argmax here to get the predicted class\n",
    "        prediction = t5_lora_model(input_ids=input_ids).logits.argmax(dim=-1).tolist()\n",
    "        print(prediction)\n",
    "        predictions += prediction#.argmax(dim=-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_item = 0\n",
    "\n",
    "actual = [config.label_decoding[x] for x in test_set['labels'][index_item].tolist()]\n",
    "print(actual.__len__())\n",
    "print(*actual)\n",
    "pred = [config.label_decoding[x] for x in predictions[index_item]]\n",
    "print(pred.__len__())\n",
    "print(*pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
