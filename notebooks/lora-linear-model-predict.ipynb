{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import types\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import (\n",
    "    CrossEntropyLoss,\n",
    "    MSELoss\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoConfig,\n",
    "    T5EncoderModel,\n",
    "    T5Tokenizer,\n",
    "    T5PreTrainedModel,\n",
    "    T5ForConditionalGeneration,\n",
    "    pipeline,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    "    )\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    prepare_model_for_kbit_training\n",
    "    )\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import src.config as config\n",
    "\n",
    "from src.model import (\n",
    "    get_prottrans_tokenizer_model,\n",
    "    df_to_dataset,\n",
    "    inject_linear_layer,\n",
    "    compute_metrics_full,\n",
    "    compute_metrics_fast\n",
    "    )\n",
    "from src.utils import get_project_root_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = config.base_model_name\n",
    "print(\"Base Model:\\t\", base_model_name)\n",
    "print(\"MPS:\\t\\t\", torch.backends.mps.is_available())\n",
    "ROOT = get_project_root_path()\n",
    "print(\"Path:\\t\\t\", ROOT)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f\"Using device:\\t {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = config.lr\n",
    "batch_size = config.batch_size\n",
    "num_epochs = config.num_epochs\n",
    "dropout_rate = config.dropout_rate\n",
    "\n",
    "label_encoding = config.label_encoding\n",
    "label_list = config.label_decoding\n",
    "\n",
    "compute_metrics = compute_metrics_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Tokenizer and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture = T5EncoderModel\n",
    "\n",
    "t5_tokenizer, t5_base_model = get_prottrans_tokenizer_model(base_model_name, model_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_lora_model_load_adapter = PeftConfig.from_pretrained(ROOT + '/models/linear_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_lora_model_load_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_location = '/models/linear_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model_config = PeftConfig.from_pretrained(ROOT + adapter_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_base_model = PeftModel.from_pretrained(\n",
    "    model=t5_base_model,\n",
    "    model_id=ROOT+adapter_location,\n",
    "    # is_trainable=False,\n",
    "    )\n",
    "# del t5_base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model = inject_linear_layer(t5_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = label_list.__len__()\n",
    "t5_lora_model.dropout = nn.Dropout(dropout_rate)\n",
    "t5_lora_model.num_labels = num_labels\n",
    "\n",
    "t5_lora_model.get_base_model().dropout = nn.Dropout(dropout_rate)\n",
    "t5_lora_model.get_base_model().classifier = nn.Linear(\n",
    "    in_features=t5_lora_model.get_base_model().config.hidden_size,\n",
    "    out_features=label_list.__len__()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Make Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_parquet(ROOT + '/data/processed/5.0_train.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Use entire test set\n",
    "ds_test = df_data[df_data.Split == 'test']\n",
    "\n",
    "ds_test = df_to_dataset(\n",
    "    t5_tokenizer,\n",
    "    ds_test.Sequence.to_list()[:3],\n",
    "    ds_test.Label.to_list()[:3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*ds_test['input_ids'][0])\n",
    "print(*ds_test['attention_mask'][0])\n",
    "print(*ds_test['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = t5_tokenizer.decode(ds_test['input_ids'][0][:-1])\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = t5_tokenizer(input_str)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     logits = t5_lora_model(inputs['input_ids']).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cpu'\n",
    "# t5_lora_model.to(device)\n",
    "\n",
    "# test_set = ds_test.with_format(\"torch\", device=device)\n",
    "\n",
    "# # For token classification we need a data collator here to pad correctly\n",
    "# data_collator = DataCollatorForTokenClassification(t5_tokenizer) \n",
    "\n",
    "# # Create a dataloader for the test dataset\n",
    "# test_dataloader = DataLoader(test_set, batch_size=16, shuffle = False, collate_fn = data_collator)\n",
    "\n",
    "# # Put the model in evaluation mode\n",
    "# t5_lora_model.eval()\n",
    "\n",
    "# # Make predictions on the test dataset\n",
    "# predictions = []\n",
    "# # We need to collect the batch[\"labels\"] as well, this allows us to filter out all positions with a -100 afterwards\n",
    "# padded_labels = []\n",
    "\n",
    "# counter = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_dataloader:\n",
    "#         print(counter)\n",
    "#         counter += 1\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         # Padded labels from the data collator\n",
    "#         padded_labels += batch['labels'].tolist()\n",
    "#         # Add batch results(logits) to predictions, we take the argmax here to get the predicted class\n",
    "#         prediction = t5_lora_model(input_ids=input_ids)\n",
    "#         print(prediction)\n",
    "#         predictions += prediction.logits.argmax(dim=-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_set[0]['labels'])\n",
    "# print(*[config.label_decoding[x] for x in test_set[0]['labels'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*[config.label_decoding[x] for x in predictions[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_lora_model(ds_test['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_tokenizer.decode(padded_labels[0])\n",
    "# print(*[config.label_decoding[x] for x in padded_labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_lora_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*[[config.label_decoding[y] for y in x] for x in predictions][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Measure Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model_test = T5ForConditionalGeneration.from_pretrained(\n",
    "#     base_model_name,\n",
    "#     device_map='auto',\n",
    "#     offload_folder='./offload',\n",
    "#     load_in_8bit=False\n",
    "# )\n",
    "# tsss_ids = t5_tokenizer('M A P T L F Q K L F S K R T G L G A P G R D A', return_tensors=\"pt\").input_ids.to(device)\n",
    "# tsss_mask = t5_tokenizer('M A P T L F Q K L F S K R T G L G A P G R D A', return_tensors=\"pt\").attention_mask.to(device)\n",
    "# base_model_test(input_ids=tsss_ids, decoder_input_ids=tsss_ids, attention_mask=tsss_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "t5_lora_model.to(device)\n",
    "\n",
    "test_set = ds_test.with_format(\"torch\", device=device)\n",
    "\n",
    "# For token classification we need a data collator here to pad correctly\n",
    "data_collator = DataCollatorForTokenClassification(t5_tokenizer) \n",
    "\n",
    "# Create a dataloader for the test dataset\n",
    "test_dataloader = DataLoader(test_set, batch_size=16, shuffle = False, collate_fn = data_collator)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "t5_lora_model.eval()\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = []\n",
    "# We need to collect the batch[\"labels\"] as well, this allows us to filter out all positions with a -100 afterwards\n",
    "padded_labels = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        print(counter)\n",
    "        counter += 1\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        padded_labels += batch['labels'].tolist()\n",
    "        # Add batch results(logits) to predictions, we take the argmax here to get the predicted class\n",
    "        prediction = t5_lora_model(input_ids=input_ids).logits.argmax(dim=-1).tolist()\n",
    "        print(prediction)\n",
    "        predictions += prediction#.argmax(dim=-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_item = 0\n",
    "\n",
    "actual = [config.label_decoding[x] for x in test_set['labels'][index_item].tolist()]\n",
    "print(actual.__len__())\n",
    "print(*actual)\n",
    "pred = [config.label_decoding[x] for x in predictions[index_item]]\n",
    "print(pred.__len__())\n",
    "print(*pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
