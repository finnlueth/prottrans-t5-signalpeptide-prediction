{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import types\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import (\n",
    "    CrossEntropyLoss,\n",
    "    MSELoss\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoConfig,\n",
    "    T5EncoderModel,\n",
    "    T5Tokenizer,\n",
    "    T5PreTrainedModel,\n",
    "    T5ForConditionalGeneration,\n",
    "    pipeline,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    "    )\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    prepare_model_for_kbit_training\n",
    "    )\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import src.config as config\n",
    "\n",
    "from src.model import (\n",
    "    get_prottrans_tokenizer_model,\n",
    "    df_to_dataset,\n",
    "    inject_linear_layer,\n",
    "    compute_metrics_full,\n",
    "    compute_metrics_fast\n",
    "    )\n",
    "from src.utils import get_project_root_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model:\t Rostlab/prot_t5_xl_uniref50\n",
      "MPS:\t\t True\n",
      "Path:\t\t /Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction\n",
      "Using device:\t mps\n"
     ]
    }
   ],
   "source": [
    "base_model_name = config.base_model_name\n",
    "print(\"Base Model:\\t\", base_model_name)\n",
    "print(\"MPS:\\t\\t\", torch.backends.mps.is_available())\n",
    "ROOT = get_project_root_path()\n",
    "print(\"Path:\\t\\t\", ROOT)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f\"Using device:\\t {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = config.lr\n",
    "batch_size = config.batch_size\n",
    "num_epochs = config.num_epochs\n",
    "dropout_rate = config.dropout_rate\n",
    "\n",
    "label_encoding = config.label_encoding\n",
    "label_list = config.label_decoding\n",
    "\n",
    "compute_metrics = compute_metrics_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Tokenizer and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture = T5EncoderModel\n",
    "t5_tokenizer, t5_base_model = get_prottrans_tokenizer_model(base_model_name, model_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_lora_model_load_adapter = PeftConfig.from_pretrained(ROOT + '/models/linear_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_lora_model_load_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_location = '/models/linear_model_10e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model_config = PeftConfig.from_pretrained(ROOT + adapter_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_base_model = PeftModel.from_pretrained(\n",
    "    model=t5_base_model,\n",
    "    model_id=ROOT+adapter_location,\n",
    "    # is_trainable=False,\n",
    "    )\n",
    "# del t5_base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model = inject_linear_layer(t5_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = label_list.__len__()\n",
    "t5_lora_model.dropout = nn.Dropout(dropout_rate)\n",
    "t5_lora_model.num_labels = num_labels\n",
    "\n",
    "t5_lora_model.get_base_model().dropout = nn.Dropout(dropout_rate)\n",
    "t5_lora_model.get_base_model().classifier = nn.Linear(\n",
    "    in_features=t5_lora_model.get_base_model().config.hidden_size,\n",
    "    out_features=label_list.__len__()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): T5EncoderModel(\n",
       "      (shared): Embedding(128, 1024)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(128, 1024)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (v): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(\n",
       "                    in_features=4096, out_features=1024, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (relative_attention_bias): Embedding(32, 32)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-23): 23 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (v): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(\n",
       "                    in_features=4096, out_features=1024, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (classifier): Linear(in_features=1024, out_features=7, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_lora_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Make Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_parquet(ROOT + '/data/processed/5.0_train.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Use entire test set\n",
    "ds_test = df_data[df_data.Split == 'test']\n",
    "\n",
    "ds_test = df_to_dataset(\n",
    "    t5_tokenizer,\n",
    "    ds_test.Sequence.to_list()[:3],\n",
    "    ds_test.Label.to_list()[:3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 4 5 11 6 14 19 9 5 20 9 11 7 10 21 17 7 18 18 3 10 11 16 9 3 18 7 7 6 13 6 7 17 19 17 7 5 4 5 7 19 17 7 19 17 11 18 19 11 19 17 11 19 11 11 7 5 17 19 11 13 3 7 15 17 19 7 18 3 17 1\n",
      "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "print(*ds_test['input_ids'][0])\n",
    "print(*ds_test['attention_mask'][0])\n",
    "print(*ds_test['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M L G T V K M E G H E T S D W N S Y Y A D T Q E A Y S S V P V S N M N S G L G S M N S M N T Y M T M N T M T T S G N M T P A S F N M S Y A N\n"
     ]
    }
   ],
   "source": [
    "input_str = t5_tokenizer.decode(ds_test['input_ids'][0][:-1])\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [19, 4, 5, 11, 6, 14, 19, 9, 5, 20, 9, 11, 7, 10, 21, 17, 7, 18, 18, 3, 10, 11, 16, 9, 3, 18, 7, 7, 6, 13, 6, 7, 17, 19, 17, 7, 5, 4, 5, 7, 19, 17, 7, 19, 17, 11, 18, 19, 11, 19, 17, 11, 19, 11, 11, 7, 5, 17, 19, 11, 13, 3, 7, 15, 17, 19, 7, 18, 3, 17, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "inputs = t5_tokenizer(input_str)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     logits = t5_lora_model(inputs['input_ids']).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cpu'\n",
    "# t5_lora_model.to(device)\n",
    "\n",
    "# test_set = ds_test.with_format(\"torch\", device=device)\n",
    "\n",
    "# # For token classification we need a data collator here to pad correctly\n",
    "# data_collator = DataCollatorForTokenClassification(t5_tokenizer) \n",
    "\n",
    "# # Create a dataloader for the test dataset\n",
    "# test_dataloader = DataLoader(test_set, batch_size=16, shuffle = False, collate_fn = data_collator)\n",
    "\n",
    "# # Put the model in evaluation mode\n",
    "# t5_lora_model.eval()\n",
    "\n",
    "# # Make predictions on the test dataset\n",
    "# predictions = []\n",
    "# # We need to collect the batch[\"labels\"] as well, this allows us to filter out all positions with a -100 afterwards\n",
    "# padded_labels = []\n",
    "\n",
    "# counter = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_dataloader:\n",
    "#         print(counter)\n",
    "#         counter += 1\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         # Padded labels from the data collator\n",
    "#         padded_labels += batch['labels'].tolist()\n",
    "#         # Add batch results(logits) to predictions, we take the argmax here to get the predicted class\n",
    "#         prediction = t5_lora_model(input_ids=input_ids)\n",
    "#         print(prediction)\n",
    "#         predictions += prediction.logits.argmax(dim=-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_set[0]['labels'])\n",
    "# print(*[config.label_decoding[x] for x in test_set[0]['labels'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*[config.label_decoding[x] for x in predictions[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_lora_model(ds_test['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_tokenizer.decode(padded_labels[0])\n",
    "# print(*[config.label_decoding[x] for x in padded_labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_lora_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*[[config.label_decoding[y] for y in x] for x in predictions][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Measure Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model_test = T5ForConditionalGeneration.from_pretrained(\n",
    "#     base_model_name,\n",
    "#     device_map='auto',\n",
    "#     offload_folder='./offload',\n",
    "#     load_in_8bit=False\n",
    "# )\n",
    "# tsss_ids = t5_tokenizer('M A P T L F Q K L F S K R T G L G A P G R D A', return_tensors=\"pt\").input_ids.to(device)\n",
    "# tsss_mask = t5_tokenizer('M A P T L F Q K L F S K R T G L G A P G R D A', return_tensors=\"pt\").attention_mask.to(device)\n",
    "# base_model_test(input_ids=tsss_ids, decoder_input_ids=tsss_ids, attention_mask=tsss_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): T5EncoderModel(\n",
       "      (shared): Embedding(128, 1024)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(128, 1024)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (v): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(\n",
       "                    in_features=4096, out_features=1024, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (relative_attention_bias): Embedding(32, 32)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-23): 23 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (v): Linear(\n",
       "                    in_features=1024, out_features=4096, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(\n",
       "                    in_features=4096, out_features=1024, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (classifier): Linear(in_features=1024, out_features=7, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[ 1.2061e-01, -2.4701e-01, -3.6933e-01,  ...,  1.3971e-01,\n",
      "          -2.4973e-02,  2.3062e-02],\n",
      "         [ 1.6829e-01, -7.5784e-02, -3.9558e-01,  ...,  1.1992e-01,\n",
      "           1.0360e-01,  5.2894e-02],\n",
      "         [ 3.6246e-02, -5.4505e-02, -3.3214e-01,  ...,  1.4672e-01,\n",
      "          -1.0342e-01,  5.0039e-02],\n",
      "         ...,\n",
      "         [-4.1874e-02, -1.9174e-01,  5.7670e-02,  ..., -1.7052e-01,\n",
      "          -1.8485e-01, -1.4074e-01],\n",
      "         [ 2.6904e-02, -2.4482e-01, -7.5830e-02,  ..., -2.4637e-01,\n",
      "          -1.8791e-01, -2.6444e-01],\n",
      "         [ 4.1467e-03, -9.0839e-02,  7.3105e-02,  ..., -3.0806e-02,\n",
      "           1.4411e-02,  6.7461e-02]],\n",
      "\n",
      "        [[ 1.1356e-01, -2.5487e-01, -2.8510e-01,  ...,  1.5056e-01,\n",
      "          -2.4442e-02,  2.8618e-02],\n",
      "         [ 1.5121e-01, -4.7758e-02, -3.7023e-01,  ...,  1.8454e-01,\n",
      "           1.4376e-01,  2.7271e-03],\n",
      "         [ 8.1871e-02, -6.2955e-02, -2.3713e-01,  ...,  2.4631e-01,\n",
      "          -1.6276e-04,  1.2636e-01],\n",
      "         ...,\n",
      "         [-4.0105e-02, -8.6196e-02, -7.4802e-02,  ..., -1.8922e-01,\n",
      "          -1.2175e-01, -2.9285e-01],\n",
      "         [-8.9281e-02, -2.0849e-01,  3.9762e-03,  ..., -1.5632e-01,\n",
      "          -5.2133e-02, -2.3360e-01],\n",
      "         [ 1.7620e-02, -8.5627e-02,  6.2587e-02,  ..., -5.6281e-02,\n",
      "          -8.8579e-03,  4.7037e-02]],\n",
      "\n",
      "        [[ 4.4615e-01, -2.2368e-01,  1.3201e-01,  ..., -2.8567e-02,\n",
      "           1.9644e-01, -2.9381e-01],\n",
      "         [ 4.6194e-02, -1.6752e-01, -6.1630e-02,  ...,  1.0302e-01,\n",
      "           4.0796e-02, -2.3228e-01],\n",
      "         [ 3.6324e-01, -1.2969e-01, -2.0465e-01,  ...,  1.2304e-01,\n",
      "           1.4172e-01, -3.5697e-02],\n",
      "         ...,\n",
      "         [-9.5539e-03, -3.0617e-01, -1.3369e-01,  ..., -6.3282e-02,\n",
      "           8.6415e-02, -2.8834e-01],\n",
      "         [ 1.3444e-02, -1.8973e-01,  8.5512e-02,  ...,  3.2710e-02,\n",
      "           2.1076e-02,  2.9173e-03],\n",
      "         [-5.9027e-02, -5.8956e-02,  7.1965e-02,  ..., -4.3678e-03,\n",
      "          -5.2423e-02,  8.0335e-02]]]), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n",
      "[[2, 6, 5, 6, 0, 2, 6, 4, 5, 2, 0, 0, 0, 2, 0, 0, 5, 2, 2, 2, 2, 0, 6, 4, 5, 2, 5, 3, 2, 2, 2, 5, 2, 1, 2, 1, 5, 6, 2, 5, 3, 2, 5, 3, 0, 0, 2, 2, 5, 3, 5, 0, 2, 0, 0, 6, 2, 2, 2, 0, 4, 4, 2, 2, 2, 2, 5, 2, 2, 2, 4], [2, 6, 5, 2, 0, 2, 1, 4, 0, 2, 2, 0, 0, 5, 2, 5, 5, 2, 2, 2, 2, 2, 2, 5, 2, 5, 5, 6, 5, 5, 1, 2, 0, 2, 6, 5, 2, 2, 2, 2, 2, 0, 2, 2, 5, 2, 5, 2, 2, 2, 2, 4, 1, 4, 2, 0, 2, 2, 0, 2, 5, 5, 2, 5, 3, 0, 0, 2, 3, 3, 4], [4, 5, 0, 0, 2, 0, 2, 5, 2, 2, 2, 0, 2, 5, 5, 0, 0, 6, 6, 5, 0, 5, 6, 0, 6, 4, 1, 4, 1, 4, 2, 4, 0, 2, 2, 4, 2, 5, 0, 0, 5, 5, 0, 6, 5, 0, 0, 0, 2, 1, 3, 0, 4, 6, 5, 5, 6, 5, 4, 1, 2, 5, 3, 3, 2, 3, 1, 2, 0, 3, 4]]\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "t5_lora_model.to(device)\n",
    "\n",
    "test_set = ds_test.with_format(\"torch\", device=device)\n",
    "\n",
    "# For token classification we need a data collator here to pad correctly\n",
    "data_collator = DataCollatorForTokenClassification(t5_tokenizer) \n",
    "\n",
    "# Create a dataloader for the test dataset\n",
    "test_dataloader = DataLoader(test_set, batch_size=16, shuffle = False, collate_fn = data_collator)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "t5_lora_model.eval()\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = []\n",
    "# We need to collect the batch[\"labels\"] as well, this allows us to filter out all positions with a -100 afterwards\n",
    "padded_labels = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        print(counter)\n",
    "        counter += 1\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        padded_labels += batch['labels'].tolist()\n",
    "        # Add batch results(logits) to predictions, we take the argmax here to get the predicted class\n",
    "        prediction = t5_lora_model(input_ids=input_ids).logits.argmax(dim=-1).tolist()\n",
    "        print(prediction)\n",
    "        predictions += prediction#.argmax(dim=-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/notebooks/lora-linear-model-predict.ipynb Cell 42\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/notebooks/lora-linear-model-predict.ipynb#Y102sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(actual\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/notebooks/lora-linear-model-predict.ipynb#Y102sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m*\u001b[39mactual)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/notebooks/lora-linear-model-predict.ipynb#Y102sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m pred \u001b[39m=\u001b[39m [config\u001b[39m.\u001b[39;49mlabel_decoding[x] \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m predictions[index_item]]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/notebooks/lora-linear-model-predict.ipynb#Y102sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(pred\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/notebooks/lora-linear-model-predict.ipynb#Y102sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m*\u001b[39mpred)\n",
      "\u001b[1;32m/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/notebooks/lora-linear-model-predict.ipynb Cell 42\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/notebooks/lora-linear-model-predict.ipynb#Y102sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(actual\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/notebooks/lora-linear-model-predict.ipynb#Y102sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m*\u001b[39mactual)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/notebooks/lora-linear-model-predict.ipynb#Y102sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m pred \u001b[39m=\u001b[39m [config\u001b[39m.\u001b[39;49mlabel_decoding[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m predictions[index_item]]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/notebooks/lora-linear-model-predict.ipynb#Y102sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(pred\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/notebooks/lora-linear-model-predict.ipynb#Y102sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m*\u001b[39mpred)\n",
      "\u001b[0;31mKeyError\u001b[0m: 6"
     ]
    }
   ],
   "source": [
    "index_item = 0\n",
    "\n",
    "actual = [config.label_decoding[x] for x in test_set['labels'][index_item].tolist()]\n",
    "print(actual.__len__())\n",
    "print(*actual)\n",
    "pred = [config.label_decoding[x] for x in predictions[index_item]]\n",
    "print(pred.__len__())\n",
    "print(*pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
