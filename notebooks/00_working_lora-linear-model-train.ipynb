{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/.venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gc\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import types\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import (\n",
    "    CrossEntropyLoss,\n",
    "    MSELoss\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoConfig,\n",
    "    T5EncoderModel,\n",
    "    T5Tokenizer,\n",
    "    T5PreTrainedModel,\n",
    "    T5ForConditionalGeneration,\n",
    "    pipeline,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    "    EvalPrediction,\n",
    "    )\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    prepare_model_for_kbit_training\n",
    "    )\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import src.config as config\n",
    "import src.config\n",
    "import src.data\n",
    "import src.model_new\n",
    "\n",
    "from src.model_working import (\n",
    "    get_prottrans_tokenizer_model,\n",
    "    df_to_dataset,\n",
    "    inject_linear_layer,\n",
    "    )\n",
    "from src.utils import get_project_root_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model:\t Rostlab/prot_t5_xl_uniref50\n",
      "MPS:\t\t True\n",
      "Path:\t\t /Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction\n",
      "Using device:\t mps\n"
     ]
    }
   ],
   "source": [
    "base_model_name = config.base_model_name\n",
    "print(\"Base Model:\\t\", base_model_name)\n",
    "print(\"MPS:\\t\\t\", torch.backends.mps.is_available())\n",
    "ROOT = get_project_root_path()\n",
    "print(\"Path:\\t\\t\", ROOT)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f\"Using device:\\t {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Tokenizer and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_architecture = T5EncoderModel\n",
    "t5_tokenizer, t5_base_model = get_prottrans_tokenizer_model(base_model_name, model_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Data, Split into Dataset, and Tokenize Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/.venv/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "FASTA_FILENAME = '5_SignalP_5.0_Training_set.fasta'\n",
    "annotations_name = 'Label' # Choose Type or Label\n",
    "\n",
    "df_data = src.data.process(src.data.parse_file(ROOT + '/data/raw/' + FASTA_FILENAME))\n",
    "\n",
    "dataset_signalp = src.model_new.create_datasets(\n",
    "    splits=src.config.splits,\n",
    "    tokenizer=t5_tokenizer,\n",
    "    data=df_data,\n",
    "    annotations_name=annotations_name,\n",
    "    dataset_size=src.config.dataset_size,\n",
    "    encoder=src.config.select_encodings[annotations_name],\n",
    "    )\n",
    "\n",
    "del df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,932,160 || all params: 1,212,073,984 || trainable%: 0.32441584027926795\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "        # task_type=TaskType.TOKEN_CLS,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=['q', 'k', 'v', 'o'],\n",
    "        # target_modules=['o'],\n",
    "        bias=\"none\",\n",
    "    )\n",
    "t5_lora_model = get_peft_model(t5_base_model, lora_config)\n",
    "t5_lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Loop\n",
    "https://huggingface.co/docs/peft/task_guides/token-classification-lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model = inject_linear_layer(\n",
    "    t5_lora_model=t5_lora_model,\n",
    "    num_labels=config.label_decoding.__len__(),\n",
    "    dropout_rate=config.dropout_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seqeval_metric = evaluate.load(\"seqeval\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "# roc_auc_score_metric = evaluate.load(\"roc_auc\", \"multiclass\")\n",
    "# roc_auc_score = evaluate.load(\"roc_auc\")\n",
    "matthews_correlation_metric = evaluate.load(\"matthews_correlation\")\n",
    "\n",
    "def batch_eval_elementwise(predictions: np.ndarray, references: np.ndarray):\n",
    "    results = {}\n",
    "    # predictions = np.nan_to_num(predictions).argmax(axis=-1)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    results.update({'accuracy_metric': np.average([accuracy_metric.compute(predictions=x, references=y)['accuracy'] for x, y in zip(predictions, references)])})\n",
    "    results.update({'precision_metric': np.average([precision_metric.compute(predictions=x, references=y, average='micro')['precision'] for x, y in zip(predictions, references)])})\n",
    "    results.update({'recall_metric': np.average([recall_metric.compute(predictions=x, references=y, average='micro')['recall'] for x, y in zip(predictions, references)])})\n",
    "    results.update({'f1_metric': np.average([f1_metric.compute(predictions=x, references=y, average='micro')['f1'] for x, y in zip(predictions, references)])})\n",
    "    # results.update({'roc_auc': np.average([roc_auc_score_metric.compute(prediction_scores=x, references=y, average='micro')['roc_auc'] for x, y in zip(predictions, references)])})\n",
    "    results.update({'matthews_correlation': np.average([matthews_correlation_metric.compute(predictions=x, references=y, average='micro')['matthews_correlation'] for x, y in zip(predictions, references)])})\n",
    "    return results\n",
    "# display(batch_eval_elementwise(predictions.numpy(), references.numpy()))\n",
    "\n",
    "def compute_metrics(p):\n",
    "    # print('=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= preds compute_metrics start =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=')\n",
    "    predictions, references = p\n",
    "    results = batch_eval_elementwise(predictions=predictions, references=references)\n",
    "    # print(results)\n",
    "    # print('=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= preds compute_metrics stop =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=')\n",
    "    return results\n",
    "# metrics = compute_metrics((predictions, references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=t5_tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./checkpoints',\n",
    "    learning_rate=config.lr,\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    per_device_eval_batch_size=config.batch_size,\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    logging_steps=config.logging_steps,\n",
    "    # save_strategy=\"steps\",\n",
    "    # save_steps=config.save_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1,\n",
    "    # gradient_accumulation_steps=accum,\n",
    "    # load_best_model_at_end=True,\n",
    "    # save_total_limit=5,\n",
    "    seed=42,\n",
    "    # fp16=True,\n",
    "    # deepspeed=deepspeed_config,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=['labels'],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=t5_lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_signalp['train'],\n",
    "    eval_dataset=dataset_signalp['valid'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(next(t5_lora_model.parameters()).is_cuda)\n",
    "# print(t5_lora_model.device)\n",
    "# print(config.label_decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155af5dfd0f94843b8efec11998147c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.get_base_model().forward <bound method T5EncoderModel.forward of T5EncoderModel(\n",
      "  (shared): Embedding(128, 1024)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(\n",
      "                in_features=4096, out_features=1024, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(\n",
      "                in_features=4096, out_features=1024, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")>\n",
      "self.config.hidden_size 1024\n",
      "self.num_labels 6\n",
      "\n",
      "encoder_outputs.last_hidden_state tensor([[[ 0.2653, -0.2492,  0.2399,  ...,  0.0000, -0.0666, -0.0345],\n",
      "         [ 0.1839, -0.2975,  0.0913,  ...,  0.1990,  0.0655,  0.1939],\n",
      "         [ 0.0936, -0.2798,  0.2663,  ...,  0.1852,  0.1629, -0.1198],\n",
      "         ...,\n",
      "         [ 0.0000, -0.1162,  0.1140,  ..., -0.0567, -0.0000, -0.3199],\n",
      "         [-0.0000, -0.0000,  0.3953,  ..., -0.1075,  0.1683,  0.1101],\n",
      "         [-0.0249, -0.0000,  0.0634,  ...,  0.0228, -0.0542, -0.0026]],\n",
      "\n",
      "        [[ 0.1230, -0.2503, -0.2469,  ...,  0.4877,  0.0787,  0.1991],\n",
      "         [-0.2411, -0.0089, -0.3791,  ...,  0.0306,  0.0303,  0.1048],\n",
      "         [-0.0280, -0.3618,  0.1582,  ..., -0.0189,  0.0044, -0.1488],\n",
      "         ...,\n",
      "         [-0.2533, -0.1529,  0.3040,  ...,  0.2405,  0.0590,  0.0516],\n",
      "         [ 0.0290, -0.1405,  0.1143,  ...,  0.0000, -0.0393,  0.1386],\n",
      "         [-0.0325, -0.0552,  0.0000,  ..., -0.0048,  0.0341,  0.1237]],\n",
      "\n",
      "        [[ 0.2651, -0.1602, -0.2149,  ...,  0.1810,  0.3881,  0.1915],\n",
      "         [ 0.3196, -0.0655, -0.0000,  ...,  0.1782,  0.5748,  0.0000],\n",
      "         [ 0.1354, -0.2681, -0.2067,  ...,  0.0000,  0.3679,  0.0820],\n",
      "         ...,\n",
      "         [-0.0342, -0.0000,  0.4235,  ..., -0.2304, -0.1151,  0.0000],\n",
      "         [ 0.6875, -0.1258, -0.5738,  ..., -0.5683,  0.3380,  0.2640],\n",
      "         [-0.0478, -0.0932,  0.0378,  ..., -0.0733, -0.0000, -0.0442]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1784, -0.2872,  0.0456,  ...,  0.2696,  0.3589,  0.0321],\n",
      "         [-0.1417, -0.1312, -0.1476,  ...,  0.1301,  0.1188, -0.0110],\n",
      "         [ 0.2515,  0.0000, -0.0000,  ...,  0.1556,  0.1215, -0.1387],\n",
      "         ...,\n",
      "         [-0.1033, -0.1650,  0.2861,  ..., -0.2066, -0.2692, -0.0126],\n",
      "         [-0.0182, -0.3835,  0.0012,  ..., -0.2301, -0.0725, -0.0000],\n",
      "         [-0.0766, -0.1088,  0.0612,  ..., -0.0043, -0.0197,  0.0176]],\n",
      "\n",
      "        [[ 0.0112, -0.0319,  0.0693,  ...,  0.3266,  0.2105, -0.0207],\n",
      "         [-0.0493, -0.1497,  0.0753,  ...,  0.0000,  0.1525, -0.0000],\n",
      "         [-0.0228, -0.2590,  0.1141,  ...,  0.2743,  0.3631, -0.3204],\n",
      "         ...,\n",
      "         [ 0.0000, -0.1285, -0.0490,  ...,  0.0456, -0.0392,  0.1034],\n",
      "         [ 0.0683, -0.0636,  0.1534,  ..., -0.0681,  0.1988, -0.2293],\n",
      "         [-0.0319, -0.1005,  0.0950,  ..., -0.0955, -0.0000, -0.0175]],\n",
      "\n",
      "        [[ 0.0751, -0.1268,  0.0170,  ...,  0.2119,  0.2976,  0.4358],\n",
      "         [-0.2717, -0.0641, -0.0799,  ...,  0.2169,  0.0568,  0.2369],\n",
      "         [-0.1938, -0.1106, -0.0697,  ...,  0.2406,  0.3537, -0.0000],\n",
      "         ...,\n",
      "         [ 0.0113, -0.1986,  0.1689,  ..., -0.0000, -0.1805,  0.1930],\n",
      "         [-0.0000, -0.0443,  0.0656,  ..., -0.3168, -0.3580, -0.2131],\n",
      "         [-0.0498, -0.0317,  0.0521,  ..., -0.0533,  0.0000,  0.0000]]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "sequence_output dropout tensor([[[ 0.2948, -0.2769,  0.2666,  ...,  0.0000, -0.0740, -0.0384],\n",
      "         [ 0.2044, -0.0000,  0.1014,  ...,  0.2211,  0.0728,  0.2155],\n",
      "         [ 0.1039, -0.3108,  0.2959,  ...,  0.2058,  0.1810, -0.1331],\n",
      "         ...,\n",
      "         [ 0.0000, -0.1291,  0.1267,  ..., -0.0000, -0.0000, -0.3554],\n",
      "         [-0.0000, -0.0000,  0.4393,  ..., -0.1195,  0.1870,  0.1223],\n",
      "         [-0.0276, -0.0000,  0.0705,  ...,  0.0254, -0.0602, -0.0000]],\n",
      "\n",
      "        [[ 0.1367, -0.2782, -0.2743,  ...,  0.5419,  0.0875,  0.2212],\n",
      "         [-0.0000, -0.0099, -0.4212,  ...,  0.0340,  0.0337,  0.1165],\n",
      "         [-0.0311, -0.4020,  0.1758,  ..., -0.0210,  0.0049, -0.1653],\n",
      "         ...,\n",
      "         [-0.2815, -0.1699,  0.3378,  ...,  0.2672,  0.0655,  0.0573],\n",
      "         [ 0.0323, -0.1561,  0.1270,  ...,  0.0000, -0.0437,  0.1540],\n",
      "         [-0.0000, -0.0614,  0.0000,  ..., -0.0054,  0.0379,  0.1374]],\n",
      "\n",
      "        [[ 0.2946, -0.1780, -0.2388,  ...,  0.2011,  0.4313,  0.2128],\n",
      "         [ 0.3551, -0.0727, -0.0000,  ...,  0.1980,  0.6387,  0.0000],\n",
      "         [ 0.1505, -0.2979, -0.2297,  ...,  0.0000,  0.4088,  0.0911],\n",
      "         ...,\n",
      "         [-0.0380, -0.0000,  0.4705,  ..., -0.2560, -0.1279,  0.0000],\n",
      "         [ 0.0000, -0.1398, -0.6376,  ..., -0.6315,  0.3756,  0.2934],\n",
      "         [-0.0531, -0.1036,  0.0420,  ..., -0.0814, -0.0000, -0.0491]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1982, -0.3191,  0.0507,  ...,  0.2996,  0.3988,  0.0356],\n",
      "         [-0.1575, -0.1458, -0.1640,  ...,  0.1446,  0.1319, -0.0122],\n",
      "         [ 0.2795,  0.0000, -0.0000,  ...,  0.0000,  0.1350, -0.1541],\n",
      "         ...,\n",
      "         [-0.1148, -0.1834,  0.3178,  ..., -0.2295, -0.2991, -0.0140],\n",
      "         [-0.0202, -0.4262,  0.0013,  ..., -0.2557, -0.0805, -0.0000],\n",
      "         [-0.0851, -0.1209,  0.0680,  ..., -0.0048, -0.0000,  0.0196]],\n",
      "\n",
      "        [[ 0.0124, -0.0355,  0.0771,  ...,  0.3629,  0.2339, -0.0000],\n",
      "         [-0.0548, -0.1663,  0.0837,  ...,  0.0000,  0.1694, -0.0000],\n",
      "         [-0.0254, -0.2878,  0.1267,  ...,  0.3048,  0.4035, -0.3561],\n",
      "         ...,\n",
      "         [ 0.0000, -0.1428, -0.0544,  ...,  0.0507, -0.0435,  0.1149],\n",
      "         [ 0.0758, -0.0706,  0.1705,  ..., -0.0757,  0.2209, -0.2548],\n",
      "         [-0.0355, -0.1117,  0.1056,  ..., -0.1061, -0.0000, -0.0194]],\n",
      "\n",
      "        [[ 0.0834, -0.1409,  0.0189,  ...,  0.2354,  0.3306,  0.4842],\n",
      "         [-0.3019, -0.0713, -0.0888,  ...,  0.2410,  0.0632,  0.2632],\n",
      "         [-0.0000, -0.1229, -0.0775,  ...,  0.2673,  0.3930, -0.0000],\n",
      "         ...,\n",
      "         [ 0.0125, -0.2206,  0.1877,  ..., -0.0000, -0.2006,  0.2145],\n",
      "         [-0.0000, -0.0492,  0.0729,  ..., -0.3520, -0.3978, -0.2368],\n",
      "         [-0.0554, -0.0352,  0.0579,  ..., -0.0592,  0.0000,  0.0000]]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "sequence_output linear tensor([[[ 0.2948, -0.2769,  0.2666,  ...,  0.0000, -0.0740, -0.0384],\n",
      "         [ 0.2044, -0.0000,  0.1014,  ...,  0.2211,  0.0728,  0.2155],\n",
      "         [ 0.1039, -0.3108,  0.2959,  ...,  0.2058,  0.1810, -0.1331],\n",
      "         ...,\n",
      "         [ 0.0000, -0.1291,  0.1267,  ..., -0.0000, -0.0000, -0.3554],\n",
      "         [-0.0000, -0.0000,  0.4393,  ..., -0.1195,  0.1870,  0.1223],\n",
      "         [-0.0276, -0.0000,  0.0705,  ...,  0.0254, -0.0602, -0.0000]],\n",
      "\n",
      "        [[ 0.1367, -0.2782, -0.2743,  ...,  0.5419,  0.0875,  0.2212],\n",
      "         [-0.0000, -0.0099, -0.4212,  ...,  0.0340,  0.0337,  0.1165],\n",
      "         [-0.0311, -0.4020,  0.1758,  ..., -0.0210,  0.0049, -0.1653],\n",
      "         ...,\n",
      "         [-0.2815, -0.1699,  0.3378,  ...,  0.2672,  0.0655,  0.0573],\n",
      "         [ 0.0323, -0.1561,  0.1270,  ...,  0.0000, -0.0437,  0.1540],\n",
      "         [-0.0000, -0.0614,  0.0000,  ..., -0.0054,  0.0379,  0.1374]],\n",
      "\n",
      "        [[ 0.2946, -0.1780, -0.2388,  ...,  0.2011,  0.4313,  0.2128],\n",
      "         [ 0.3551, -0.0727, -0.0000,  ...,  0.1980,  0.6387,  0.0000],\n",
      "         [ 0.1505, -0.2979, -0.2297,  ...,  0.0000,  0.4088,  0.0911],\n",
      "         ...,\n",
      "         [-0.0380, -0.0000,  0.4705,  ..., -0.2560, -0.1279,  0.0000],\n",
      "         [ 0.0000, -0.1398, -0.6376,  ..., -0.6315,  0.3756,  0.2934],\n",
      "         [-0.0531, -0.1036,  0.0420,  ..., -0.0814, -0.0000, -0.0491]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1982, -0.3191,  0.0507,  ...,  0.2996,  0.3988,  0.0356],\n",
      "         [-0.1575, -0.1458, -0.1640,  ...,  0.1446,  0.1319, -0.0122],\n",
      "         [ 0.2795,  0.0000, -0.0000,  ...,  0.0000,  0.1350, -0.1541],\n",
      "         ...,\n",
      "         [-0.1148, -0.1834,  0.3178,  ..., -0.2295, -0.2991, -0.0140],\n",
      "         [-0.0202, -0.4262,  0.0013,  ..., -0.2557, -0.0805, -0.0000],\n",
      "         [-0.0851, -0.1209,  0.0680,  ..., -0.0048, -0.0000,  0.0196]],\n",
      "\n",
      "        [[ 0.0124, -0.0355,  0.0771,  ...,  0.3629,  0.2339, -0.0000],\n",
      "         [-0.0548, -0.1663,  0.0837,  ...,  0.0000,  0.1694, -0.0000],\n",
      "         [-0.0254, -0.2878,  0.1267,  ...,  0.3048,  0.4035, -0.3561],\n",
      "         ...,\n",
      "         [ 0.0000, -0.1428, -0.0544,  ...,  0.0507, -0.0435,  0.1149],\n",
      "         [ 0.0758, -0.0706,  0.1705,  ..., -0.0757,  0.2209, -0.2548],\n",
      "         [-0.0355, -0.1117,  0.1056,  ..., -0.1061, -0.0000, -0.0194]],\n",
      "\n",
      "        [[ 0.0834, -0.1409,  0.0189,  ...,  0.2354,  0.3306,  0.4842],\n",
      "         [-0.3019, -0.0713, -0.0888,  ...,  0.2410,  0.0632,  0.2632],\n",
      "         [-0.0000, -0.1229, -0.0775,  ...,  0.2673,  0.3930, -0.0000],\n",
      "         ...,\n",
      "         [ 0.0125, -0.2206,  0.1877,  ..., -0.0000, -0.2006,  0.2145],\n",
      "         [-0.0000, -0.0492,  0.0729,  ..., -0.3520, -0.3978, -0.2368],\n",
      "         [-0.0554, -0.0352,  0.0579,  ..., -0.0592,  0.0000,  0.0000]]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "found labels\n",
      "logits.device mps:0\n",
      "tensor([[   1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3, -100],\n",
      "        [   4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3, -100],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [   4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3, -100],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [   4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3, -100]],\n",
      "       device='mps:0')\n",
      "loss tensor(1.7924, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "loss tensor(1.7924, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "{'loss': 1.7924, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "self.get_base_model().forward <bound method T5EncoderModel.forward of T5EncoderModel(\n",
      "  (shared): Embedding(128, 1024)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(\n",
      "                in_features=4096, out_features=1024, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(\n",
      "                in_features=4096, out_features=1024, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")>\n",
      "self.config.hidden_size 1024\n",
      "self.num_labels 6\n",
      "\n",
      "encoder_outputs.last_hidden_state tensor([[[ 1.2375e-01, -4.8629e-01,  3.9872e-01,  ...,  1.6893e-01,\n",
      "           7.3654e-02,  7.1800e-02],\n",
      "         [-1.8832e-02, -2.8825e-01,  7.4608e-02,  ...,  1.7238e-01,\n",
      "           3.1314e-01,  5.7781e-02],\n",
      "         [ 1.1225e-01,  6.0611e-02, -6.5913e-02,  ..., -1.1262e-01,\n",
      "          -3.5694e-02, -3.4978e-01],\n",
      "         ...,\n",
      "         [-2.6904e-01, -1.3911e-01, -5.7208e-02,  ..., -5.2893e-02,\n",
      "          -1.4785e-01, -4.1311e-01],\n",
      "         [-1.2000e-01,  9.4425e-02,  1.4164e-01,  ...,  1.8941e-01,\n",
      "           3.2264e-04, -3.1129e-01],\n",
      "         [-2.6453e-02, -8.9668e-02,  4.0289e-02,  ...,  9.9320e-03,\n",
      "          -1.0007e-01,  3.5352e-02]],\n",
      "\n",
      "        [[ 2.7390e-01, -1.3383e-01,  6.3151e-02,  ...,  2.4540e-01,\n",
      "           2.5042e-01,  1.1205e-01],\n",
      "         [ 2.2027e-02, -3.3821e-01,  1.8124e-02,  ...,  8.7925e-02,\n",
      "          -4.6127e-02,  1.2867e-01],\n",
      "         [ 5.6413e-01, -2.1803e-01, -1.9680e-03,  ...,  2.4934e-02,\n",
      "           3.6739e-01, -1.6236e-01],\n",
      "         ...,\n",
      "         [-1.4055e-01, -1.4510e-01, -1.5750e-01,  ..., -4.2653e-02,\n",
      "           2.2649e-01, -5.7282e-02],\n",
      "         [ 2.4534e-01, -8.7595e-03,  2.8439e-04,  ..., -3.1259e-01,\n",
      "           9.3517e-02,  1.0352e-01],\n",
      "         [-1.0279e-02, -8.7609e-02,  4.9097e-03,  ..., -1.6145e-02,\n",
      "          -8.5884e-03,  4.0168e-02]],\n",
      "\n",
      "        [[ 1.2445e-01, -2.3113e-01, -2.3602e-01,  ...,  2.9269e-01,\n",
      "           3.2545e-01,  3.4910e-01],\n",
      "         [ 3.9973e-02, -2.6143e-01,  8.3983e-02,  ...,  1.4909e-01,\n",
      "           1.8539e-01,  3.5900e-01],\n",
      "         [ 1.1115e-01, -1.1139e-01, -1.3701e-01,  ..., -2.2539e-01,\n",
      "           1.4283e-02,  7.3366e-02],\n",
      "         ...,\n",
      "         [-2.0387e-01, -7.6420e-02,  3.7006e-02,  ..., -1.1626e-01,\n",
      "           2.0864e-02,  2.6999e-01],\n",
      "         [-1.2999e-01, -8.3637e-02,  2.0148e-02,  ..., -1.1530e-01,\n",
      "          -2.8728e-01,  1.4261e-01],\n",
      "         [-2.1702e-03, -5.8295e-02, -2.3955e-03,  ..., -3.1265e-02,\n",
      "          -4.8293e-02, -5.0737e-02]]], device='mps:0')\n",
      "sequence_output dropout tensor([[[ 1.2375e-01, -4.8629e-01,  3.9872e-01,  ...,  1.6893e-01,\n",
      "           7.3654e-02,  7.1800e-02],\n",
      "         [-1.8832e-02, -2.8825e-01,  7.4608e-02,  ...,  1.7238e-01,\n",
      "           3.1314e-01,  5.7781e-02],\n",
      "         [ 1.1225e-01,  6.0611e-02, -6.5913e-02,  ..., -1.1262e-01,\n",
      "          -3.5694e-02, -3.4978e-01],\n",
      "         ...,\n",
      "         [-2.6904e-01, -1.3911e-01, -5.7208e-02,  ..., -5.2893e-02,\n",
      "          -1.4785e-01, -4.1311e-01],\n",
      "         [-1.2000e-01,  9.4425e-02,  1.4164e-01,  ...,  1.8941e-01,\n",
      "           3.2264e-04, -3.1129e-01],\n",
      "         [-2.6453e-02, -8.9668e-02,  4.0289e-02,  ...,  9.9320e-03,\n",
      "          -1.0007e-01,  3.5352e-02]],\n",
      "\n",
      "        [[ 2.7390e-01, -1.3383e-01,  6.3151e-02,  ...,  2.4540e-01,\n",
      "           2.5042e-01,  1.1205e-01],\n",
      "         [ 2.2027e-02, -3.3821e-01,  1.8124e-02,  ...,  8.7925e-02,\n",
      "          -4.6127e-02,  1.2867e-01],\n",
      "         [ 5.6413e-01, -2.1803e-01, -1.9680e-03,  ...,  2.4934e-02,\n",
      "           3.6739e-01, -1.6236e-01],\n",
      "         ...,\n",
      "         [-1.4055e-01, -1.4510e-01, -1.5750e-01,  ..., -4.2653e-02,\n",
      "           2.2649e-01, -5.7282e-02],\n",
      "         [ 2.4534e-01, -8.7595e-03,  2.8439e-04,  ..., -3.1259e-01,\n",
      "           9.3517e-02,  1.0352e-01],\n",
      "         [-1.0279e-02, -8.7609e-02,  4.9097e-03,  ..., -1.6145e-02,\n",
      "          -8.5884e-03,  4.0168e-02]],\n",
      "\n",
      "        [[ 1.2445e-01, -2.3113e-01, -2.3602e-01,  ...,  2.9269e-01,\n",
      "           3.2545e-01,  3.4910e-01],\n",
      "         [ 3.9973e-02, -2.6143e-01,  8.3983e-02,  ...,  1.4909e-01,\n",
      "           1.8539e-01,  3.5900e-01],\n",
      "         [ 1.1115e-01, -1.1139e-01, -1.3701e-01,  ..., -2.2539e-01,\n",
      "           1.4283e-02,  7.3366e-02],\n",
      "         ...,\n",
      "         [-2.0387e-01, -7.6420e-02,  3.7006e-02,  ..., -1.1626e-01,\n",
      "           2.0864e-02,  2.6999e-01],\n",
      "         [-1.2999e-01, -8.3637e-02,  2.0148e-02,  ..., -1.1530e-01,\n",
      "          -2.8728e-01,  1.4261e-01],\n",
      "         [-2.1702e-03, -5.8295e-02, -2.3955e-03,  ..., -3.1265e-02,\n",
      "          -4.8293e-02, -5.0737e-02]]], device='mps:0')\n",
      "sequence_output linear tensor([[[ 1.2375e-01, -4.8629e-01,  3.9872e-01,  ...,  1.6893e-01,\n",
      "           7.3654e-02,  7.1800e-02],\n",
      "         [-1.8832e-02, -2.8825e-01,  7.4608e-02,  ...,  1.7238e-01,\n",
      "           3.1314e-01,  5.7781e-02],\n",
      "         [ 1.1225e-01,  6.0611e-02, -6.5913e-02,  ..., -1.1262e-01,\n",
      "          -3.5694e-02, -3.4978e-01],\n",
      "         ...,\n",
      "         [-2.6904e-01, -1.3911e-01, -5.7208e-02,  ..., -5.2893e-02,\n",
      "          -1.4785e-01, -4.1311e-01],\n",
      "         [-1.2000e-01,  9.4425e-02,  1.4164e-01,  ...,  1.8941e-01,\n",
      "           3.2264e-04, -3.1129e-01],\n",
      "         [-2.6453e-02, -8.9668e-02,  4.0289e-02,  ...,  9.9320e-03,\n",
      "          -1.0007e-01,  3.5352e-02]],\n",
      "\n",
      "        [[ 2.7390e-01, -1.3383e-01,  6.3151e-02,  ...,  2.4540e-01,\n",
      "           2.5042e-01,  1.1205e-01],\n",
      "         [ 2.2027e-02, -3.3821e-01,  1.8124e-02,  ...,  8.7925e-02,\n",
      "          -4.6127e-02,  1.2867e-01],\n",
      "         [ 5.6413e-01, -2.1803e-01, -1.9680e-03,  ...,  2.4934e-02,\n",
      "           3.6739e-01, -1.6236e-01],\n",
      "         ...,\n",
      "         [-1.4055e-01, -1.4510e-01, -1.5750e-01,  ..., -4.2653e-02,\n",
      "           2.2649e-01, -5.7282e-02],\n",
      "         [ 2.4534e-01, -8.7595e-03,  2.8439e-04,  ..., -3.1259e-01,\n",
      "           9.3517e-02,  1.0352e-01],\n",
      "         [-1.0279e-02, -8.7609e-02,  4.9097e-03,  ..., -1.6145e-02,\n",
      "          -8.5884e-03,  4.0168e-02]],\n",
      "\n",
      "        [[ 1.2445e-01, -2.3113e-01, -2.3602e-01,  ...,  2.9269e-01,\n",
      "           3.2545e-01,  3.4910e-01],\n",
      "         [ 3.9973e-02, -2.6143e-01,  8.3983e-02,  ...,  1.4909e-01,\n",
      "           1.8539e-01,  3.5900e-01],\n",
      "         [ 1.1115e-01, -1.1139e-01, -1.3701e-01,  ..., -2.2539e-01,\n",
      "           1.4283e-02,  7.3366e-02],\n",
      "         ...,\n",
      "         [-2.0387e-01, -7.6420e-02,  3.7006e-02,  ..., -1.1626e-01,\n",
      "           2.0864e-02,  2.6999e-01],\n",
      "         [-1.2999e-01, -8.3637e-02,  2.0148e-02,  ..., -1.1530e-01,\n",
      "          -2.8728e-01,  1.4261e-01],\n",
      "         [-2.1702e-03, -5.8295e-02, -2.3955e-03,  ..., -3.1265e-02,\n",
      "          -4.8293e-02, -5.0737e-02]]], device='mps:0')\n",
      "found labels\n",
      "logits.device mps:0\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100]],\n",
      "       device='mps:0')\n",
      "loss tensor(1.7906, device='mps:0')\n",
      "loss tensor(1.7906, device='mps:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501da9a4da7c453e89db6354abe8df03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.790602684020996, 'eval_accuracy_metric': 0.14084507042253522, 'eval_precision_metric': 0.14084507042253522, 'eval_recall_metric': 0.14084507042253522, 'eval_f1_metric': 0.14084507042253522, 'eval_matthews_correlation': 0.013285066517684194, 'eval_runtime': 19.0725, 'eval_samples_per_second': 0.157, 'eval_steps_per_second': 0.052, 'epoch': 1.0}\n",
      "{'train_runtime': 72.3093, 'train_samples_per_second': 0.124, 'train_steps_per_second': 0.014, 'train_loss': 1.7924234867095947, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=1.7924234867095947, metrics={'train_runtime': 72.3093, 'train_samples_per_second': 0.124, 'train_steps_per_second': 0.014, 'train_loss': 1.7924234867095947, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics=trainer.evaluate()\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_log = pd.DataFrame(trainer.state.log_history)\n",
    "display(result_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_location = '/models/testing_2'\n",
    "t5_lora_model.save_pretrained(ROOT + adapter_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(sequence: str, tokenizer: T5Tokenizer, model):\n",
    "    # print('sequence', sequence)\n",
    "    tokenized_string = tokenizer.encode(sequence, padding=True, truncation=True, return_tensors=\"pt\", max_length=1024)\n",
    "    # print('tokenized_string', tokenized_string)\n",
    "    with torch.no_grad():\n",
    "        output = model(tokenized_string.to(device))\n",
    "    # print('output', output)\n",
    "    return output\n",
    "\n",
    "def tranlate_logits(logits):\n",
    "    return [src.config.label_decoding[x] for x in logits.argmax(-1).tolist()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ds_index = 2\n",
    "_ds_type = 'test'\n",
    "\n",
    "_inids_test = t5_tokenizer.decode(dataset_signalp[_ds_type][_ds_index]['input_ids'])\n",
    "_labels_test = dataset_signalp[_ds_type][_ds_index]['labels']\n",
    "_labels_test_decoded = [src.config.label_decoding[x] for x in _labels_test]\n",
    "print(_inids_test)\n",
    "print(_labels_test)\n",
    "print(_labels_test_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict_model(_inids_test, t5_tokenizer, t5_lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_res = tranlate_logits(preds.logits.cpu().numpy())\n",
    "print(_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
