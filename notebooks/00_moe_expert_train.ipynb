{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.config\n",
    "import src.data\n",
    "import src.model_new\n",
    "import src.utils\n",
    "from src.model_new import (\n",
    "    T5EncoderModelForTokenClassification,\n",
    ")\n",
    "\n",
    "import gc\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import src.utils\n",
    "\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback\n",
    ")\n",
    "\n",
    "import peft\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model:\t Rostlab/prot_t5_xl_uniref50\n",
      "MPS:\t\t False\n",
      "Path:\t\t /home/ec2-user/developer/prottrans-t5-signalpeptide-prediction\n",
      "Using device:\t cuda:0\n"
     ]
    }
   ],
   "source": [
    "ROOT = src.utils.get_project_root_path()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Base Model:\\t\", src.config.base_model_name)\n",
    "print(\"MPS:\\t\\t\", torch.backends.mps.is_available())\n",
    "print(\"Path:\\t\\t\", ROOT)\n",
    "print(f\"Using device:\\t {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=src.config.base_model_name,\n",
    "    do_lower_case=False,\n",
    "    use_fast=True,\n",
    "    legacy=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTA_FILENAME = '5_SignalP_5.0_Training_set.fasta'\n",
    "# FASTA_FILENAME = '5_SignalP_5.0_Training_set_testing.fasta'\n",
    "annotations_name = 'Label' # Choose Type or Label\n",
    "\n",
    "df_data = src.data.process(src.data.parse_file(ROOT + '/data/raw/' + FASTA_FILENAME))\n",
    "\n",
    "dataset_signalp_type_splits = {}\n",
    "\n",
    "for sequence_type in src.config.type_encoding.keys():\n",
    "    dataset_signalp = src.model_new.create_datasets(\n",
    "        splits=src.config.splits,\n",
    "        tokenizer=t5_tokenizer,\n",
    "        data=df_data,\n",
    "        annotations_name=annotations_name,\n",
    "        dataset_size=src.config.dataset_size,\n",
    "        # dataset_size=3,\n",
    "        encoder=src.config.select_encoding_type[sequence_type],\n",
    "        sequence_type=sequence_type\n",
    "        )\n",
    "    dataset_signalp_type_splits.update({sequence_type: dataset_signalp})\n",
    "\n",
    "del df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2017\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 679\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 676\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset_signalp_type_splits\n",
    "# dataset_signalp_type_splits['TAT']['train'][200]['labels']\n",
    "\n",
    "expert = 'SP'\n",
    "dataset_signalp = dataset_signalp_type_splits[expert]\n",
    "# len(src.config.select_decoding_type[expert])\n",
    "dataset_signalp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5EncoderModelForTokenClassification were not initialized from the model checkpoint at Rostlab/prot_t5_xl_uniref50 and are newly initialized: ['custom_classifier.weight', 'custom_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "t5_base_model = T5EncoderModelForTokenClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=src.config.base_model_name,\n",
    "    device_map='auto',\n",
    "    load_in_8bit=False,\n",
    "    custom_num_labels=len(src.config.select_decoding_type[expert]),\n",
    "    custom_dropout_rate=0.1,\n",
    ")\n",
    "\n",
    "tmp_lin = nn.Linear(\n",
    "    in_features=t5_base_model.config.hidden_size,\n",
    "    out_features=t5_base_model.custom_num_labels\n",
    ")\n",
    "t5_base_model.custom_classifier.weight = tmp_lin.weight\n",
    "t5_base_model.custom_classifier.bias = tmp_lin.bias\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q', 'k', 'v', 'o'],\n",
    "    bias=\"none\",\n",
    "    modules_to_save=['custom_classifier'],\n",
    ")\n",
    "\n",
    "t5_lora_model = peft.get_peft_model(t5_base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0239,  0.0259, -0.0073,  ..., -0.0119, -0.0269, -0.0198],\n",
       "          [ 0.0174, -0.0047,  0.0133,  ...,  0.0008,  0.0057,  0.0295],\n",
       "          [ 0.0145,  0.0067, -0.0063,  ...,  0.0145, -0.0186,  0.0159],\n",
       "          [-0.0079,  0.0028, -0.0084,  ...,  0.0109, -0.0184,  0.0002]],\n",
       "         requires_grad=True)),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0221,  0.0252,  0.0264,  0.0246], requires_grad=True))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in t5_lora_model.custom_classifier.modules_to_save.default.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0002, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_lora_model.custom_classifier.modules_to_save.default.weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0135, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_lora_model.custom_classifier.modules_to_save.default.bias.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=t5_tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./checkpoints',\n",
    "    learning_rate=src.config.lr,\n",
    "    per_device_train_batch_size=src.config.batch_size,\n",
    "    per_device_eval_batch_size=src.config.batch_size,\n",
    "    num_train_epochs=src.config.num_epochs,\n",
    "    logging_steps=src.config.logging_steps,\n",
    "    # save_strategy=\"steps\",\n",
    "    # save_steps=src.config.save_steps,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=src.config.eval_steps,\n",
    "    # gradient_accumulation_steps=accum,\n",
    "    # load_best_model_at_end=True,\n",
    "    # save_total_limit=5,\n",
    "    seed=42,\n",
    "    # fp16=True,\n",
    "    # deepspeed=deepspeed_config,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=['labels'],\n",
    "    # debug=\"underflow_overflow\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=t5_lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_signalp['train'],\n",
    "    eval_dataset=dataset_signalp['valid'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=src.model_new.compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4709, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.4675, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='127' max='127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [127/127 03:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.383700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.378200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.377100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.366800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.354100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.352200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.345500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.345200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.343100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.324700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.305400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.290900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.291400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.279300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.277400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.256200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.244600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.227400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.221400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.217700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.195900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.136200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.145800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.132100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.111700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.078500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.093300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.983700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.980600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.926500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.904100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.899600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.897200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.866300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.840300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.842300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.821900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.774700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.846800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.753200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.763000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.694700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.704300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.670200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.632100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.561600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.604600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.574100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.588800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.508300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.569900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.458800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.505400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.544200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.413700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.401700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.379200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.439400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.443200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.418900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.332900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.354500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.311400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.333300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.336900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.300700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.285700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.266900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.390800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.258200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.292500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.224100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.243900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.259100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.216100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.263600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.194400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.200800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4600, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.5402, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.4716, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.4788, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.4392, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.8139, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.4940, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.6035, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.4519, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.4354, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.4271, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.5726, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.5432, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.5368, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.4870, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.5344, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.5050, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.5965, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.5002, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.4581, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.5424, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.6941, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.5653, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.5503, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.5171, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.6273, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.5521, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.7221, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.5640, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.5368, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.5613, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.5447, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.5684, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.6119, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.7281, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.5630, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.5585, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.5926, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.5619, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.6048, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.5641, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.6237, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.5809, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.5904, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.5595, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.5956, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.7122, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.5703, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.5621, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.6128, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.6188, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.6913, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.6659, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.6829, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.6400, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.7171, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.6012, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.7673, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.6565, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.7418, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.6229, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.7654, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.6860, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.7608, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.6071, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.8342, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.7493, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.8434, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.6803, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.7971, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.6782, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.9046, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.7514, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.8983, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.7485, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.8483, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.7088, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.8894, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.6743, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.8875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.8483, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.0496, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.8024, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.0219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.7587, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.0032, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.7657, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.0713, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.8619, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.0788, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.9005, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.1477, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.8631, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.1185, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.9551, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.2040, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.8359, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.1906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.9377, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.1222, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.9980, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.4247, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.9452, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.2827, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.9609, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.3552, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-0.9823, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.3604, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.2413, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.3770, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.0275, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.4436, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.0714, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.3778, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.0903, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.4096, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.1383, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.4149, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.1454, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.4848, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.1283, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.5353, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.1365, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.4464, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.1411, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.5029, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.1266, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.4863, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.1720, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.5998, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.1186, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.6692, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.1542, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.6286, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.1910, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.6670, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.1907, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.6459, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.1764, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.6605, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.2356, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.6735, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.1904, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.7765, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.2226, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.6802, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.2193, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.7391, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.2322, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.8022, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.1815, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.7993, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.2860, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.8359, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.2633, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.8494, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.1826, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.8544, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3918, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.9047, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.2627, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.9792, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3048, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.9999, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3069, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.0136, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3510, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.0138, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4210, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.9814, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.2754, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.0577, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3649, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.0548, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.2930, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.0257, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3089, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.1717, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3252, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.0624, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3676, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.0646, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3544, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.2515, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3208, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.1616, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4236, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.1729, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4281, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.2159, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3249, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.2153, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3926, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.2062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4104, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.2874, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4303, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.2586, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4805, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.2813, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4894, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.3696, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.5326, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.3787, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4164, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.3554, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4293, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.3940, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.5064, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.3599, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.5024, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.4099, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4607, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.5328, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4227, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.4499, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4415, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.3654, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4183, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.4674, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4937, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.5055, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4381, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.5756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4680, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.5197, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4878, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.4835, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4265, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.5079, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.5592, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.5014, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4733, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.5491, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.6118, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.6348, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.5125, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.5461, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.5772, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.5683, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.5310, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.4714, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.5550, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.5678, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4992, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.5089, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.5184, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.6303, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4670, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.5815, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4789, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.4601, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=127, training_loss=0.7657558720881544, metrics={'train_runtime': 184.6188, 'train_samples_per_second': 10.925, 'train_steps_per_second': 0.688, 'total_flos': 1041358417513296.0, 'train_loss': 0.7657558720881544, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0187,  0.0257, -0.0138,  ..., -0.0137, -0.0291, -0.0223],\n",
       "          [ 0.0132, -0.0048,  0.0069,  ..., -0.0011,  0.0031,  0.0270],\n",
       "          [ 0.0205,  0.0038,  0.0005,  ...,  0.0159, -0.0243,  0.0217],\n",
       "          [-0.0132,  0.0063, -0.0045,  ...,  0.0129, -0.0137, -0.0055]],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0274,  0.0199,  0.0318,  0.0287], device='cuda:0',\n",
       "         requires_grad=True))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in t5_lora_model.custom_classifier.modules_to_save.default.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.4134, device='cuda:0') tensor(2.5316, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43' max='43' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43/43 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.3909, device='cuda:0') tensor(2.5564, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4554, device='cuda:0') tensor(2.5889, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.5278, device='cuda:0') tensor(2.5794, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.5032, device='cuda:0') tensor(2.5227, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3825, device='cuda:0') tensor(2.5083, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4368, device='cuda:0') tensor(2.5718, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3962, device='cuda:0') tensor(2.6592, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3820, device='cuda:0') tensor(2.5595, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3873, device='cuda:0') tensor(2.4675, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4296, device='cuda:0') tensor(2.3423, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4261, device='cuda:0') tensor(2.5297, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4800, device='cuda:0') tensor(2.4352, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4136, device='cuda:0') tensor(2.5168, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3479, device='cuda:0') tensor(2.5242, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3605, device='cuda:0') tensor(2.4628, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3518, device='cuda:0') tensor(2.4656, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3948, device='cuda:0') tensor(2.5651, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3904, device='cuda:0') tensor(2.5175, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3337, device='cuda:0') tensor(2.4888, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4135, device='cuda:0') tensor(2.5015, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4633, device='cuda:0') tensor(2.4699, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3431, device='cuda:0') tensor(2.5277, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3945, device='cuda:0') tensor(2.5942, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.5085, device='cuda:0') tensor(2.4960, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3437, device='cuda:0') tensor(2.5385, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3866, device='cuda:0') tensor(2.4619, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4267, device='cuda:0') tensor(2.4841, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.5066, device='cuda:0') tensor(2.5457, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3440, device='cuda:0') tensor(2.4567, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3538, device='cuda:0') tensor(2.5101, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3996, device='cuda:0') tensor(2.5011, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4921, device='cuda:0') tensor(2.5009, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3952, device='cuda:0') tensor(2.4159, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4849, device='cuda:0') tensor(2.4982, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3522, device='cuda:0') tensor(2.4557, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3568, device='cuda:0') tensor(2.4653, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.4065, device='cuda:0') tensor(2.3934, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3540, device='cuda:0') tensor(2.4148, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3612, device='cuda:0') tensor(2.4116, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3480, device='cuda:0') tensor(2.3995, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.3381, device='cuda:0') tensor(2.3423, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(-1.2782, device='cuda:0') tensor(2.2842, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "{'eval_loss': 0.20727592706680298, 'eval_accuracy_metric': 0.9781465387647573, 'eval_precision_metric': 0.9781465387647573, 'eval_recall_metric': 0.9781465387647573, 'eval_f1_metric': 0.9781465387647573, 'eval_matthews_correlation': 0.961126926646345, 'eval_confusion_matrix': array([[    0,     0,     0,     0],\n",
      "       [    0,     0,     0,     0],\n",
      "       [  107,   207, 30668,   441],\n",
      "       [    0,     0,   265, 15611]]), 'eval_runtime': 43.4999, 'eval_samples_per_second': 15.609, 'eval_steps_per_second': 0.989, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "metrics=trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy_metric</th>\n",
       "      <th>eval_precision_metric</th>\n",
       "      <th>eval_recall_metric</th>\n",
       "      <th>eval_f1_metric</th>\n",
       "      <th>eval_matthews_correlation</th>\n",
       "      <th>eval_confusion_matrix</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.3837</td>\n",
       "      <td>9.921260e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3855</td>\n",
       "      <td>9.842520e-05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3782</td>\n",
       "      <td>9.763780e-05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.3699</td>\n",
       "      <td>9.685039e-05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.3771</td>\n",
       "      <td>9.606299e-05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.2088</td>\n",
       "      <td>1.574803e-06</td>\n",
       "      <td>0.98</td>\n",
       "      <td>125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.2245</td>\n",
       "      <td>7.874016e-07</td>\n",
       "      <td>0.99</td>\n",
       "      <td>126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.2008</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>127</td>\n",
       "      <td>184.6188</td>\n",
       "      <td>10.925</td>\n",
       "      <td>0.688</td>\n",
       "      <td>1.041358e+15</td>\n",
       "      <td>0.765756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.207276</td>\n",
       "      <td>0.978147</td>\n",
       "      <td>0.978147</td>\n",
       "      <td>0.978147</td>\n",
       "      <td>0.978147</td>\n",
       "      <td>0.961127</td>\n",
       "      <td>[[0, 0, 0, 0], [0, 0, 0, 0], [107, 207, 30668,...</td>\n",
       "      <td>43.4999</td>\n",
       "      <td>15.609</td>\n",
       "      <td>0.989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows  19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  learning_rate  epoch  step  train_runtime  \\\n",
       "0    1.3837   9.921260e-05   0.01     1            NaN   \n",
       "1    1.3855   9.842520e-05   0.02     2            NaN   \n",
       "2    1.3782   9.763780e-05   0.02     3            NaN   \n",
       "3    1.3699   9.685039e-05   0.03     4            NaN   \n",
       "4    1.3771   9.606299e-05   0.04     5            NaN   \n",
       "..      ...            ...    ...   ...            ...   \n",
       "124  0.2088   1.574803e-06   0.98   125            NaN   \n",
       "125  0.2245   7.874016e-07   0.99   126            NaN   \n",
       "126  0.2008   0.000000e+00   1.00   127            NaN   \n",
       "127     NaN            NaN   1.00   127       184.6188   \n",
       "128     NaN            NaN   1.00   127            NaN   \n",
       "\n",
       "     train_samples_per_second  train_steps_per_second    total_flos  \\\n",
       "0                         NaN                     NaN           NaN   \n",
       "1                         NaN                     NaN           NaN   \n",
       "2                         NaN                     NaN           NaN   \n",
       "3                         NaN                     NaN           NaN   \n",
       "4                         NaN                     NaN           NaN   \n",
       "..                        ...                     ...           ...   \n",
       "124                       NaN                     NaN           NaN   \n",
       "125                       NaN                     NaN           NaN   \n",
       "126                       NaN                     NaN           NaN   \n",
       "127                    10.925                   0.688  1.041358e+15   \n",
       "128                       NaN                     NaN           NaN   \n",
       "\n",
       "     train_loss  eval_loss  eval_accuracy_metric  eval_precision_metric  \\\n",
       "0           NaN        NaN                   NaN                    NaN   \n",
       "1           NaN        NaN                   NaN                    NaN   \n",
       "2           NaN        NaN                   NaN                    NaN   \n",
       "3           NaN        NaN                   NaN                    NaN   \n",
       "4           NaN        NaN                   NaN                    NaN   \n",
       "..          ...        ...                   ...                    ...   \n",
       "124         NaN        NaN                   NaN                    NaN   \n",
       "125         NaN        NaN                   NaN                    NaN   \n",
       "126         NaN        NaN                   NaN                    NaN   \n",
       "127    0.765756        NaN                   NaN                    NaN   \n",
       "128         NaN   0.207276              0.978147               0.978147   \n",
       "\n",
       "     eval_recall_metric  eval_f1_metric  eval_matthews_correlation  \\\n",
       "0                   NaN             NaN                        NaN   \n",
       "1                   NaN             NaN                        NaN   \n",
       "2                   NaN             NaN                        NaN   \n",
       "3                   NaN             NaN                        NaN   \n",
       "4                   NaN             NaN                        NaN   \n",
       "..                  ...             ...                        ...   \n",
       "124                 NaN             NaN                        NaN   \n",
       "125                 NaN             NaN                        NaN   \n",
       "126                 NaN             NaN                        NaN   \n",
       "127                 NaN             NaN                        NaN   \n",
       "128            0.978147        0.978147                   0.961127   \n",
       "\n",
       "                                 eval_confusion_matrix  eval_runtime  \\\n",
       "0                                                  NaN           NaN   \n",
       "1                                                  NaN           NaN   \n",
       "2                                                  NaN           NaN   \n",
       "3                                                  NaN           NaN   \n",
       "4                                                  NaN           NaN   \n",
       "..                                                 ...           ...   \n",
       "124                                                NaN           NaN   \n",
       "125                                                NaN           NaN   \n",
       "126                                                NaN           NaN   \n",
       "127                                                NaN           NaN   \n",
       "128  [[0, 0, 0, 0], [0, 0, 0, 0], [107, 207, 30668,...       43.4999   \n",
       "\n",
       "     eval_samples_per_second  eval_steps_per_second  \n",
       "0                        NaN                    NaN  \n",
       "1                        NaN                    NaN  \n",
       "2                        NaN                    NaN  \n",
       "3                        NaN                    NaN  \n",
       "4                        NaN                    NaN  \n",
       "..                       ...                    ...  \n",
       "124                      NaN                    NaN  \n",
       "125                      NaN                    NaN  \n",
       "126                      NaN                    NaN  \n",
       "127                      NaN                    NaN  \n",
       "128                   15.609                  0.989  \n",
       "\n",
       "[129 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_log = pd.DataFrame(trainer.state.log_history)\n",
    "display(training_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0],\n",
       "       [    0,     0,     0,     0],\n",
       "       [  107,   207, 30668,   441],\n",
       "       [    0,     0,   265, 15611]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_log['eval_confusion_matrix'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    92502\n",
       "3    47167\n",
       "1      617\n",
       "0      314\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2    30933\n",
       "3    16052\n",
       "1      207\n",
       "0      107\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2    31236\n",
       "3    15677\n",
       "1      112\n",
       "0      100\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.Series([item for row in dataset_signalp['train']['labels'] for item in row]).value_counts())\n",
    "display(pd.Series([item for row in dataset_signalp['valid']['labels'] for item in row]).value_counts())\n",
    "display(pd.Series([item for row in dataset_signalp['test']['labels'] for item in row]).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'I', 1: 'M', 2: 'O', 3: 'S'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Confusion Matrix'}, xlabel='Actual', ylabel='Predicted'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "src.model_new.make_confusion_matrix(\n",
    "    training_log['eval_confusion_matrix'].iloc[-1],\n",
    "    src.config.select_decoding_type[expert])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log = pd.DataFrame(trainer.state.log_history)\n",
    "if 'eval_confusion_matrix' in training_log.columns:\n",
    "    training_log['eval_confusion_matrix'] = training_log['eval_confusion_matrix'].apply(lambda x: x.tolist() if type(x)==np.ndarray else None)\n",
    "adapter_location = '/models/expert_testing_1/' + expert + '_expert/'\n",
    "t5_lora_model.save_pretrained(ROOT + adapter_location)\n",
    "training_log.to_csv(ROOT + adapter_location + '/training_log.csv', index=False)\n",
    "training_log.to_parquet(ROOT + adapter_location + '/training_log.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ds_index = 2\n",
    "_ds_type = 'test'\n",
    "\n",
    "_input_ids_test = t5_tokenizer.decode(dataset_signalp[_ds_type][_ds_index]['input_ids'])\n",
    "_labels_test = torch.tensor(dataset_signalp[_ds_type][_ds_index]['labels'] + [-100]).to(device)\n",
    "_attention_mask_test = torch.tensor([dataset_signalp[_ds_type][_ds_index]['attention_mask']]).to(device)\n",
    "\n",
    "_labels_test_decoded = [src.config.label_decoding[x] for x in _labels_test.tolist()[:-1]]\n",
    "print('Iput IDs:\\t', _input_ids_test)\n",
    "print('Labels:\\t\\t', *_labels_test.tolist())\n",
    "print('Labels Decoded:\\t', *_labels_test_decoded)\n",
    "print('Attention Mask:\\t', *_attention_mask_test.tolist()[0])\n",
    "print('----')\n",
    "\n",
    "preds = src.model_new.predict_model(\n",
    "    sequence=_input_ids_test,\n",
    "    tokenizer=t5_tokenizer,\n",
    "    model=t5_lora_model,\n",
    "    labels=_labels_test,\n",
    "    attention_mask=_attention_mask_test,\n",
    "    device=device,\n",
    "    )\n",
    "\n",
    "_result = src.model_new.translate_logits(preds.logits.cpu().numpy())\n",
    "print('Result: \\t',* _result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
