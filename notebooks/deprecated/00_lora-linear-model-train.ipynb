{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup, Packages, Setup, and Variables\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import types\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import (\n",
    "    CrossEntropyLoss,\n",
    "    MSELoss\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoConfig,\n",
    "    T5EncoderModel,\n",
    "    T5Tokenizer,\n",
    "    T5PreTrainedModel,\n",
    "    T5ForConditionalGeneration,\n",
    "    pipeline,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    "    EvalPrediction,\n",
    "    )\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    prepare_model_for_kbit_training\n",
    "    )\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import src.config as config\n",
    "\n",
    "from src.model import (\n",
    "    get_prottrans_tokenizer_model,\n",
    "    df_to_dataset,\n",
    "    inject_linear_layer,\n",
    "    compute_metrics_fast\n",
    "    )\n",
    "from src.utils import get_project_root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model_name = config.base_model_name\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "ROOT = get_project_root_path()\n",
    "\n",
    "if config.VERBOSE:\n",
    "    print(\"Base Model:\\t\", base_model_name)\n",
    "    print(\"MPS:\\t\\t\", torch.backends.mps.is_available())\n",
    "    print(\"Path:\\t\\t\", ROOT)\n",
    "    print(f\"Using device:\\t {device}\")\n",
    "\n",
    "pd.set_option('display.max_colwidth',3000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Tokenizer and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_architecture = T5EncoderModel\n",
    "t5_tokenizer, t5_base_model = get_prottrans_tokenizer_model(base_model_name, model_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_example = 'MSLSRREFVKLCSAGVAGLGISQIY'\n",
    "# print(len(sequence_example))\n",
    "# sequence_example = \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence_example)))\n",
    "\n",
    "# print(t5_tokenizer.decode([4, 7, 7, 10, 8, 1]))\n",
    "# print(t5_tokenizer.decode([4, 7, 7, 10, 8]))\n",
    "\n",
    "# t_tensor = torch.tensor([4, 7, 7, 10, 8, 1]).to('mps')\n",
    "# t_tensor = t_tensor.unsqueeze(0)\n",
    "# with torch.no_grad():\n",
    "#     res = t5_base_model(input_ids=t_tensor)\n",
    "# print(res)\n",
    "# print(res.last_hidden_state.argmax(-1))\n",
    "\n",
    "# t_tensor = torch.tensor([4, 7, 7, 10, 8]).to('mps')\n",
    "# t_tensor = t_tensor.unsqueeze(0)\n",
    "# with torch.no_grad():\n",
    "#     res = t5_base_model(input_ids=t_tensor)\n",
    "# print(res)\n",
    "# print(res.last_hidden_state.argmax(dim=-1))\n",
    "\n",
    "# tkns = t5_tokenizer.batch_encode_plus([sequence_example, sequence_example[::-1]], padding=True, return_tensors=\"pt\")\n",
    "# tkns.to(device)\n",
    "# print(tkns.input_ids)\n",
    "\n",
    "# print(tkns.input_ids.shape)\n",
    "# print(tkns.input_ids[0])\n",
    "# print(tkns.attention_mask[0][:-1].shape)\n",
    "\n",
    "# t_tensor_input = torch.tensor([[4, 7, 7, 12, 1, 1]]).to('mps')\n",
    "# t_tensor_mask = torch.tensor([[1, 1, 1, 1, 1, 0]]).to('mps')\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     res = t5_base_model(\n",
    "#         input_ids=t_tensor_input,\n",
    "#         attention_mask=t_tensor_mask,\n",
    "#         )\n",
    "# print(res.last_hidden_state.shape)\n",
    "# print(res.last_hidden_state.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Data, Split into Dataset, and Tokenize Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_parquet(ROOT + '/data/processed/5.0_train.parquet.gzip')\n",
    "if config.VERBOSE:\n",
    "    df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_train = df_data[df_data.Split.isin([0, 1, 2])].head(config.dataset_size*3 if config.dataset_size else None)\n",
    "ds_train = df_to_dataset(\n",
    "    t5_tokenizer,\n",
    "    ds_train.Sequence.to_list(),\n",
    "    ds_train.Label.to_list(),\n",
    ")\n",
    "\n",
    "ds_validate = df_data[df_data.Split.isin([3])].head(config.dataset_size)\n",
    "ds_validate = df_to_dataset(\n",
    "    t5_tokenizer,\n",
    "    ds_validate.Sequence.to_list(),\n",
    "    ds_validate.Label.to_list(),\n",
    ")\n",
    "\n",
    "ds_test = df_data[df_data.Split.isin([4])].head(config.dataset_size)\n",
    "ds_test = df_to_dataset(\n",
    "    t5_tokenizer,\n",
    "    ds_test.Sequence.to_list(),\n",
    "    ds_test.Label.to_list()\n",
    ")\n",
    "\n",
    "dataset_signalp = DatasetDict({\n",
    "    'train': ds_train,\n",
    "    'valid': ds_validate,\n",
    "    'test': ds_test\n",
    "        })\n",
    "\n",
    "del df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.VERBOSE:\n",
    "    print(ds_train)\n",
    "    print(ds_validate)\n",
    "    print(ds_test)\n",
    "    print('----------------------------------')\n",
    "    print(ds_test[0]['input_ids'])\n",
    "    print(len(ds_test[0]['input_ids']))\n",
    "    print(ds_test[0]['attention_mask'])\n",
    "    print(len(ds_test[0]['attention_mask']))\n",
    "    print(ds_test[0]['labels'])\n",
    "    print(len(ds_test[0]['labels']))\n",
    "    print('----------------------------------')\n",
    "    print(t5_tokenizer.decode(ds_test[0]['input_ids']))\n",
    "    print(t5_tokenizer.decode(range(0, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_examples = [\"PRTEINO\", \"SEQWENCE\"]\n",
    "# sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
    "# ids = t5_tokenizer.batch_encode_plus(sequence_examples, add_special_tokens=True, padding=\"longest\")\n",
    "# input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "# attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "# print(input_ids)\n",
    "# print(attention_mask)\n",
    "# with torch.no_grad():\n",
    "#     embedding_repr = t5_base_model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "\n",
    "# emb_0 = embedding_repr.last_hidden_state[0,:7]\n",
    "# print(f\"Shape of per-residue embedding of first sequences: {emb_0.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        task_type=TaskType.FEATURE_EXTRACTION,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        # target_modules=['q', 'k', 'v', 'o'],\n",
    "        target_modules=['o'],\n",
    "        bias=\"none\",\n",
    "    )\n",
    "t5_lora_model = get_peft_model(t5_base_model, lora_config)\n",
    "# t5_lora_model = prepare_model_for_kbit_training(t5_lora_model) # add quantization\n",
    "# t5_lora_model = t5_base_model\n",
    "t5_lora_model.print_trainable_parameters()\n",
    "# del t5_base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.VERBOSE:\n",
    "    print(t5_lora_model)\n",
    "    print()\n",
    "    print(t5_lora_model.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_tensor_input = torch.tensor([[4, 7, 7, 12, 10, 8]]).to('mps')\n",
    "# t_tensor_mask = torch.tensor([[1, 1, 1, 1, 0, 0]]).to('mps')\n",
    "\n",
    "# t_tensor_input = torch.tensor([[4, 7, 7, 12, 10, 8]]).to('mps')\n",
    "# t_tensor_mask = torch.tensor([[1, 1, 1, 1, 0, 0]]).to('mps')\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     res = t5_lora_model(\n",
    "#         input_ids=t_tensor,\n",
    "#         attention_mask=t_tensor_mask,\n",
    "#         )\n",
    "# print(res)\n",
    "# print(res.last_hidden_state.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Loop\n",
    "https://huggingface.co/docs/peft/task_guides/token-classification-lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=t5_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model = inject_linear_layer(\n",
    "    t5_lora_model=t5_lora_model,\n",
    "    num_labels=len(config.label_decoding),\n",
    "    dropout_rate=config.dropout_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_lora_model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "# def compute_metrics_custom(eval_preds: EvalPrediction):\n",
    "#     print(*eval_preds)\n",
    "#     logits, labels = eval_preds\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./checkpoints',\n",
    "    learning_rate=config.lr,\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    per_device_eval_batch_size=config.batch_size,\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    logging_steps=config.logging_steps,\n",
    "    # save_strategy=\"steps\",\n",
    "    save_steps=config.save_steps,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=config.eval_steps,\n",
    "    # load_best_model_at_end=True,\n",
    "    # save_total_limit=5,\n",
    "    seed=42,\n",
    "    # fp16=True,\n",
    "    # deepspeed=deepspeed_config\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=t5_lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_signalp['train'],\n",
    "    # eval_dataset=ds_validate,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=config.metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ([set(x) for x in ds_validate['labels']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_validate['labels'][\n",
    "#     [set(x) for x in ds_validate['labels']] != {0}\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(next(t5_lora_model.parameters()).is_cuda)\n",
    "print(t5_lora_model.device)\n",
    "# print(config.label_decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t5_lora_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics=trainer.evaluate()\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_validate['labels'][9].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ds_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = torch.zeros(4148,70)\n",
    "# target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x + [-1] * (70-len(x)) for x in ds_validate['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inlab = torch.tensor([x + [-1] * (70-len(x)) for x in ds_validate['labels']]).to('cpu')\n",
    "# inlab.shape\n",
    "# del inlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inid = torch.tensor(ds_validate['input_ids']).to(device)\n",
    "# inid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = t5_lora_model(input_ids=inid[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "# for index, _ in enumerate(inid):\n",
    "#     if index % 100 == 0:\n",
    "#         torch.cuda.empty_cache()\n",
    "#     results += t5_lora_model(input_ids=inid[index:index+1]).logits.argmax(dim=-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_labels = [[config.label_decoding[y] for y in x] for x in ds_validate['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(correct_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct = 0\n",
    "# incorrect = 0\n",
    "\n",
    "# for index, item in enumerate(results):\n",
    "#     truth = correct_labels[index]\n",
    "#     prediction = [config.label_decoding[x] for x in item[:len(correct_labels[index])]]\n",
    "    \n",
    "#     if index % 50 == 0:\n",
    "#         print(*truth, sep='')\n",
    "#         print(*prediction, sep='')\n",
    "#         print()\n",
    "    \n",
    "#     for t, p in zip(truth, prediction):\n",
    "#         if t == p:\n",
    "#             correct += 1\n",
    "#         else:\n",
    "#             incorrect += 1\n",
    "    \n",
    "\n",
    "    \n",
    "# print(\"Correct\", correct)\n",
    "# print(\"Incorrect\", incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(correct/(correct+incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*prediction[0].argmax(dim=-1)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*[config.label_decoding[x] for x in prediction[0].argmax(dim=-1)[0].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prediction[0].argmax(dim=-1))\n",
    "# print(inlab)\n",
    "# print(inlab == prediction[0].argmax(dim=-1)[:, :70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cust_pred = EvalPrediction(prediction, inlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*cust_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_metrics_custom(cust_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_log = pd.DataFrame(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "my_plot = sns.lineplot(data=result_log, x='epoch', y='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = my_plot.get_figure()\n",
    "fig.savefig(\"./plots/out.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model.save_pretrained(ROOT + f'/models/{config.model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reload Model and compare weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = config.label_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_location = '/models/linear_model_v3'\n",
    "t5_lora_model_config = PeftConfig.from_pretrained(ROOT + adapter_location)\n",
    "t5_base_model = PeftModel.from_pretrained(\n",
    "    model=t5_base_model,\n",
    "    model_id=ROOT+adapter_location,\n",
    ")\n",
    "t5_lora_model = inject_linear_layer(t5_base_model, dropout_rate=config.dropout_rate, num_labels=len(label_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
