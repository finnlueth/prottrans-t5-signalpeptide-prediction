{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProtTans Finetuning with LoRA for Signal Peptide Prediction\n",
    "\n",
    "## Links\n",
    "### Papers/ Knowledge\n",
    "- https://www.sciencedirect.com/science/article/pii/S2001037021000945\n",
    "- https://huggingface.co/blog/peft\n",
    "- https://ieeexplore.ieee.org/ielx7/34/9893033/9477085/supp1-3095381.pdf?arnumber=9477085\n",
    "### Architecture\n",
    "- https://www.philschmid.de/fine-tune-flan-t5-peft\n",
    "- https://huggingface.co/spaces/evaluate-metric/seqeval\n",
    "- https://huggingface.co/docs/transformers/v4.33.3/en/model_doc/esm#transformers.EsmForTokenClassification\n",
    "- https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/builder_classes#datasets.SplitGenerator\n",
    "- https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.Dataset.add_column\n",
    "- https://huggingface.co/docs/transformers/main_classes/data_collator\n",
    "- https://huggingface.co/docs/transformers/main/en/main_classes/trainer#checkpoints\n",
    "### Code\n",
    "- https://github.com/ziegler-ingo/cleavage_extended/blob/master/models/final/c_bilstm_t5_coteaching.ipynb\n",
    "- https://www.kaggle.com/code/henriupton/proteinet-pytorch-ems2-t5-protbert-embeddings/notebook#7.-Train-the-Model\n",
    "- https://www.kaggle.com/code/prithvijaunjale/t5-multi-label-classification\n",
    "### Optmization\n",
    "- https://huggingface.co/blog/accelerate-large-models\n",
    "- https://huggingface.co/docs/transformers/hpo_train\n",
    "\n",
    "## ToDo\n",
    "- Implement BitsAndBites (QLoRA)\n",
    "- Implement DeepSpeed\n",
    "- Fix weird extra char on inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import types\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import (\n",
    "    CrossEntropyLoss,\n",
    "    MSELoss\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoConfig,\n",
    "    T5EncoderModel,\n",
    "    T5Tokenizer,\n",
    "    T5PreTrainedModel,\n",
    "    T5ForConditionalGeneration,\n",
    "    pipeline,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    "    EvalPrediction,\n",
    "    )\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    prepare_model_for_kbit_training\n",
    "    )\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import src.config as config\n",
    "\n",
    "from src.model import (\n",
    "    get_prottrans_tokenizer_model,\n",
    "    df_to_dataset,\n",
    "    inject_linear_layer,\n",
    "    compute_metrics_fast\n",
    "    )\n",
    "from src.utils import get_project_root_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model:\t Rostlab/prot_t5_xl_uniref50\n",
      "MPS:\t\t False\n",
      "Path:\t\t /home/ec2-user/developer/prottrans-t5-signalpeptide-prediction\n",
      "Using device:\t cuda:0\n"
     ]
    }
   ],
   "source": [
    "base_model_name = config.base_model_name\n",
    "print(\"Base Model:\\t\", base_model_name)\n",
    "print(\"MPS:\\t\\t\", torch.backends.mps.is_available())\n",
    "ROOT = get_project_root_path()\n",
    "print(\"Path:\\t\\t\", ROOT)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f\"Using device:\\t {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',3000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Tokenizer and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_architecture = T5ForConditionalGeneration\n",
    "model_architecture = T5EncoderModel\n",
    "t5_tokenizer, t5_base_model = get_prottrans_tokenizer_model(base_model_name, model_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Data, Split into Dataset, and Tokenize Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_parquet(ROOT + '/data/processed/5.0_train.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M A P T L F Q K L F S K R T G L G A P G R D A R D P D C G F S W P L P E F D P S Q I R L I V Y Q D C E R R G R N V L F D S S V K R R N E D I</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M D F T S L E T T T F E E V V I A L G S N V G N R M N N F K E A L R L M K D Y G I S V T R H S C L Y E T E P V H V T D Q P R F L N A A I R G</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M D D I S G R Q T L P R I N R L L E H V G N P Q D S L S I L H I A G T N G K E T V S K F L T S I L Q H P G Q Q R Q R V L I G R Y T T S S L L</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M L G T V K M E G H E T S D W N S Y Y A D T Q E A Y S S V P V S N M N S G L G S M N S M N T Y M T M N T M T T S G N M T P A S F N M S Y A N</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M L G A V K M E G H E P S D W S S Y Y A E P E G Y S S V S N M N A G L G M N G M N T Y M S M S A A A M G G G S G N M S A G S M N M S S Y V G</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                      Sequence  \\\n",
       "0  M A P T L F Q K L F S K R T G L G A P G R D A R D P D C G F S W P L P E F D P S Q I R L I V Y Q D C E R R G R N V L F D S S V K R R N E D I   \n",
       "1  M D F T S L E T T T F E E V V I A L G S N V G N R M N N F K E A L R L M K D Y G I S V T R H S C L Y E T E P V H V T D Q P R F L N A A I R G   \n",
       "2  M D D I S G R Q T L P R I N R L L E H V G N P Q D S L S I L H I A G T N G K E T V S K F L T S I L Q H P G Q Q R Q R V L I G R Y T T S S L L   \n",
       "3  M L G T V K M E G H E T S D W N S Y Y A D T Q E A Y S S V P V S N M N S G L G S M N S M N T Y M T M N T M T T S G N M T P A S F N M S Y A N   \n",
       "4  M L G A V K M E G H E P S D W S S Y Y A E P E G Y S S V S N M N A G L G M N G M N T Y M S M S A A A M G G G S G N M S A G S M N M S S Y V G   \n",
       "\n",
       "                                                                                                                                                                                                                Label  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "   Split  \n",
       "0      0  \n",
       "1      1  \n",
       "2      1  \n",
       "3      4  \n",
       "4      4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_train = df_data[df_data.Split.isin([0, 1, 2])].head(config.dataset_size*3)\n",
    "ds_train = df_to_dataset(\n",
    "    t5_tokenizer,\n",
    "    ds_train.Sequence.to_list(),\n",
    "    ds_train.Label.to_list(),\n",
    ")\n",
    "\n",
    "ds_validate = df_data[df_data.Split.isin([3])].head(config.dataset_size)\n",
    "ds_validate = df_to_dataset(\n",
    "    t5_tokenizer,\n",
    "    ds_validate.Sequence.to_list(),\n",
    "    ds_validate.Label.to_list(),\n",
    ")\n",
    "\n",
    "ds_test = df_data[df_data.Split.isin([4])].head(config.dataset_size)\n",
    "ds_test = df_to_dataset(\n",
    "    t5_tokenizer,\n",
    "    ds_test.Sequence.to_list(),\n",
    "    ds_test.Label.to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 12459\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 4148\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 4146\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds_train)\n",
    "print(ds_validate)\n",
    "print(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 4, 5, 11, 6, 14, 19, 9, 5, 20, 9, 11, 7, 10, 21, 17, 7, 18, 18, 3, 10, 11, 16, 9, 3, 18, 7, 7, 6, 13, 6, 7, 17, 19, 17, 7, 5, 4, 5, 7, 19, 17, 7, 19, 17, 11, 18, 19, 11, 19, 17, 11, 19, 11, 11, 7, 5, 17, 19, 11, 13, 3, 7, 15, 17, 19, 7, 18, 3, 17, 1]\n",
      "71\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "71\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "print(ds_test[0]['input_ids'])\n",
    "print(len(ds_test[0]['input_ids']))\n",
    "print(ds_test[0]['attention_mask'])\n",
    "print(len(ds_test[0]['attention_mask']))\n",
    "print(ds_test[0]['labels'])\n",
    "print(len(ds_test[0]['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M L G T V K M E G H E T S D W N S Y Y A D T Q E A Y S S V P V S N M N S G L G S M N S M N T Y M T M N T M T T S G N M T P A S F N M S Y A N</s>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer.decode(ds_test[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad></s><unk>A L G V S R E D T I P K F Q N Y M H W C X B O U Z'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer.decode(range(0, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        # task_type=TaskType.TOKEN_CLS,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=['q', 'k', 'v', 'o'],\n",
    "        # target_modules=['o'],\n",
    "        bias=\"none\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t5_lora_model = get_peft_model(t5_base_model, lora_config)\n",
    "# del t5_base_model\n",
    "# t5_lora_model = prepare_model_for_kbit_training(t5_lora_model) # add quantization\n",
    "# t5_lora_model = t5_base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,932,160 || all params: 1,212,073,984 || trainable%: 0.32441584027926795\n"
     ]
    }
   ],
   "source": [
    "t5_lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method T5EncoderModel.forward of T5EncoderModel(\n",
       "  (shared): Embedding(128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(\n",
       "                in_features=1024, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k): Linear(\n",
       "                in_features=1024, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v): Linear(\n",
       "                in_features=1024, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o): Linear(\n",
       "                in_features=4096, out_features=1024, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(\n",
       "                in_features=1024, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k): Linear(\n",
       "                in_features=1024, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v): Linear(\n",
       "                in_features=1024, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o): Linear(\n",
       "                in_features=4096, out_features=1024, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_lora_model.get_base_model().forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DeepSpeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
    "# os.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\n",
    "# os.environ[\"RANK\"] = \"0\"\n",
    "# os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "# os.environ[\"WORLD_SIZE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(ROOT+'/deepspeed_config.yaml', 'r') as file:\n",
    "#     deepspeed_config = yaml.safe_load(file)\n",
    "#     del file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepspeed_config = {\n",
    "#     \"fp16\": {\n",
    "#         \"enabled\": \"auto\",\n",
    "#         \"loss_scale\": 0,\n",
    "#         \"loss_scale_window\": 1000,\n",
    "#         \"initial_scale_power\": 16,\n",
    "#         \"hysteresis\": 2,\n",
    "#         \"min_loss_scale\": 1\n",
    "#     },\n",
    "\n",
    "#     \"optimizer\": {\n",
    "#         \"type\": \"AdamW\",\n",
    "#         \"params\": {\n",
    "#             \"lr\": \"auto\",\n",
    "#             \"betas\": \"auto\",\n",
    "#             \"eps\": \"auto\",\n",
    "#             \"weight_decay\": \"auto\"\n",
    "#         }\n",
    "#     },\n",
    "\n",
    "#     \"scheduler\": {\n",
    "#         \"type\": \"WarmupLR\",\n",
    "#         \"params\": {\n",
    "#             \"warmup_min_lr\": \"auto\",\n",
    "#             \"warmup_max_lr\": \"auto\",\n",
    "#             \"warmup_num_steps\": \"auto\"\n",
    "#         }\n",
    "#     },\n",
    "\n",
    "#     \"zero_optimization\": {\n",
    "#         \"stage\": 2,\n",
    "#         \"offload_optimizer\": {\n",
    "#             \"device\": \"cpu\",\n",
    "#             \"pin_memory\": True\n",
    "#         },\n",
    "#         \"allgather_partitions\": True,\n",
    "#         \"allgather_bucket_size\": 2e8,\n",
    "#         \"overlap_comm\": True,\n",
    "#         \"reduce_scatter\": True,\n",
    "#         \"reduce_bucket_size\": 2e8,\n",
    "#         \"contiguous_gradients\": True\n",
    "#     },\n",
    "\n",
    "#     \"gradient_accumulation_steps\": \"auto\",\n",
    "#     \"gradient_clipping\": \"auto\",\n",
    "#     \"steps_per_print\": 2000,\n",
    "#     \"train_batch_size\": \"auto\",\n",
    "#     \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "#     \"wall_clock_breakdown\": False\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Loop\n",
    "https://huggingface.co/docs/peft/task_guides/token-classification-lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=t5_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model = inject_linear_layer(\n",
    "    t5_lora_model=t5_lora_model,\n",
    "    num_labels=config.label_decoding.__len__(),\n",
    "    dropout_rate=config.dropout_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method T5EncoderModel.forward of T5EncoderModel(\n",
       "  (shared): Embedding(128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(\n",
       "                in_features=1024, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k): Linear(\n",
       "                in_features=1024, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v): Linear(\n",
       "                in_features=1024, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o): Linear(\n",
       "                in_features=4096, out_features=1024, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(\n",
       "                in_features=1024, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k): Linear(\n",
       "                in_features=1024, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v): Linear(\n",
       "                in_features=1024, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o): Linear(\n",
       "                in_features=4096, out_features=1024, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=6, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_lora_model.get_base_model().forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "def compute_metrics_custom(eval_preds: EvalPrediction):\n",
    "    print(*eval_preds)\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./checkpoints',\n",
    "    learning_rate=config.lr,\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    per_device_eval_batch_size=config.batch_size,\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    logging_steps=config.logging_steps,\n",
    "    # save_strategy=\"steps\",\n",
    "    save_steps=config.save_steps,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=config.eval_steps,\n",
    "    # load_best_model_at_end=True,\n",
    "    # save_total_limit=5,\n",
    "    seed=42,\n",
    "    # fp16=True,\n",
    "    # deepspeed=deepspeed_config\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=t5_lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    # eval_dataset=ds_validate,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=config.metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ([set(x) for x in ds_validate['labels']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_validate['labels'][\n",
    "#     [set(x) for x in ds_validate['labels']] != {0}\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n",
      "{0: 'I', 1: 'L', 2: 'M', 3: 'O', 4: 'S', 5: 'T'}\n"
     ]
    }
   ],
   "source": [
    "print(next(t5_lora_model.parameters()).is_cuda)\n",
    "print(t5_lora_model.device)\n",
    "print(config.label_decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='779' max='779' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [779/779 18:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.139800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.310100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.177600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.128900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.153300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.077700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.074300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.103600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.093300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.143900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.109900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.102300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.055600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.066600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.094100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.085800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.126700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.057500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.086000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.123400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.063700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.103600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.111500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.066100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.094200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.088100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=779, training_loss=0.13150859598323836, metrics={'train_runtime': 1132.2796, 'train_samples_per_second': 11.003, 'train_steps_per_second': 0.688, 'total_flos': 6432455417918292.0, 'train_loss': 0.13150859598323836, 'epoch': 1.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics=trainer.evaluate()\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_validate['labels'][9].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 4148\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4148, 70])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.zeros(4148,70)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x + [-1] * (70-len(x)) for x in ds_validate['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inlab = torch.tensor([x + [-1] * (70-len(x)) for x in ds_validate['labels']]).to('cpu')\n",
    "# inlab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "del inlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4148, 71])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inid = torch.tensor(ds_validate['input_ids']).to(device)\n",
    "inid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = t5_lora_model(input_ids=inid[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for index, _ in enumerate(inid):\n",
    "    if index % 100 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "    results += t5_lora_model(input_ids=inid[index:index+1]).logits.argmax(dim=-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4148"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_labels = [[config.label_decoding[y] for y in x] for x in ds_validate['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4148"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct 283592\n",
      "Incorrect 6200\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "for index, item in enumerate(results):\n",
    "    truth = correct_labels[index]\n",
    "    prediction = [config.label_decoding[x] for x in item[:len(correct_labels[index])]]\n",
    "    \n",
    "    for t, p in zip(truth, prediction):\n",
    "        if t == p:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    \n",
    "    # print(*truth, sep='')\n",
    "    # print(*prediction, sep='')\n",
    "    # print()\n",
    "    \n",
    "print(\"Correct\", correct)\n",
    "print(\"Incorrect\", incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9786053445229682\n"
     ]
    }
   ],
   "source": [
    "print(correct/(correct+incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*prediction[0].argmax(dim=-1)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*[config.label_decoding[x] for x in prediction[0].argmax(dim=-1)[0].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prediction[0].argmax(dim=-1))\n",
    "# print(inlab)\n",
    "# print(inlab == prediction[0].argmax(dim=-1)[:, :70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cust_pred = EvalPrediction(prediction, inlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*cust_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_metrics_custom(cust_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_log = pd.DataFrame(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1398</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.03</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3101</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.05</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1908</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>0.08</td>\n",
       "      <td>60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1776</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.10</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1289</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.13</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1300</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.15</td>\n",
       "      <td>120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1121</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.18</td>\n",
       "      <td>140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1533</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.21</td>\n",
       "      <td>160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>0.23</td>\n",
       "      <td>180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0777</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.26</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0852</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.28</td>\n",
       "      <td>220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.31</td>\n",
       "      <td>240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1036</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.33</td>\n",
       "      <td>260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.1232</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.36</td>\n",
       "      <td>280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0933</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.39</td>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0861</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.41</td>\n",
       "      <td>320</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.1439</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.44</td>\n",
       "      <td>340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.46</td>\n",
       "      <td>360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.1023</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.49</td>\n",
       "      <td>380</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0556</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.51</td>\n",
       "      <td>400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.54</td>\n",
       "      <td>420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0941</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.56</td>\n",
       "      <td>440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0795</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.59</td>\n",
       "      <td>460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0858</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.62</td>\n",
       "      <td>480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0574</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.64</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.67</td>\n",
       "      <td>520</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0575</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.69</td>\n",
       "      <td>540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0860</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.72</td>\n",
       "      <td>560</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.74</td>\n",
       "      <td>580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0912</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.77</td>\n",
       "      <td>600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0770</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.80</td>\n",
       "      <td>620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0637</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.82</td>\n",
       "      <td>640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.1036</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.85</td>\n",
       "      <td>660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0664</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.87</td>\n",
       "      <td>680</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.1115</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.90</td>\n",
       "      <td>700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0661</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.92</td>\n",
       "      <td>720</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0942</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.95</td>\n",
       "      <td>740</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.98</td>\n",
       "      <td>760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>779</td>\n",
       "      <td>1132.2796</td>\n",
       "      <td>11.003</td>\n",
       "      <td>0.688</td>\n",
       "      <td>6.432455e+15</td>\n",
       "      <td>0.131509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  learning_rate  epoch  step  train_runtime  \\\n",
       "0   1.1398       0.000974   0.03    20            NaN   \n",
       "1   0.3101       0.000949   0.05    40            NaN   \n",
       "2   0.1908       0.000923   0.08    60            NaN   \n",
       "3   0.1776       0.000897   0.10    80            NaN   \n",
       "4   0.1289       0.000872   0.13   100            NaN   \n",
       "5   0.1300       0.000846   0.15   120            NaN   \n",
       "6   0.1121       0.000820   0.18   140            NaN   \n",
       "7   0.1533       0.000795   0.21   160            NaN   \n",
       "8   0.1100       0.000769   0.23   180            NaN   \n",
       "9   0.0777       0.000743   0.26   200            NaN   \n",
       "10  0.0852       0.000718   0.28   220            NaN   \n",
       "11  0.0743       0.000692   0.31   240            NaN   \n",
       "12  0.1036       0.000666   0.33   260            NaN   \n",
       "13  0.1232       0.000641   0.36   280            NaN   \n",
       "14  0.0933       0.000615   0.39   300            NaN   \n",
       "15  0.0861       0.000589   0.41   320            NaN   \n",
       "16  0.1439       0.000564   0.44   340            NaN   \n",
       "17  0.1099       0.000538   0.46   360            NaN   \n",
       "18  0.1023       0.000512   0.49   380            NaN   \n",
       "19  0.0556       0.000487   0.51   400            NaN   \n",
       "20  0.0666       0.000461   0.54   420            NaN   \n",
       "21  0.0941       0.000435   0.56   440            NaN   \n",
       "22  0.0795       0.000409   0.59   460            NaN   \n",
       "23  0.0858       0.000384   0.62   480            NaN   \n",
       "24  0.0574       0.000358   0.64   500            NaN   \n",
       "25  0.1267       0.000332   0.67   520            NaN   \n",
       "26  0.0575       0.000307   0.69   540            NaN   \n",
       "27  0.0860       0.000281   0.72   560            NaN   \n",
       "28  0.1234       0.000255   0.74   580            NaN   \n",
       "29  0.0912       0.000230   0.77   600            NaN   \n",
       "30  0.0770       0.000204   0.80   620            NaN   \n",
       "31  0.0637       0.000178   0.82   640            NaN   \n",
       "32  0.1036       0.000153   0.85   660            NaN   \n",
       "33  0.0664       0.000127   0.87   680            NaN   \n",
       "34  0.1115       0.000101   0.90   700            NaN   \n",
       "35  0.0661       0.000076   0.92   720            NaN   \n",
       "36  0.0942       0.000050   0.95   740            NaN   \n",
       "37  0.0881       0.000024   0.98   760            NaN   \n",
       "38     NaN            NaN   1.00   779      1132.2796   \n",
       "\n",
       "    train_samples_per_second  train_steps_per_second    total_flos  train_loss  \n",
       "0                        NaN                     NaN           NaN         NaN  \n",
       "1                        NaN                     NaN           NaN         NaN  \n",
       "2                        NaN                     NaN           NaN         NaN  \n",
       "3                        NaN                     NaN           NaN         NaN  \n",
       "4                        NaN                     NaN           NaN         NaN  \n",
       "5                        NaN                     NaN           NaN         NaN  \n",
       "6                        NaN                     NaN           NaN         NaN  \n",
       "7                        NaN                     NaN           NaN         NaN  \n",
       "8                        NaN                     NaN           NaN         NaN  \n",
       "9                        NaN                     NaN           NaN         NaN  \n",
       "10                       NaN                     NaN           NaN         NaN  \n",
       "11                       NaN                     NaN           NaN         NaN  \n",
       "12                       NaN                     NaN           NaN         NaN  \n",
       "13                       NaN                     NaN           NaN         NaN  \n",
       "14                       NaN                     NaN           NaN         NaN  \n",
       "15                       NaN                     NaN           NaN         NaN  \n",
       "16                       NaN                     NaN           NaN         NaN  \n",
       "17                       NaN                     NaN           NaN         NaN  \n",
       "18                       NaN                     NaN           NaN         NaN  \n",
       "19                       NaN                     NaN           NaN         NaN  \n",
       "20                       NaN                     NaN           NaN         NaN  \n",
       "21                       NaN                     NaN           NaN         NaN  \n",
       "22                       NaN                     NaN           NaN         NaN  \n",
       "23                       NaN                     NaN           NaN         NaN  \n",
       "24                       NaN                     NaN           NaN         NaN  \n",
       "25                       NaN                     NaN           NaN         NaN  \n",
       "26                       NaN                     NaN           NaN         NaN  \n",
       "27                       NaN                     NaN           NaN         NaN  \n",
       "28                       NaN                     NaN           NaN         NaN  \n",
       "29                       NaN                     NaN           NaN         NaN  \n",
       "30                       NaN                     NaN           NaN         NaN  \n",
       "31                       NaN                     NaN           NaN         NaN  \n",
       "32                       NaN                     NaN           NaN         NaN  \n",
       "33                       NaN                     NaN           NaN         NaN  \n",
       "34                       NaN                     NaN           NaN         NaN  \n",
       "35                       NaN                     NaN           NaN         NaN  \n",
       "36                       NaN                     NaN           NaN         NaN  \n",
       "37                       NaN                     NaN           NaN         NaN  \n",
       "38                    11.003                   0.688  6.432455e+15    0.131509  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "my_plot = sns.lineplot(data=result_log, x='epoch', y='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = my_plot.get_figure()\n",
    "fig.savefig(\"./plots/out.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model.save_pretrained(ROOT + '/models/linear_model_v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q02742|EUKARYA|NO_SP|4\\\n",
    "MLRTLLRRRLFSYPTKYYFMVLVLSLITFSVLRIHQKPEFVSVRHLELAGENPSSDINCTKVLQGDVNEI\\\n",
    "IIIIIIIIIMMMMMMMMMMMMMMMMMMMMMMMOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n",
    "\n",
    "Q9NQR9|EUKARYA|NO_SP|4\\\n",
    "MDFLHRNGVLIIQHLQKDYRAYYTFLNFMSNVGDPRNIFFIYFPLCFQFNQTVGTKMIWVAVIGDWLNLI\\\n",
    "OOOOOOOOOOOOOOOOOOOOOOOOMMMMMMMMMMMMMMMMMMMMMIIIIIIIIIIIMMMMMMMMMMMMMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*ds_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model.to(device)\n",
    "\n",
    "test_set = ds_test.select(range(0,2)).with_format(\"torch\", device=device)\n",
    "\n",
    "# For token classification we need a data collator here to pad correctly\n",
    "data_collator = DataCollatorForTokenClassification(t5_tokenizer) \n",
    "\n",
    "# Create a dataloader for the test dataset\n",
    "test_dataloader = DataLoader(test_set, batch_size=16, shuffle = False, collate_fn = data_collator)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "t5_lora_model.eval()\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = []\n",
    "# We need to collect the batch[\"labels\"] as well, this allows us to filter out all positions with a -100 afterwards\n",
    "padded_labels = []\n",
    "\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        # print(counter)\n",
    "        counter += 1\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        padded_labels += batch['labels'].tolist()\n",
    "        print(padded_labels)\n",
    "        # Add batch results(logits) to predictions, we take the argmax here to get the predicted class\n",
    "        print(input_ids)\n",
    "        prediction = t5_lora_model(input_ids=input_ids).logits.argmax(dim=-1).tolist()\n",
    "        print(prediction)\n",
    "        predictions += prediction#.argmax(dim=-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_item = 1\n",
    "\n",
    "actual = [config.label_decoding[x] for x in test_set['labels'][index_item].tolist()]\n",
    "print(actual.__len__())\n",
    "print(*actual)\n",
    "\n",
    "pred = [config.label_decoding[x] for x in predictions[index_item]]\n",
    "print(pred.__len__())\n",
    "print(*pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
