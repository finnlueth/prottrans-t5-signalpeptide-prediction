{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "from src.model_new import (\n",
    "    T5EncoderModelForTokenClassification,\n",
    "    T5EncoderModelForSequenceClassification,\n",
    "    create_datasets,\n",
    ")\n",
    "import src.config\n",
    "import src.data\n",
    "import src.model_new\n",
    "\n",
    "\n",
    "import peft\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = src.utils.get_project_root_path()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "\n",
    "USE_CRF = False\n",
    "\n",
    "EXPERT = 'ALL'\n",
    "MODEL_VERRSION = src.config.model_version\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Base Model:\\t\", src.config.base_model_name)\n",
    "print(\"MPS:\\t\\t\", torch.backends.mps.is_available())\n",
    "print(\"Path:\\t\\t\", ROOT)\n",
    "print(f\"Using device:\\t {device}\")\n",
    "\n",
    "# torch.set_printoptions(threshold=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path=src.config.base_model_name,\n",
    "        do_lower_case=False,\n",
    "        use_fast=True,\n",
    "        legacy=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTA_FILENAME = '5_SignalP_5.0_Training_set.fasta'\n",
    "# FASTA_FILENAME = '5_SignalP_5.0_Training_set_testing.fasta'\n",
    "annotations_name = ['Label'] + ['Type'] # Choose Type or Label\n",
    "\n",
    "df_data = src.data.process(src.data.parse_file(ROOT + '/data/raw/' + FASTA_FILENAME))\n",
    "\n",
    "dataset_signalp_type_splits = {}\n",
    "\n",
    "for sequence_type in src.config.select_encoding_type.keys():\n",
    "    dataset_signalp = src.model_new.create_datasets(\n",
    "        splits=src.config.splits,\n",
    "        tokenizer=t5_tokenizer,\n",
    "        data=df_data,\n",
    "        annotations_name=annotations_name,\n",
    "        dataset_size=src.config.dataset_size,\n",
    "        sequence_type=sequence_type\n",
    "        )\n",
    "    dataset_signalp_type_splits.update({sequence_type: dataset_signalp})\n",
    "\n",
    "del df_data\n",
    "\n",
    "dataset_signalp = dataset_signalp_type_splits[EXPERT]\n",
    "display(dataset_signalp_type_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_base_model_gate = T5EncoderModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=src.config.base_model_name,\n",
    "    device_map='auto',\n",
    "    load_in_8bit=False,\n",
    "    custom_num_labels=len(src.config.type_encoding),\n",
    "    custom_dropout_rate=0.1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_base_model_expert = T5EncoderModelForTokenClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=src.config.base_model_name,\n",
    "    device_map='auto',\n",
    "    load_in_8bit=False,\n",
    "    custom_num_labels=len(src.config.label_decoding),\n",
    "    custom_dropout_rate=0.1,\n",
    "    use_crf=USE_CRF\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_location = f'/models/moe_v{MODEL_VERRSION}_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_adapter_location = adapter_location+'gate'\n",
    "t5_base_model_gate.load_adapter(ROOT+gate_adapter_location, adapter_name=f\"gate\")\n",
    "t5_base_model_gate.set_adapter(\"gate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "todo\n",
    "\n",
    "X gate\\\n",
    "O lin all\\\n",
    "O lin expert\\\n",
    "O crf all \\\n",
    "O crf expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTA_FILENAME = '5_SignalP_5.0_Training_set.fasta'\n",
    "df_data = src.data.process(src.data.parse_file(ROOT + '/data/raw/' + FASTA_FILENAME))\n",
    "df_data = df_data[df_data['Partition_No'] == 4].reset_index(drop=True)\n",
    "df_data['Sequence_Raw'] = df_data['Sequence'].apply(lambda x: x.replace(' ', ''))\n",
    "# df_data['Mask'] = [x[1:] for x in dataset_signalp_type_splits['ALL']['test']['attention_mask']]\n",
    "df_data['input_ids'] = dataset_signalp_type_splits['ALL']['test']['input_ids']\n",
    "df_data['ds_attention_mask'] = dataset_signalp_type_splits['ALL']['test']['attention_mask']\n",
    "df_data['ds_labels'] = dataset_signalp_type_splits['ALL']['test']['labels']\n",
    "df_data['ds_type'] = dataset_signalp_type_splits['ALL']['test']['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_signalp_type_splits['ALL']['test'])\n",
    "display(df_data.head(), df_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = df_data.head().copy(deep=True)\n",
    "\n",
    "df_data['predicted_type'] = df_data.progress_apply(lambda x:\n",
    "    src.model_new.translate_logits(\n",
    "        src.model_new.predict_model(\n",
    "            sequence=x['Sequence'],\n",
    "            tokenizer=t5_tokenizer,\n",
    "            model=t5_base_model_gate,\n",
    "            attention_mask=torch.Tensor([x['ds_attention_mask']]).to(device),\n",
    "            device=device\n",
    "            )['logits'].unsqueeze(0),\n",
    "        src.config.type_decoding,\n",
    "        viterbi_decoding=False\n",
    "        )[0],  axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data[df_data['predicted_type'] != df_data['Type']]\n",
    "# t5_tokenizer.decode(t5_tokenizer.encode('M T E T L P P V T E S A V A L Q A E V T Q R E L F E F V L N D P L L A S S L Y I N I A L A G L S I L L F V F M T R G L D D P R A K L I A V S'))\n",
    "# dataset_signalp_type_splits['ALL']['test']['attention_mask']\n",
    "# decoded_shit = [(x, ''.join([t5_tokenizer.decode(z) for z in y])) for x, y in zip(df_data['Sequence'], dataset_signalp_type_splits['ALL']['test']['input_ids'])]\n",
    "# print(*decoded_shit, sep='\\n')\n",
    "# len(decoded_shit)\n",
    "# len(df_data['Sequence'].at[0]), len(df_data['Mask'].at[0])# print(*[(x, y) for x, y in zip(df_data['Sequence'], df_data['Mask'])], sep='\\n')\n",
    "#  df_singalp_split = df_singalp6_preds[\n",
    "#     df_singalp6_preds['Type'] == 'SP'\n",
    "# ]\n",
    "# test_df = df_data.head().copy(deep=True)\n",
    "# test_df\n",
    "# test_df['asd'] = test_df.apply(lambda x: (x['Type'], x['Mask']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_parquet('./results/df_data.parquet.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert_adapter_location = adapter_location + f'expert_{EXPERT}'\n",
    "# t5_base_model_expert.load_adapter(ROOT+expert_adapter_location)\n",
    "\n",
    "# FASTA_FILENAME = '5_SignalP_5.0_Training_set_testing.fasta'\n",
    "# df_data = src.data.process(src.data.parse_file(ROOT + '/data/raw/' + FASTA_FILENAME))\n",
    "# df_data = df_data[df_data['Partition_No'] == 4].reset_index(drop=True)\n",
    "# df_data['Sequence'] = df_data['Sequence'].apply(lambda x: x.replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_data['Type_Prediction'] = \n",
    "# df_data['Label'].iloc[17:18].apply(lambda x: src.model_new.moe_inference(\n",
    "#     sequence=x,\n",
    "#     tokenizer=t5_tokenizer,\n",
    "#     model_gate=t5_base_model_gate,\n",
    "#     model_expert=t5_base_model_expert,\n",
    "#     device=device,\n",
    "#     # result_type='SP',\n",
    "#     )[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERT = 'LIPO'\n",
    "expert_adapter_location = ROOT + adapter_location + f'expert_{EXPERT}'\n",
    "print(expert_adapter_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_base_model_expert.load_adapter(expert_adapter_location, adapter_name=f\"{EXPERT}_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_base_model_expert.unload(EXPERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ds_index = 4\n",
    "# _input_ids_test = df_data['Sequence'].iloc[_ds_index]\n",
    "# _labels_test = df_data['Label'].iloc[_ds_index]\n",
    "# _type_test = df_data['Type'].iloc[_ds_index]\n",
    "_input_ids_test = t5_tokenizer.decode(dataset_signalp[_ds_type][_ds_index]['input_ids'][:-1])\n",
    "_labels_test = torch.tensor([dataset_signalp[_ds_type][_ds_index]['labels'] + [-100]]).to(device)\n",
    "_attention_mask_test = torch.tensor([dataset_signalp[_ds_type][_ds_index]['attention_mask']]).to(device)\n",
    "\n",
    "\n",
    "print('Iput IDs:\\t', _input_ids_test)\n",
    "print('Labels:\\t\\t', _labels_test)\n",
    "print('Type:\\t\\t', _type_test)\n",
    "\n",
    "result = src.model_new.moe_inference(\n",
    "    sequence=_input_ids_test,\n",
    "    attentino_mask=_attention_mask_test,\n",
    "    tokenizer=t5_tokenizer,\n",
    "    model_gate=t5_base_model_gate,\n",
    "    model_expert=t5_base_model_expert,\n",
    "    device=device,\n",
    "    result_type='LIPO',\n",
    "    use_crf=True,\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_base_model_gate.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ds_index = 220\n",
    "# _ds_type = 'test'\n",
    "# USE_CRF = True\n",
    "\n",
    "# _input_ids_test = t5_tokenizer.decode(dataset_signalp[_ds_type][_ds_index]['input_ids'][:-1])\n",
    "# _labels_test = torch.tensor([dataset_signalp[_ds_type][_ds_index]['labels'] + [-100]]).to(device)\n",
    "# _attention_mask_test = torch.tensor([dataset_signalp[_ds_type][_ds_index]['attention_mask']]).to(device)\n",
    "\n",
    "# _labels_test_decoded = [src.config.label_decoding[x] for x in _labels_test.tolist()[0][:-1]]\n",
    "\n",
    "# print('Iput IDs:\\t', _input_ids_test)\n",
    "# print('Labels:\\t\\t', *_labels_test.tolist()[0])\n",
    "# print('Labels Decoded:\\t', *_labels_test_decoded)\n",
    "# print('Attention Mask:\\t', *_attention_mask_test.tolist()[0])\n",
    "# print('----')\n",
    "\n",
    "# _ds_index = 3250\n",
    "_ds_index = 3250\n",
    "_ds_type = 'test'\n",
    "USE_CRF = True\n",
    "\n",
    "_input_ids_test = t5_tokenizer.decode(dataset_signalp[_ds_type][_ds_index]['input_ids'][:-1])\n",
    "_labels_test = torch.tensor([dataset_signalp[_ds_type][_ds_index]['labels'] + [-100]]).to(device)\n",
    "_attention_mask_test = torch.tensor([dataset_signalp[_ds_type][_ds_index]['attention_mask']]).to(device)\n",
    "\n",
    "_labels_test_decoded = [src.config.label_decoding[x] for x in _labels_test.tolist()[0][:-1]]\n",
    "\n",
    "print('Iput IDs:\\t', _input_ids_test)\n",
    "print('Labels:\\t\\t', *_labels_test.tolist()[0])\n",
    "print('Labels Decoded:\\t', *_labels_test_decoded)\n",
    "print('Attention Mask:\\t', *_attention_mask_test.tolist()[0])\n",
    "print('----')\n",
    "\n",
    "preds = src.model_new.predict_model(\n",
    "    sequence=_input_ids_test,\n",
    "    tokenizer=t5_tokenizer,\n",
    "    model=t5_base_model_expert,\n",
    "    labels=_labels_test,\n",
    "    attention_mask=_attention_mask_test,\n",
    "    device=device,\n",
    "    viterbi_decoding=USE_CRF,\n",
    "    )\n",
    "\n",
    "_result = src.model_new.translate_logits(\n",
    "    logits=preds.logits,\n",
    "    viterbi_decoding=USE_CRF,\n",
    "    decoding=src.config.label_decoding\n",
    "    )\n",
    "\n",
    "print('Result: \\t',* _result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
