{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import src.evaluation\n",
    "import src.config\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_parquet('./results/df_data.parquet.gzip')\n",
    "\n",
    "# display(results.head())\n",
    "# display(results.sample(5))\n",
    "# print(results.shape)\n",
    "# print(results.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mcc(targets, predictions, labels):\n",
    "    p = {}\n",
    "    \n",
    "    for x in labels:\n",
    "        target = [1 if y == x else 0 for y in targets]\n",
    "        prediction = [1 if y == x else 0 for y in predictions]\n",
    "\n",
    "        number_errors = 100\n",
    "        slice_len = 10000\n",
    "        errors = np.zeros(number_errors)\n",
    "        for err in range(number_errors):\n",
    "            index = random.randint(1, len(target) - slice_len)\n",
    "            \n",
    "            target_slice = target[index:index+slice_len]\n",
    "            pred_slice = prediction[index:index+slice_len]\n",
    "            # print(target_slice)\n",
    "            # print(pred_slice)\n",
    "            \n",
    "            if target_slice == pred_slice:\n",
    "                errors[err] = 1\n",
    "            else:\n",
    "                errors[err] = matthews_corrcoef(target_slice, pred_slice)\n",
    "                \n",
    "            # print(errors[err])\n",
    "            # if errors[err] == 0:\n",
    "            #     print(target[index:index+slice_len])\n",
    "            #     print(prediction[index:index+slice_len])\n",
    "        error = 0\n",
    "        avg_err = np.mean(errors)\n",
    "        error = np.sqrt(1/(errors.shape[0]-1) * np.sum(errors-avg_err)**2)\n",
    "        print(errors)\n",
    "        # print(error)\n",
    "\n",
    "        print()\n",
    "        p.update({x: [matthews_corrcoef(target, prediction), error]})\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_labels = ['S', 'L', 'T', 'O', 'I', 'M']\n",
    "index_names = ['predicted_label_linear_ALL', 'predicted_label_linear_experts', 'predicted_label_linear_experts_imperfect', 'predicted_label_crf_ALL', 'predicted_label_crf_experts', 'predicted_label_crf_experts_imperfect']\n",
    "real_names = ['Linear Broad', 'Linear Experts Perfect Gate', 'Linear Experts Imperfect Gate', 'CRF Broad', 'CRF Experts Perfect Gate', 'CRF Experts Imperfect Gate']\n",
    "\n",
    "mcc_values = {}\n",
    "for index_name, real_name in zip(index_names, real_names):\n",
    "    # print(index_name, real_name)\n",
    "    tmp_mcc_values = evaluate_mcc(\n",
    "        targets=list(''.join(results['Label'].tolist())),\n",
    "        predictions=list(''.join(results[index_name].tolist())),\n",
    "        labels=expert_labels\n",
    "    )\n",
    "    # tmp_mcc_values['real_names'] = real_name\n",
    "    mcc_values.update({real_name: tmp_mcc_values})\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcc_values = pd.DataFrame(mcc_values).reset_index().rename(columns={'index': 'Label'}).melt(id_vars=['Label'], var_name='Model', value_name='MCC')\n",
    "df_mcc_values['Error'] = df_mcc_values['MCC'].apply(lambda x: x[1])\n",
    "df_mcc_values['MCC'] = df_mcc_values['MCC'].apply(lambda x: x[0])\n",
    "# sns.set(style=\"whitegrid\")\n",
    "# ax = sns.barplot(data=df_mcc_values, ci=None)\n",
    "# ax.set(ylim=(0.5, None))\n",
    "df_mcc_values['Label'] = df_mcc_values['Label'].map({'S': 'Sec/SPI\\nSignal (S)', 'L': 'Sec/SPII\\nSignal (L)', 'T': 'Tat/SPI Signal\\n(T)', 'O': 'Outer\\nRegion (O)', 'I': 'Inner Region (I)', 'M': 'Membrane\\nRegion (M)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.evaluation.plot_mcc_split_label(df_mcc_values, 'mako')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mcc_simple(targets, predictions, labels):\n",
    "    p = {}\n",
    "    \n",
    "    for x in labels:\n",
    "        target = [1 if y == x else 0 for y in targets]\n",
    "        prediction = [1 if y == x else 0 for y in predictions]\n",
    "        if target == prediction:\n",
    "            mcc = 1\n",
    "        else:\n",
    "            mcc = matthews_corrcoef(target, prediction)\n",
    "        p.update({x: mcc})\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_results = [None] * results.shape[0]\n",
    "for index, item in results.iterrows():\n",
    "    target = item['Label']\n",
    "    prediction = item['predicted_label_linear_ALL']\n",
    "    mcc = evaluate_mcc_simple(target, prediction, expert_labels)\n",
    "    mcc_results[index] = mcc\n",
    "mcc_results = pd.DataFrame(mcc_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcc_results['M'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mcc_error(mcc_results_iter):\n",
    "    mcc_results_iter = [x for x in mcc_results_iter if x is not None]\n",
    "    mccs = np.array(mcc_results_iter)\n",
    "    mean_mcc = np.mean(mccs)\n",
    "    # error = np.sqrt(1/(mccs.shape[0]-1) * np.sum(mccs-mean_mcc)**2)\n",
    "    error = mccs.std()/np.sqrt(mccs.shape[0])\n",
    "    return mean_mcc, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[calc_mcc_error(mcc_results[x]) for x in expert_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_res = results.apply(lambda x: evaluate_mcc_simple(\n",
    "    targets=list(results['Label']),\n",
    "    predictions=list(results['predicted_label_crf_experts_imperfect']),\n",
    "    labels=expert_labels), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp_res = pd.DataFrame([pd.Series(x) for x in tmp_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp_res.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(1/(df_tmp_res['S'].shape[0]-1) * (np.sum(df_tmp_res['S']-df_tmp_res['S'].mean()**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcc_values = {}\n",
    "\n",
    "# for index_name, real_name in zip(index_names, real_names):\n",
    "#      p.update({real_name: evaluate_mcc_simple()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_mcc = results.apply(lambda x: 1 if x['Label'] == x['predicted_label_linear_ALL'] else matthews_corrcoef(list(x['Label']), list(x['predicted_label_linear_ALL'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(1/(mc_mcc.shape[0]-1) * (np.sum(mc_mcc-mc_mcc.mean()**2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_mcc.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1/(mc_mcc.shape[0]-1)) * np.sum((mc_mcc-mc_mcc.mean())**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_mcc.std()/np.sqrt(mc_mcc.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
