{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/.venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "from src.model_new import (\n",
    "    T5EncoderModelForTokenClassification,\n",
    "    create_datasets\n",
    ")\n",
    "import src.config\n",
    "import src.data\n",
    "import src.model_new\n",
    "\n",
    "\n",
    "import peft\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "ROOT = '../'\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path=src.config.base_model_name,\n",
    "        do_lower_case=False,\n",
    "        use_fast=True,\n",
    "        legacy=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5EncoderModelForTokenClassification were not initialized from the model checkpoint at Rostlab/prot_t5_xl_uniref50 and are newly initialized: ['custom_classifier.bias', 'custom_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "t5_base_model = T5EncoderModelForTokenClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=src.config.base_model_name,\n",
    "    device_map='auto',\n",
    "    load_in_8bit=False,\n",
    "    custom_num_labels=len(src.config.label_decoding),\n",
    "    custom_dropout_rate=0.1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copy for comparing later\n",
    "t5_base_model_copy = copy.deepcopy(t5_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*[(n, type(m)) for n, m in t5_base_model.named_modules()], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,944,460 || all params: 1,212,086,284 || trainable%: 0.32542732741623864\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q', 'k', 'v', 'o'],\n",
    "    bias=\"none\",\n",
    "    modules_to_save=['custom_classifier'],\n",
    ")\n",
    "t5_lora_model = peft.get_peft_model(t5_base_model, lora_config)\n",
    "t5_lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_params = 0\n",
    "\n",
    "# for name, param in t5_lora_model.base_model.named_parameters():\n",
    "#     # if \"lora\" not in name:\n",
    "#     #     continue\n",
    "#     count_params += param.numel()\n",
    "#     print(f\"New parameter {name:<13} | {param.numel():>5} parameters | updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_before = dict(t5_base_model_copy.named_parameters())\n",
    "# for name, param in t5_lora_model.base_model.named_parameters():\n",
    "#     if \"lora\" in name:\n",
    "#         continue\n",
    "\n",
    "#     name_before = name.partition(\".\")[-1].replace(\"original_\", \"\").replace(\"module.\", \"\").replace(\"modules_to_save.default.\", \"\")\n",
    "#     param_before = params_before[name_before]\n",
    "#     if torch.allclose(param, param_before):\n",
    "#         print(f\"Parameter {name_before:<14} | {param.numel():>7} parameters | not updated\")\n",
    "#     else:\n",
    "#         print(f\"Parameter {name_before:<14} | {param.numel():>7} parameters | updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*[(n, type(m)) for n, m in t5_base_model.named_modules()], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/.venv/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "FASTA_FILENAME = '5_SignalP_5.0_Training_set.fasta'\n",
    "annotations_name = 'Label' # Choose Type or Label\n",
    "\n",
    "df_data = src.data.process(src.data.parse_file(ROOT + '/data/raw/' + FASTA_FILENAME))\n",
    "\n",
    "dataset_signalp = src.model_new.create_datasets(\n",
    "    splits=src.config.splits,\n",
    "    tokenizer=t5_tokenizer,\n",
    "    data=df_data,\n",
    "    annotations_name=annotations_name,\n",
    "    # dataset_size=src.config.dataset_size,\n",
    "    dataset_size=src.config.dataset_size,\n",
    "    encoder=src.config.select_encodings[annotations_name],\n",
    "    )\n",
    "\n",
    "del df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_signalp['train']['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = torch.tensor([[[1,0,0,0,1], [2,3,2,1,2], [2,2,5,1,3]],\n",
    "#                             [[1,2,0,0,0], [1,float('nan'),1,0,100], [1,4,3,7,10]]])\n",
    "# predictions_argmaxed = np.nan_to_num(predictions).argmax(axis=-1)\n",
    "# predictions_argmaxed = predictions.nan_to_num().argmax(dim=-1)\n",
    "# print(predictions_argmaxed)\n",
    "\n",
    "# references = torch.tensor([[0,1,3], [1,4,3]])\n",
    "# print(references)\n",
    "\n",
    "# torch.Size([3, 71, 1024])\n",
    "# print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor([19,  4,  5, 11,  6, 14, 19,  9,  5, 20,  9, 11,  7, 10, 21, 17,  7, 18,\n",
    "#         18,  3, 10, 11, 16,  9,  3, 18,  7,  7,  6, 13,  6,  7, 17, 19, 17,  7,\n",
    "#          5,  4,  5,  7, 19, 17,  7, 19, 17, 11, 18, 19, 11, 19, 17, 11, 19, 11,\n",
    "#         11,  7,  5, 17, 19, 11, 13,  3,  7, 15, 17, 19,  7, 18,  3, 17,  1],\n",
    "#        device='mps:0')\n",
    "# results = roc_auc_score_metric.compute(references=references, prediction_scores=predictions[0], multi_class='ovr')\n",
    "# print(round(results['roc_auc'], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seqeval_metric = evaluate.load(\"seqeval\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "# roc_auc_score_metric = evaluate.load(\"roc_auc\", \"multiclass\")\n",
    "# roc_auc_score = evaluate.load(\"roc_auc\")\n",
    "matthews_correlation_metric = evaluate.load(\"matthews_correlation\")\n",
    "\n",
    "def batch_eval_elementwise(predictions: np.ndarray, references: np.ndarray):\n",
    "    results = {}\n",
    "    # predictions = np.nan_to_num(predictions).argmax(axis=-1)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    results.update({'accuracy_metric': np.average([accuracy_metric.compute(predictions=x, references=y)['accuracy'] for x, y in zip(predictions, references)])})\n",
    "    results.update({'precision_metric': np.average([precision_metric.compute(predictions=x, references=y, average='micro')['precision'] for x, y in zip(predictions, references)])})\n",
    "    results.update({'recall_metric': np.average([recall_metric.compute(predictions=x, references=y, average='micro')['recall'] for x, y in zip(predictions, references)])})\n",
    "    results.update({'f1_metric': np.average([f1_metric.compute(predictions=x, references=y, average='micro')['f1'] for x, y in zip(predictions, references)])})\n",
    "    # results.update({'roc_auc': np.average([roc_auc_score_metric.compute(prediction_scores=x, references=y, average='micro')['roc_auc'] for x, y in zip(predictions, references)])})\n",
    "    results.update({'matthews_correlation': np.average([matthews_correlation_metric.compute(predictions=x, references=y, average='micro')['matthews_correlation'] for x, y in zip(predictions, references)])})\n",
    "    return results\n",
    "# display(batch_eval_elementwise(predictions.numpy(), references.numpy()))\n",
    "\n",
    "def compute_metrics(p):\n",
    "    # print('=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= preds compute_metrics start =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=')\n",
    "    predictions, references = p\n",
    "    results = batch_eval_elementwise(predictions=predictions, references=references)\n",
    "    # print(results)\n",
    "    # print('=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= preds compute_metrics stop =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=')\n",
    "    return results\n",
    "# metrics = compute_metrics((predictions, references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=t5_tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./checkpoints',\n",
    "    learning_rate=src.config.lr,\n",
    "    per_device_train_batch_size=src.config.batch_size,\n",
    "    per_device_eval_batch_size=src.config.batch_size,\n",
    "    num_train_epochs=src.config.num_epochs,\n",
    "    logging_steps=src.config.logging_steps,\n",
    "    # save_strategy=\"steps\",\n",
    "    # save_steps=src.config.save_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1,\n",
    "    # gradient_accumulation_steps=accum,\n",
    "    # load_best_model_at_end=True,\n",
    "    # save_total_limit=5,\n",
    "    seed=42,\n",
    "    # fp16=True,\n",
    "    # deepspeed=deepspeed_config,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=['labels'],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=t5_lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_signalp['train'],\n",
    "    eval_dataset=dataset_signalp['valid'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_lora_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(55012) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68447bc9133a4eafab01d307cc6515c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.encoder <bound method T5EncoderModel.forward of T5EncoderModelForTokenClassification(\n",
      "  (shared): Embedding(128, 1024)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(\n",
      "                in_features=4096, out_features=1024, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(\n",
      "                in_features=4096, out_features=1024, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (custom_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (custom_classifier): ModulesToSaveWrapper(\n",
      "    (original_module): Linear(in_features=1024, out_features=6, bias=True)\n",
      "    (modules_to_save): ModuleDict(\n",
      "      (default): Linear(in_features=1024, out_features=6, bias=True)\n",
      "    )\n",
      "  )\n",
      ")>\n",
      "self.config.hidden_size 1024\n",
      "self.custom_num_labels 6\n",
      "\n",
      "encoder_outputs.last_hidden_state tensor([[[ 0.2653, -0.2492,  0.2399,  ...,  0.0000, -0.0666, -0.0345],\n",
      "         [ 0.1839, -0.2975,  0.0913,  ...,  0.1990,  0.0655,  0.1939],\n",
      "         [ 0.0936, -0.2798,  0.2663,  ...,  0.1852,  0.1629, -0.1198],\n",
      "         ...,\n",
      "         [ 0.0000, -0.1162,  0.1140,  ..., -0.0567, -0.0000, -0.3199],\n",
      "         [-0.0000, -0.0000,  0.3953,  ..., -0.1075,  0.1683,  0.1101],\n",
      "         [-0.0249, -0.0000,  0.0634,  ...,  0.0228, -0.0542, -0.0026]],\n",
      "\n",
      "        [[ 0.1230, -0.2503, -0.2469,  ...,  0.4877,  0.0787,  0.1991],\n",
      "         [-0.2411, -0.0089, -0.3791,  ...,  0.0306,  0.0303,  0.1048],\n",
      "         [-0.0280, -0.3618,  0.1582,  ..., -0.0189,  0.0044, -0.1488],\n",
      "         ...,\n",
      "         [-0.2533, -0.1529,  0.3040,  ...,  0.2405,  0.0590,  0.0516],\n",
      "         [ 0.0290, -0.1405,  0.1143,  ...,  0.0000, -0.0393,  0.1386],\n",
      "         [-0.0325, -0.0552,  0.0000,  ..., -0.0048,  0.0341,  0.1237]],\n",
      "\n",
      "        [[ 0.2651, -0.1602, -0.2149,  ...,  0.1810,  0.3881,  0.1915],\n",
      "         [ 0.3196, -0.0655, -0.0000,  ...,  0.1782,  0.5748,  0.0000],\n",
      "         [ 0.1354, -0.2681, -0.2067,  ...,  0.0000,  0.3679,  0.0820],\n",
      "         ...,\n",
      "         [-0.0342, -0.0000,  0.4235,  ..., -0.2304, -0.1151,  0.0000],\n",
      "         [ 0.6875, -0.1258, -0.5738,  ..., -0.5683,  0.3380,  0.2640],\n",
      "         [-0.0478, -0.0932,  0.0378,  ..., -0.0733, -0.0000, -0.0442]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1784, -0.2872,  0.0456,  ...,  0.2696,  0.3589,  0.0321],\n",
      "         [-0.1417, -0.1312, -0.1476,  ...,  0.1301,  0.1188, -0.0110],\n",
      "         [ 0.2515,  0.0000, -0.0000,  ...,  0.1556,  0.1215, -0.1387],\n",
      "         ...,\n",
      "         [-0.1033, -0.1650,  0.2861,  ..., -0.2066, -0.2692, -0.0126],\n",
      "         [-0.0182, -0.3835,  0.0012,  ..., -0.2301, -0.0725, -0.0000],\n",
      "         [-0.0766, -0.1088,  0.0612,  ..., -0.0043, -0.0197,  0.0176]],\n",
      "\n",
      "        [[ 0.0112, -0.0319,  0.0693,  ...,  0.3266,  0.2105, -0.0207],\n",
      "         [-0.0493, -0.1497,  0.0753,  ...,  0.0000,  0.1525, -0.0000],\n",
      "         [-0.0228, -0.2590,  0.1141,  ...,  0.2743,  0.3631, -0.3204],\n",
      "         ...,\n",
      "         [ 0.0000, -0.1285, -0.0490,  ...,  0.0456, -0.0392,  0.1034],\n",
      "         [ 0.0683, -0.0636,  0.1534,  ..., -0.0681,  0.1988, -0.2293],\n",
      "         [-0.0319, -0.1005,  0.0950,  ..., -0.0955, -0.0000, -0.0175]],\n",
      "\n",
      "        [[ 0.0751, -0.1268,  0.0170,  ...,  0.2119,  0.2976,  0.4358],\n",
      "         [-0.2717, -0.0641, -0.0799,  ...,  0.2169,  0.0568,  0.2369],\n",
      "         [-0.1938, -0.1106, -0.0697,  ...,  0.2406,  0.3537, -0.0000],\n",
      "         ...,\n",
      "         [ 0.0113, -0.1986,  0.1689,  ..., -0.0000, -0.1805,  0.1930],\n",
      "         [-0.0000, -0.0443,  0.0656,  ..., -0.3168, -0.3580, -0.2131],\n",
      "         [-0.0498, -0.0317,  0.0521,  ..., -0.0533,  0.0000,  0.0000]]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "sequence_output dropout tensor([[[ 0.2948, -0.2769,  0.2666,  ...,  0.0000, -0.0740, -0.0384],\n",
      "         [ 0.2044, -0.0000,  0.1014,  ...,  0.2211,  0.0728,  0.2155],\n",
      "         [ 0.1039, -0.3108,  0.2959,  ...,  0.2058,  0.1810, -0.1331],\n",
      "         ...,\n",
      "         [ 0.0000, -0.1291,  0.1267,  ..., -0.0000, -0.0000, -0.3554],\n",
      "         [-0.0000, -0.0000,  0.4393,  ..., -0.1195,  0.1870,  0.1223],\n",
      "         [-0.0276, -0.0000,  0.0705,  ...,  0.0254, -0.0602, -0.0000]],\n",
      "\n",
      "        [[ 0.1367, -0.2782, -0.2743,  ...,  0.5419,  0.0875,  0.2212],\n",
      "         [-0.0000, -0.0099, -0.4212,  ...,  0.0340,  0.0337,  0.1165],\n",
      "         [-0.0311, -0.4020,  0.1758,  ..., -0.0210,  0.0049, -0.1653],\n",
      "         ...,\n",
      "         [-0.2815, -0.1699,  0.3378,  ...,  0.2672,  0.0655,  0.0573],\n",
      "         [ 0.0323, -0.1561,  0.1270,  ...,  0.0000, -0.0437,  0.1540],\n",
      "         [-0.0000, -0.0614,  0.0000,  ..., -0.0054,  0.0379,  0.1374]],\n",
      "\n",
      "        [[ 0.2946, -0.1780, -0.2388,  ...,  0.2011,  0.4313,  0.2128],\n",
      "         [ 0.3551, -0.0727, -0.0000,  ...,  0.1980,  0.6387,  0.0000],\n",
      "         [ 0.1505, -0.2979, -0.2297,  ...,  0.0000,  0.4088,  0.0911],\n",
      "         ...,\n",
      "         [-0.0380, -0.0000,  0.4705,  ..., -0.2560, -0.1279,  0.0000],\n",
      "         [ 0.0000, -0.1398, -0.6376,  ..., -0.6315,  0.3756,  0.2934],\n",
      "         [-0.0531, -0.1036,  0.0420,  ..., -0.0814, -0.0000, -0.0491]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1982, -0.3191,  0.0507,  ...,  0.2996,  0.3988,  0.0356],\n",
      "         [-0.1575, -0.1458, -0.1640,  ...,  0.1446,  0.1319, -0.0122],\n",
      "         [ 0.2795,  0.0000, -0.0000,  ...,  0.0000,  0.1350, -0.1541],\n",
      "         ...,\n",
      "         [-0.1148, -0.1834,  0.3178,  ..., -0.2295, -0.2991, -0.0140],\n",
      "         [-0.0202, -0.4262,  0.0013,  ..., -0.2557, -0.0805, -0.0000],\n",
      "         [-0.0851, -0.1209,  0.0680,  ..., -0.0048, -0.0000,  0.0196]],\n",
      "\n",
      "        [[ 0.0124, -0.0355,  0.0771,  ...,  0.3629,  0.2339, -0.0000],\n",
      "         [-0.0548, -0.1663,  0.0837,  ...,  0.0000,  0.1694, -0.0000],\n",
      "         [-0.0254, -0.2878,  0.1267,  ...,  0.3048,  0.4035, -0.3561],\n",
      "         ...,\n",
      "         [ 0.0000, -0.1428, -0.0544,  ...,  0.0507, -0.0435,  0.1149],\n",
      "         [ 0.0758, -0.0706,  0.1705,  ..., -0.0757,  0.2209, -0.2548],\n",
      "         [-0.0355, -0.1117,  0.1056,  ..., -0.1061, -0.0000, -0.0194]],\n",
      "\n",
      "        [[ 0.0834, -0.1409,  0.0189,  ...,  0.2354,  0.3306,  0.4842],\n",
      "         [-0.3019, -0.0713, -0.0888,  ...,  0.2410,  0.0632,  0.2632],\n",
      "         [-0.0000, -0.1229, -0.0775,  ...,  0.2673,  0.3930, -0.0000],\n",
      "         ...,\n",
      "         [ 0.0125, -0.2206,  0.1877,  ..., -0.0000, -0.2006,  0.2145],\n",
      "         [-0.0000, -0.0492,  0.0729,  ..., -0.3520, -0.3978, -0.2368],\n",
      "         [-0.0554, -0.0352,  0.0579,  ..., -0.0592,  0.0000,  0.0000]]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "sequence_output linear tensor([[[ 0.2948, -0.2769,  0.2666,  ...,  0.0000, -0.0740, -0.0384],\n",
      "         [ 0.2044, -0.0000,  0.1014,  ...,  0.2211,  0.0728,  0.2155],\n",
      "         [ 0.1039, -0.3108,  0.2959,  ...,  0.2058,  0.1810, -0.1331],\n",
      "         ...,\n",
      "         [ 0.0000, -0.1291,  0.1267,  ..., -0.0000, -0.0000, -0.3554],\n",
      "         [-0.0000, -0.0000,  0.4393,  ..., -0.1195,  0.1870,  0.1223],\n",
      "         [-0.0276, -0.0000,  0.0705,  ...,  0.0254, -0.0602, -0.0000]],\n",
      "\n",
      "        [[ 0.1367, -0.2782, -0.2743,  ...,  0.5419,  0.0875,  0.2212],\n",
      "         [-0.0000, -0.0099, -0.4212,  ...,  0.0340,  0.0337,  0.1165],\n",
      "         [-0.0311, -0.4020,  0.1758,  ..., -0.0210,  0.0049, -0.1653],\n",
      "         ...,\n",
      "         [-0.2815, -0.1699,  0.3378,  ...,  0.2672,  0.0655,  0.0573],\n",
      "         [ 0.0323, -0.1561,  0.1270,  ...,  0.0000, -0.0437,  0.1540],\n",
      "         [-0.0000, -0.0614,  0.0000,  ..., -0.0054,  0.0379,  0.1374]],\n",
      "\n",
      "        [[ 0.2946, -0.1780, -0.2388,  ...,  0.2011,  0.4313,  0.2128],\n",
      "         [ 0.3551, -0.0727, -0.0000,  ...,  0.1980,  0.6387,  0.0000],\n",
      "         [ 0.1505, -0.2979, -0.2297,  ...,  0.0000,  0.4088,  0.0911],\n",
      "         ...,\n",
      "         [-0.0380, -0.0000,  0.4705,  ..., -0.2560, -0.1279,  0.0000],\n",
      "         [ 0.0000, -0.1398, -0.6376,  ..., -0.6315,  0.3756,  0.2934],\n",
      "         [-0.0531, -0.1036,  0.0420,  ..., -0.0814, -0.0000, -0.0491]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1982, -0.3191,  0.0507,  ...,  0.2996,  0.3988,  0.0356],\n",
      "         [-0.1575, -0.1458, -0.1640,  ...,  0.1446,  0.1319, -0.0122],\n",
      "         [ 0.2795,  0.0000, -0.0000,  ...,  0.0000,  0.1350, -0.1541],\n",
      "         ...,\n",
      "         [-0.1148, -0.1834,  0.3178,  ..., -0.2295, -0.2991, -0.0140],\n",
      "         [-0.0202, -0.4262,  0.0013,  ..., -0.2557, -0.0805, -0.0000],\n",
      "         [-0.0851, -0.1209,  0.0680,  ..., -0.0048, -0.0000,  0.0196]],\n",
      "\n",
      "        [[ 0.0124, -0.0355,  0.0771,  ...,  0.3629,  0.2339, -0.0000],\n",
      "         [-0.0548, -0.1663,  0.0837,  ...,  0.0000,  0.1694, -0.0000],\n",
      "         [-0.0254, -0.2878,  0.1267,  ...,  0.3048,  0.4035, -0.3561],\n",
      "         ...,\n",
      "         [ 0.0000, -0.1428, -0.0544,  ...,  0.0507, -0.0435,  0.1149],\n",
      "         [ 0.0758, -0.0706,  0.1705,  ..., -0.0757,  0.2209, -0.2548],\n",
      "         [-0.0355, -0.1117,  0.1056,  ..., -0.1061, -0.0000, -0.0194]],\n",
      "\n",
      "        [[ 0.0834, -0.1409,  0.0189,  ...,  0.2354,  0.3306,  0.4842],\n",
      "         [-0.3019, -0.0713, -0.0888,  ...,  0.2410,  0.0632,  0.2632],\n",
      "         [-0.0000, -0.1229, -0.0775,  ...,  0.2673,  0.3930, -0.0000],\n",
      "         ...,\n",
      "         [ 0.0125, -0.2206,  0.1877,  ..., -0.0000, -0.2006,  0.2145],\n",
      "         [-0.0000, -0.0492,  0.0729,  ..., -0.3520, -0.3978, -0.2368],\n",
      "         [-0.0554, -0.0352,  0.0579,  ..., -0.0592,  0.0000,  0.0000]]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "found labels\n",
      "logits.device mps:0\n",
      "tensor([[   1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3, -100],\n",
      "        [   4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3, -100],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [   4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3, -100],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [   4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3, -100]],\n",
      "       device='mps:0')\n",
      "loss tensor(7.1982e+27, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "loss tensor(7.1982e+27, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "{'loss': 7.198218227241509e+27, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "self.encoder <bound method T5EncoderModel.forward of T5EncoderModelForTokenClassification(\n",
      "  (shared): Embedding(128, 1024)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(\n",
      "                in_features=4096, out_features=1024, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(\n",
      "                in_features=4096, out_features=1024, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (custom_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (custom_classifier): ModulesToSaveWrapper(\n",
      "    (original_module): Linear(in_features=1024, out_features=6, bias=True)\n",
      "    (modules_to_save): ModuleDict(\n",
      "      (default): Linear(in_features=1024, out_features=6, bias=True)\n",
      "    )\n",
      "  )\n",
      ")>\n",
      "self.config.hidden_size 1024\n",
      "self.custom_num_labels 6\n",
      "\n",
      "encoder_outputs.last_hidden_state tensor([[[ 0.1258, -0.4898,  0.3970,  ...,  0.1655,  0.0757,  0.0745],\n",
      "         [-0.0177, -0.2888,  0.0770,  ...,  0.1759,  0.3155,  0.0562],\n",
      "         [ 0.1084,  0.0609, -0.0646,  ..., -0.1142, -0.0382, -0.3531],\n",
      "         ...,\n",
      "         [-0.2728, -0.1399, -0.0552,  ..., -0.0507, -0.1485, -0.4162],\n",
      "         [-0.1256,  0.0933,  0.1446,  ...,  0.1904,  0.0044, -0.3102],\n",
      "         [-0.0265, -0.0900,  0.0405,  ...,  0.0098, -0.1002,  0.0353]],\n",
      "\n",
      "        [[ 0.2768, -0.1358,  0.0627,  ...,  0.2451,  0.2539,  0.1122],\n",
      "         [ 0.0266, -0.3342,  0.0212,  ...,  0.0929, -0.0359,  0.1259],\n",
      "         [ 0.5652, -0.2167,  0.0045,  ...,  0.0212,  0.3683, -0.1575],\n",
      "         ...,\n",
      "         [-0.1425, -0.1494, -0.1502,  ..., -0.0379,  0.2264, -0.0523],\n",
      "         [ 0.2479, -0.0105,  0.0006,  ..., -0.3117,  0.0983,  0.1037],\n",
      "         [-0.0106, -0.0880,  0.0048,  ..., -0.0158, -0.0081,  0.0417]],\n",
      "\n",
      "        [[ 0.1224, -0.2342, -0.2352,  ...,  0.2915,  0.3208,  0.3461],\n",
      "         [ 0.0405, -0.2632,  0.0840,  ...,  0.1484,  0.1852,  0.3580],\n",
      "         [ 0.1108, -0.1123, -0.1375,  ..., -0.2267,  0.0158,  0.0700],\n",
      "         ...,\n",
      "         [-0.2020, -0.0764,  0.0363,  ..., -0.1167,  0.0207,  0.2709],\n",
      "         [-0.1307, -0.0823,  0.0208,  ..., -0.1164, -0.2871,  0.1446],\n",
      "         [-0.0019, -0.0585, -0.0021,  ..., -0.0312, -0.0482, -0.0504]]],\n",
      "       device='mps:0')\n",
      "sequence_output dropout tensor([[[ 0.1258, -0.4898,  0.3970,  ...,  0.1655,  0.0757,  0.0745],\n",
      "         [-0.0177, -0.2888,  0.0770,  ...,  0.1759,  0.3155,  0.0562],\n",
      "         [ 0.1084,  0.0609, -0.0646,  ..., -0.1142, -0.0382, -0.3531],\n",
      "         ...,\n",
      "         [-0.2728, -0.1399, -0.0552,  ..., -0.0507, -0.1485, -0.4162],\n",
      "         [-0.1256,  0.0933,  0.1446,  ...,  0.1904,  0.0044, -0.3102],\n",
      "         [-0.0265, -0.0900,  0.0405,  ...,  0.0098, -0.1002,  0.0353]],\n",
      "\n",
      "        [[ 0.2768, -0.1358,  0.0627,  ...,  0.2451,  0.2539,  0.1122],\n",
      "         [ 0.0266, -0.3342,  0.0212,  ...,  0.0929, -0.0359,  0.1259],\n",
      "         [ 0.5652, -0.2167,  0.0045,  ...,  0.0212,  0.3683, -0.1575],\n",
      "         ...,\n",
      "         [-0.1425, -0.1494, -0.1502,  ..., -0.0379,  0.2264, -0.0523],\n",
      "         [ 0.2479, -0.0105,  0.0006,  ..., -0.3117,  0.0983,  0.1037],\n",
      "         [-0.0106, -0.0880,  0.0048,  ..., -0.0158, -0.0081,  0.0417]],\n",
      "\n",
      "        [[ 0.1224, -0.2342, -0.2352,  ...,  0.2915,  0.3208,  0.3461],\n",
      "         [ 0.0405, -0.2632,  0.0840,  ...,  0.1484,  0.1852,  0.3580],\n",
      "         [ 0.1108, -0.1123, -0.1375,  ..., -0.2267,  0.0158,  0.0700],\n",
      "         ...,\n",
      "         [-0.2020, -0.0764,  0.0363,  ..., -0.1167,  0.0207,  0.2709],\n",
      "         [-0.1307, -0.0823,  0.0208,  ..., -0.1164, -0.2871,  0.1446],\n",
      "         [-0.0019, -0.0585, -0.0021,  ..., -0.0312, -0.0482, -0.0504]]],\n",
      "       device='mps:0')\n",
      "sequence_output linear tensor([[[ 0.1258, -0.4898,  0.3970,  ...,  0.1655,  0.0757,  0.0745],\n",
      "         [-0.0177, -0.2888,  0.0770,  ...,  0.1759,  0.3155,  0.0562],\n",
      "         [ 0.1084,  0.0609, -0.0646,  ..., -0.1142, -0.0382, -0.3531],\n",
      "         ...,\n",
      "         [-0.2728, -0.1399, -0.0552,  ..., -0.0507, -0.1485, -0.4162],\n",
      "         [-0.1256,  0.0933,  0.1446,  ...,  0.1904,  0.0044, -0.3102],\n",
      "         [-0.0265, -0.0900,  0.0405,  ...,  0.0098, -0.1002,  0.0353]],\n",
      "\n",
      "        [[ 0.2768, -0.1358,  0.0627,  ...,  0.2451,  0.2539,  0.1122],\n",
      "         [ 0.0266, -0.3342,  0.0212,  ...,  0.0929, -0.0359,  0.1259],\n",
      "         [ 0.5652, -0.2167,  0.0045,  ...,  0.0212,  0.3683, -0.1575],\n",
      "         ...,\n",
      "         [-0.1425, -0.1494, -0.1502,  ..., -0.0379,  0.2264, -0.0523],\n",
      "         [ 0.2479, -0.0105,  0.0006,  ..., -0.3117,  0.0983,  0.1037],\n",
      "         [-0.0106, -0.0880,  0.0048,  ..., -0.0158, -0.0081,  0.0417]],\n",
      "\n",
      "        [[ 0.1224, -0.2342, -0.2352,  ...,  0.2915,  0.3208,  0.3461],\n",
      "         [ 0.0405, -0.2632,  0.0840,  ...,  0.1484,  0.1852,  0.3580],\n",
      "         [ 0.1108, -0.1123, -0.1375,  ..., -0.2267,  0.0158,  0.0700],\n",
      "         ...,\n",
      "         [-0.2020, -0.0764,  0.0363,  ..., -0.1167,  0.0207,  0.2709],\n",
      "         [-0.1307, -0.0823,  0.0208,  ..., -0.1164, -0.2871,  0.1446],\n",
      "         [-0.0019, -0.0585, -0.0021,  ..., -0.0312, -0.0482, -0.0504]]],\n",
      "       device='mps:0')\n",
      "found labels\n",
      "logits.device mps:0\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100]],\n",
      "       device='mps:0')\n",
      "loss tensor(7.5367e+27, device='mps:0')\n",
      "loss tensor(7.5367e+27, device='mps:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4db63628544f30b07b4b86926148cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.536747561819933e+27, 'eval_accuracy_metric': 0.46948356807511743, 'eval_precision_metric': 0.46948356807511743, 'eval_recall_metric': 0.46948356807511743, 'eval_f1_metric': 0.46948356807511743, 'eval_matthews_correlation': 0.048757524879298776, 'eval_runtime': 15.851, 'eval_samples_per_second': 0.189, 'eval_steps_per_second': 0.063, 'epoch': 1.0}\n",
      "{'train_runtime': 61.9058, 'train_samples_per_second': 0.145, 'train_steps_per_second': 0.016, 'train_loss': 7.198218227241509e+27, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=7.198218227241509e+27, metrics={'train_runtime': 61.9058, 'train_samples_per_second': 0.145, 'train_steps_per_second': 0.016, 'train_loss': 7.198218227241509e+27, 'epoch': 1.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_lora_model.encoder.block[4].layer[0].SelfAttention.v.lora_A.default.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in t5_lora_model.base_model.named_parameters():\n",
    "    if \"lora\" not in name:\n",
    "        continue\n",
    "    if param.isnan().any():\n",
    "        print(f\"New parameter {name:<13} | {param.numel():>5} parameters | not updated\")\n",
    "    else:\n",
    "        print(f\"New parameter {name:<13} | {param.numel():>5} parameters | updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_before = dict(t5_base_model_copy.named_parameters())\n",
    "for name, param in t5_lora_model.base_model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        continue\n",
    "\n",
    "    name_before = name.partition(\".\")[-1].replace(\"original_\", \"\").replace(\"module.\", \"\").replace(\"modules_to_save.default.\", \"\")\n",
    "    param_before = params_before[name_before]\n",
    "    if torch.allclose(param, param_before):\n",
    "        print(f\"Parameter {name_before:<14} | {param.numel():>7} parameters | not updated\")\n",
    "    else:\n",
    "        print(f\"Parameter {name_before:<14} | {param.numel():>7} parameters | updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.encoder <bound method T5EncoderModel.forward of T5EncoderModelForTokenClassification(\n",
      "  (shared): Embedding(128, 1024)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(\n",
      "                in_features=4096, out_features=1024, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(\n",
      "                in_features=4096, out_features=1024, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (custom_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (custom_classifier): ModulesToSaveWrapper(\n",
      "    (original_module): Linear(in_features=1024, out_features=6, bias=True)\n",
      "    (modules_to_save): ModuleDict(\n",
      "      (default): Linear(in_features=1024, out_features=6, bias=True)\n",
      "    )\n",
      "  )\n",
      ")>\n",
      "self.config.hidden_size 1024\n",
      "self.custom_num_labels 6\n",
      "\n",
      "encoder_outputs.last_hidden_state tensor([[[ 0.1258, -0.4898,  0.3970,  ...,  0.1655,  0.0757,  0.0745],\n",
      "         [-0.0177, -0.2888,  0.0770,  ...,  0.1759,  0.3155,  0.0562],\n",
      "         [ 0.1084,  0.0609, -0.0646,  ..., -0.1142, -0.0382, -0.3531],\n",
      "         ...,\n",
      "         [-0.2728, -0.1399, -0.0552,  ..., -0.0507, -0.1485, -0.4162],\n",
      "         [-0.1256,  0.0933,  0.1446,  ...,  0.1904,  0.0044, -0.3102],\n",
      "         [-0.0265, -0.0900,  0.0405,  ...,  0.0098, -0.1002,  0.0353]],\n",
      "\n",
      "        [[ 0.2768, -0.1358,  0.0627,  ...,  0.2451,  0.2539,  0.1122],\n",
      "         [ 0.0266, -0.3342,  0.0212,  ...,  0.0929, -0.0359,  0.1259],\n",
      "         [ 0.5652, -0.2167,  0.0045,  ...,  0.0212,  0.3683, -0.1575],\n",
      "         ...,\n",
      "         [-0.1425, -0.1494, -0.1502,  ..., -0.0379,  0.2264, -0.0523],\n",
      "         [ 0.2479, -0.0105,  0.0006,  ..., -0.3117,  0.0983,  0.1037],\n",
      "         [-0.0106, -0.0880,  0.0048,  ..., -0.0158, -0.0081,  0.0417]],\n",
      "\n",
      "        [[ 0.1224, -0.2342, -0.2352,  ...,  0.2915,  0.3208,  0.3461],\n",
      "         [ 0.0405, -0.2632,  0.0840,  ...,  0.1484,  0.1852,  0.3580],\n",
      "         [ 0.1108, -0.1123, -0.1375,  ..., -0.2267,  0.0158,  0.0700],\n",
      "         ...,\n",
      "         [-0.2020, -0.0764,  0.0363,  ..., -0.1167,  0.0207,  0.2709],\n",
      "         [-0.1307, -0.0823,  0.0208,  ..., -0.1164, -0.2871,  0.1446],\n",
      "         [-0.0019, -0.0585, -0.0021,  ..., -0.0312, -0.0482, -0.0504]]],\n",
      "       device='mps:0')\n",
      "sequence_output dropout tensor([[[ 0.1258, -0.4898,  0.3970,  ...,  0.1655,  0.0757,  0.0745],\n",
      "         [-0.0177, -0.2888,  0.0770,  ...,  0.1759,  0.3155,  0.0562],\n",
      "         [ 0.1084,  0.0609, -0.0646,  ..., -0.1142, -0.0382, -0.3531],\n",
      "         ...,\n",
      "         [-0.2728, -0.1399, -0.0552,  ..., -0.0507, -0.1485, -0.4162],\n",
      "         [-0.1256,  0.0933,  0.1446,  ...,  0.1904,  0.0044, -0.3102],\n",
      "         [-0.0265, -0.0900,  0.0405,  ...,  0.0098, -0.1002,  0.0353]],\n",
      "\n",
      "        [[ 0.2768, -0.1358,  0.0627,  ...,  0.2451,  0.2539,  0.1122],\n",
      "         [ 0.0266, -0.3342,  0.0212,  ...,  0.0929, -0.0359,  0.1259],\n",
      "         [ 0.5652, -0.2167,  0.0045,  ...,  0.0212,  0.3683, -0.1575],\n",
      "         ...,\n",
      "         [-0.1425, -0.1494, -0.1502,  ..., -0.0379,  0.2264, -0.0523],\n",
      "         [ 0.2479, -0.0105,  0.0006,  ..., -0.3117,  0.0983,  0.1037],\n",
      "         [-0.0106, -0.0880,  0.0048,  ..., -0.0158, -0.0081,  0.0417]],\n",
      "\n",
      "        [[ 0.1224, -0.2342, -0.2352,  ...,  0.2915,  0.3208,  0.3461],\n",
      "         [ 0.0405, -0.2632,  0.0840,  ...,  0.1484,  0.1852,  0.3580],\n",
      "         [ 0.1108, -0.1123, -0.1375,  ..., -0.2267,  0.0158,  0.0700],\n",
      "         ...,\n",
      "         [-0.2020, -0.0764,  0.0363,  ..., -0.1167,  0.0207,  0.2709],\n",
      "         [-0.1307, -0.0823,  0.0208,  ..., -0.1164, -0.2871,  0.1446],\n",
      "         [-0.0019, -0.0585, -0.0021,  ..., -0.0312, -0.0482, -0.0504]]],\n",
      "       device='mps:0')\n",
      "sequence_output linear tensor([[[ 0.1258, -0.4898,  0.3970,  ...,  0.1655,  0.0757,  0.0745],\n",
      "         [-0.0177, -0.2888,  0.0770,  ...,  0.1759,  0.3155,  0.0562],\n",
      "         [ 0.1084,  0.0609, -0.0646,  ..., -0.1142, -0.0382, -0.3531],\n",
      "         ...,\n",
      "         [-0.2728, -0.1399, -0.0552,  ..., -0.0507, -0.1485, -0.4162],\n",
      "         [-0.1256,  0.0933,  0.1446,  ...,  0.1904,  0.0044, -0.3102],\n",
      "         [-0.0265, -0.0900,  0.0405,  ...,  0.0098, -0.1002,  0.0353]],\n",
      "\n",
      "        [[ 0.2768, -0.1358,  0.0627,  ...,  0.2451,  0.2539,  0.1122],\n",
      "         [ 0.0266, -0.3342,  0.0212,  ...,  0.0929, -0.0359,  0.1259],\n",
      "         [ 0.5652, -0.2167,  0.0045,  ...,  0.0212,  0.3683, -0.1575],\n",
      "         ...,\n",
      "         [-0.1425, -0.1494, -0.1502,  ..., -0.0379,  0.2264, -0.0523],\n",
      "         [ 0.2479, -0.0105,  0.0006,  ..., -0.3117,  0.0983,  0.1037],\n",
      "         [-0.0106, -0.0880,  0.0048,  ..., -0.0158, -0.0081,  0.0417]],\n",
      "\n",
      "        [[ 0.1224, -0.2342, -0.2352,  ...,  0.2915,  0.3208,  0.3461],\n",
      "         [ 0.0405, -0.2632,  0.0840,  ...,  0.1484,  0.1852,  0.3580],\n",
      "         [ 0.1108, -0.1123, -0.1375,  ..., -0.2267,  0.0158,  0.0700],\n",
      "         ...,\n",
      "         [-0.2020, -0.0764,  0.0363,  ..., -0.1167,  0.0207,  0.2709],\n",
      "         [-0.1307, -0.0823,  0.0208,  ..., -0.1164, -0.2871,  0.1446],\n",
      "         [-0.0019, -0.0585, -0.0021,  ..., -0.0312, -0.0482, -0.0504]]],\n",
      "       device='mps:0')\n",
      "found labels\n",
      "logits.device mps:0\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100]],\n",
      "       device='mps:0')\n",
      "loss tensor(7.5367e+27, device='mps:0')\n",
      "loss tensor(7.5367e+27, device='mps:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864e0f382f8b45b0b42b6d20049e94ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.536747561819933e+27, 'eval_accuracy_metric': 0.46948356807511743, 'eval_precision_metric': 0.46948356807511743, 'eval_recall_metric': 0.46948356807511743, 'eval_f1_metric': 0.46948356807511743, 'eval_matthews_correlation': 0.048757524879298776, 'eval_runtime': 20.0174, 'eval_samples_per_second': 0.15, 'eval_steps_per_second': 0.05, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "metrics=trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy_metric</th>\n",
       "      <th>eval_precision_metric</th>\n",
       "      <th>eval_recall_metric</th>\n",
       "      <th>eval_f1_metric</th>\n",
       "      <th>eval_matthews_correlation</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.198218e+27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.536748e+27</td>\n",
       "      <td>0.469484</td>\n",
       "      <td>0.469484</td>\n",
       "      <td>0.469484</td>\n",
       "      <td>0.469484</td>\n",
       "      <td>0.048758</td>\n",
       "      <td>15.8510</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.9058</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.016</td>\n",
       "      <td>4.646632e+12</td>\n",
       "      <td>7.198218e+27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.536748e+27</td>\n",
       "      <td>0.469484</td>\n",
       "      <td>0.469484</td>\n",
       "      <td>0.469484</td>\n",
       "      <td>0.469484</td>\n",
       "      <td>0.048758</td>\n",
       "      <td>20.0174</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           loss  learning_rate  epoch  step     eval_loss  \\\n",
       "0  7.198218e+27            0.0    1.0     1           NaN   \n",
       "1           NaN            NaN    1.0     1  7.536748e+27   \n",
       "2           NaN            NaN    1.0     1           NaN   \n",
       "3           NaN            NaN    1.0     1  7.536748e+27   \n",
       "\n",
       "   eval_accuracy_metric  eval_precision_metric  eval_recall_metric  \\\n",
       "0                   NaN                    NaN                 NaN   \n",
       "1              0.469484               0.469484            0.469484   \n",
       "2                   NaN                    NaN                 NaN   \n",
       "3              0.469484               0.469484            0.469484   \n",
       "\n",
       "   eval_f1_metric  eval_matthews_correlation  eval_runtime  \\\n",
       "0             NaN                        NaN           NaN   \n",
       "1        0.469484                   0.048758       15.8510   \n",
       "2             NaN                        NaN           NaN   \n",
       "3        0.469484                   0.048758       20.0174   \n",
       "\n",
       "   eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "0                      NaN                    NaN            NaN   \n",
       "1                    0.189                  0.063            NaN   \n",
       "2                      NaN                    NaN        61.9058   \n",
       "3                    0.150                  0.050            NaN   \n",
       "\n",
       "   train_samples_per_second  train_steps_per_second    total_flos  \\\n",
       "0                       NaN                     NaN           NaN   \n",
       "1                       NaN                     NaN           NaN   \n",
       "2                     0.145                   0.016  4.646632e+12   \n",
       "3                       NaN                     NaN           NaN   \n",
       "\n",
       "     train_loss  \n",
       "0           NaN  \n",
       "1           NaN  \n",
       "2  7.198218e+27  \n",
       "3           NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_log = pd.DataFrame(trainer.state.log_history)\n",
    "display(training_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_location = '/models/testing_4'\n",
    "t5_lora_model.save_pretrained(ROOT + adapter_location)\n",
    "training_log.to_parquet(ROOT + adapter_location + '/training_log.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(sequence: str, tokenizer: T5Tokenizer, model: T5EncoderModelForTokenClassification):\n",
    "    # print('sequence', sequence)\n",
    "    tokenized_string = tokenizer.encode(sequence, padding=True, truncation=True, return_tensors=\"pt\", max_length=1024)\n",
    "    # print('tokenized_string', tokenized_string)\n",
    "    with torch.no_grad():\n",
    "        output = model(tokenized_string.to(device))\n",
    "    # print('output', output)\n",
    "    return output\n",
    "\n",
    "def tranlate_logits(logits):\n",
    "    return [src.config.label_decoding[x] for x in logits.argmax(-1).tolist()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M A A V I L E R L G A L W V Q N L R G K L A L G I L P Q S H I H T S A S L E I S R K W E K K N K I V Y P P Q L P G E P R R P A E I Y H C R R</s>\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I']\n"
     ]
    }
   ],
   "source": [
    "_ds_index = 2\n",
    "_ds_type = 'test'\n",
    "\n",
    "_inids_test = t5_tokenizer.decode(dataset_signalp[_ds_type][_ds_index]['input_ids'])\n",
    "_labels_test = dataset_signalp[_ds_type][_ds_index]['labels']\n",
    "_labels_test_decoded = [src.config.label_decoding[x] for x in _labels_test]\n",
    "print(_inids_test)\n",
    "print(_labels_test)\n",
    "print(_labels_test_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/.venv/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5.py:303: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.encoder <bound method T5EncoderModel.forward of T5EncoderModelForTokenClassification(\n",
      "  (shared): Embedding(128, 1024)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(\n",
      "                in_features=4096, out_features=1024, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v): Linear(\n",
      "                in_features=1024, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(\n",
      "                in_features=4096, out_features=1024, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (custom_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (custom_classifier): ModulesToSaveWrapper(\n",
      "    (original_module): Linear(in_features=1024, out_features=6, bias=True)\n",
      "    (modules_to_save): ModuleDict(\n",
      "      (default): Linear(in_features=1024, out_features=6, bias=True)\n",
      "    )\n",
      "  )\n",
      ")>\n",
      "self.config.hidden_size 1024\n",
      "self.custom_num_labels 6\n",
      "\n",
      "encoder_outputs.last_hidden_state tensor([[[ 0.1609,  0.0306, -0.0656,  ...,  0.2968,  0.2651,  0.2967],\n",
      "         [-0.0069, -0.0748, -0.0185,  ...,  0.3656,  0.4584,  0.0053],\n",
      "         [ 0.0914, -0.2668,  0.1158,  ...,  0.1771,  0.3154, -0.1050],\n",
      "         ...,\n",
      "         [-0.0575, -0.1665,  0.1147,  ..., -0.0512,  0.0070,  0.0021],\n",
      "         [-0.0386, -0.2055, -0.1462,  ..., -0.0826,  0.0652, -0.0878],\n",
      "         [-0.0418, -0.0755,  0.1421,  ..., -0.0593, -0.0271,  0.0060]]],\n",
      "       device='mps:0')\n",
      "sequence_output dropout tensor([[[ 0.1609,  0.0306, -0.0656,  ...,  0.2968,  0.2651,  0.2967],\n",
      "         [-0.0069, -0.0748, -0.0185,  ...,  0.3656,  0.4584,  0.0053],\n",
      "         [ 0.0914, -0.2668,  0.1158,  ...,  0.1771,  0.3154, -0.1050],\n",
      "         ...,\n",
      "         [-0.0575, -0.1665,  0.1147,  ..., -0.0512,  0.0070,  0.0021],\n",
      "         [-0.0386, -0.2055, -0.1462,  ..., -0.0826,  0.0652, -0.0878],\n",
      "         [-0.0418, -0.0755,  0.1421,  ..., -0.0593, -0.0271,  0.0060]]],\n",
      "       device='mps:0')\n",
      "sequence_output linear tensor([[[ 0.1609,  0.0306, -0.0656,  ...,  0.2968,  0.2651,  0.2967],\n",
      "         [-0.0069, -0.0748, -0.0185,  ...,  0.3656,  0.4584,  0.0053],\n",
      "         [ 0.0914, -0.2668,  0.1158,  ...,  0.1771,  0.3154, -0.1050],\n",
      "         ...,\n",
      "         [-0.0575, -0.1665,  0.1147,  ..., -0.0512,  0.0070,  0.0021],\n",
      "         [-0.0386, -0.2055, -0.1462,  ..., -0.0826,  0.0652, -0.0878],\n",
      "         [-0.0418, -0.0755,  0.1421,  ..., -0.0593, -0.0271,  0.0060]]],\n",
      "       device='mps:0')\n",
      "loss None\n"
     ]
    }
   ],
   "source": [
    "preds = predict_model(_inids_test, t5_tokenizer, t5_lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'L', 'O', 'I', 'I', 'I', 'I', 'I', 'L', 'L', 'O', 'I', 'T', 'L', 'O', 'I', 'L', 'L', 'I', 'L', 'L', 'L', 'M', 'L', 'T', 'L', 'I', 'L', 'L', 'L', 'L', 'L', 'I', 'L', 'L', 'L', 'M', 'M', 'I', 'L', 'O', 'S', 'I', 'I', 'L', 'L', 'I', 'O', 'I', 'I', 'I', 'L', 'L', 'L', 'L', 'L', 'L', 'O', 'L', 'L', 'O', 'L', 'L', 'O', 'I', 'I', 'O', 'T', 'O', 'O', 'L']\n"
     ]
    }
   ],
   "source": [
    "_res = tranlate_logits(preds.logits.cpu().numpy())\n",
    "print(_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x for x in t5_lora_model.custom_classifier.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
