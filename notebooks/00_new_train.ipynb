{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/.venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback\n",
    ")\n",
    "\n",
    "from src.model_new import (\n",
    "    T5EncoderModelForTokenClassification\n",
    ")\n",
    "\n",
    "import src.config\n",
    "import src.data\n",
    "import src.model_new\n",
    "import src.utils\n",
    "\n",
    "\n",
    "import peft\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    ")\n",
    "\n",
    "\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model:\t Rostlab/prot_t5_xl_uniref50\n",
      "MPS:\t\t True\n",
      "Path:\t\t /Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction\n",
      "Using device:\t mps\n"
     ]
    }
   ],
   "source": [
    "print(\"Base Model:\\t\", src.config.base_model_name)\n",
    "print(\"MPS:\\t\\t\", torch.backends.mps.is_available())\n",
    "ROOT = src.utils.get_project_root_path()\n",
    "print(\"Path:\\t\\t\", ROOT)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f\"Using device:\\t {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Tokenizer and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=src.config.base_model_name,\n",
    "    do_lower_case=False,\n",
    "    use_fast=True,\n",
    "    legacy=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5EncoderModelForTokenClassification were not initialized from the model checkpoint at Rostlab/prot_t5_xl_uniref50 and are newly initialized: ['custom_classifier.bias', 'custom_classifier.weight', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "t5_base_model = T5EncoderModelForTokenClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=src.config.base_model_name,\n",
    "    device_map='auto',\n",
    "    load_in_8bit=False,\n",
    "    custom_num_labels=len(src.config.label_decoding),\n",
    "    custom_dropout_rate=0.1,\n",
    "    use_crf=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_base_model.custom_classifier.weight = nn.Linear(\n",
    "    in_features=t5_base_model.config.hidden_size,\n",
    "    out_features=t5_base_model.custom_num_labels).weight\n",
    "t5_base_model.crf.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,944,556 || all params: 1,212,086,380 || trainable%: 0.3254352218692532\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q', 'k', 'v', 'o'],\n",
    "    bias=\"none\",\n",
    "    modules_to_save=['custom_classifier', 'crf'],\n",
    ")\n",
    "\n",
    "t5_lora_model = peft.get_peft_model(t5_base_model, lora_config)\n",
    "t5_lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_lora_model.base_model.custom_classifier.modules_to_save.default.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x[0] for x in t5_base_model.crf.named_parameters()]\n",
    "# t5_lora_model.crf.transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x for x in t5_lora_model.named_parameters() if 'crf' in x[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Data, Split into Dataset, and Tokenize Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/.venv/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "FASTA_FILENAME = '5_SignalP_5.0_Training_set.fasta'\n",
    "# FASTA_FILENAME = '5_SignalP_5.0_Training_set_testing.fasta'\n",
    "annotations_name = 'Label' # Choose Type or Label\n",
    "\n",
    "df_data = src.data.process(src.data.parse_file(ROOT + '/data/raw/' + FASTA_FILENAME))\n",
    "\n",
    "dataset_signalp = src.model_new.create_datasets(\n",
    "    splits=src.config.splits,\n",
    "    tokenizer=t5_tokenizer,\n",
    "    data=df_data,\n",
    "    annotations_name=annotations_name,\n",
    "    # dataset_size=src.config.dataset_size,\n",
    "    dataset_size=3,\n",
    "    encoder=src.config.select_encodings[annotations_name],\n",
    "    )\n",
    "\n",
    "del df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 9, 13, 12, 10, 10, 12, 4, 15, 9, 6, 11, 10, 3, 15, 14, 11, 16, 14, 9, 10, 4, 4, 9, 4, 6, 11, 4, 12, 10, 12, 18, 5, 9, 16, 6, 17, 16, 9, 5, 7, 18, 9, 9, 14, 11, 8, 15, 12, 9, 11, 4, 17, 11, 4, 4, 9, 10, 17, 13, 7, 11, 11, 5, 9, 12, 5, 21, 10, 4, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "display(dataset_signalp)\n",
    "\n",
    "ds_index = 0\n",
    "print(dataset_signalp['valid'][ds_index]['input_ids'])\n",
    "print(dataset_signalp['valid'][ds_index]['labels'])\n",
    "print(dataset_signalp['valid'][ds_index]['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchcrf import CRF\n",
    "num_tags = 5\n",
    "model = CRF(num_tags=num_tags, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9631,  1.0844, -1.5489,  2.1360,  0.0671],\n",
       "         [ 0.7196, -0.4391, -1.2591, -2.4885, -0.5998],\n",
       "         [ 0.3651, -2.0420,  0.7909,  0.2740,  1.1014],\n",
       "         [ 0.2062,  0.2345, -0.7621, -0.4025,  1.3991]],\n",
       "\n",
       "        [[ 1.0017, -0.4141, -0.2636, -2.1617,  0.5561],\n",
       "         [ 0.1655,  0.3286, -0.6225, -0.5243, -0.1821],\n",
       "         [-0.1025, -0.4402, -1.6304,  0.0114, -0.0502],\n",
       "         [ 2.1811,  0.5598,  0.8657,  0.4453,  0.1986]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 3],\n",
       "        [2, 2, 2, 3]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/.venv/lib/python3.11/site-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorCompare.cpp:493.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-17.9651, grad_fn=<SumBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[3, 0, 4, 4], [0, 1, 3, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_length = 4\n",
    "emissions = torch.randn(batch_size, seq_length, num_tags)\n",
    "tags = torch.tensor([\n",
    "    [1, 2, 3, 3],\n",
    "    [2, 2, 2, 3]\n",
    "    ], dtype=torch.long)  # (seq_length, batch_size)\n",
    "\n",
    "display(emissions, emissions.shape)\n",
    "display(tags, tags.shape)\n",
    "display(model(emissions, tags))\n",
    "display(torch.Tensor(model.decode(emissions)).shape)\n",
    "display(model.decode(emissions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Loop\n",
    "https://huggingface.co/docs/peft/task_guides/token-classification-lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=t5_tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./checkpoints',\n",
    "    learning_rate=src.config.lr,\n",
    "    per_device_train_batch_size=src.config.batch_size,\n",
    "    per_device_eval_batch_size=src.config.batch_size,\n",
    "    num_train_epochs=src.config.num_epochs,\n",
    "    logging_steps=src.config.logging_steps,\n",
    "    # save_strategy=\"steps\",\n",
    "    # save_steps=src.config.save_steps,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=src.config.eval_steps,\n",
    "    # gradient_accumulation_steps=accum,\n",
    "    # load_best_model_at_end=True,\n",
    "    # save_total_limit=5,\n",
    "    seed=42,\n",
    "    # fp16=True,\n",
    "    # deepspeed=deepspeed_config,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=['labels'],\n",
    "    # debug=\"underflow_overflow\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=t5_lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_signalp['train'],\n",
    "    eval_dataset=dataset_signalp['valid'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=src.model_new.compute_metrics,\n",
    ")\n",
    "\n",
    "# class EvaluateFirstStepCallback(TrainerCallback):\n",
    "#     def on_step_begin(self, args, state, control, **kwargs):\n",
    "#         if state.global_step == 0:\n",
    "#             control.should_evaluate = True\n",
    "# trainer.add_callback(EvaluateFirstStepCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbe67b37e22454f8ce2de2457bb1f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/75428952-3aa4-11ee-8b65-46d450270006/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x9x1x1xi1>'\n",
      "/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/.venv/lib/python3.11/site-packages/torchcrf/__init__.py:305: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/TensorCompare.mm:298.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
      "/Users/finnlueth/Developer/gits/prottrans-t5-signalpeptide-prediction/.venv/lib/python3.11/site-packages/torchcrf/__init__.py:315: UserWarning: MPS: no support for int64 reduction ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:144.)\n",
      "  seq_ends = mask.long().sum(dim=0) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded_tags [[1, 5, 2, 2, 2, 2, 2, 2, 2, 4, 3, 3, 2, 2, 4, 4, 4, 3, 2, 4, 3, 2, 4, 4, 3, 2, 4, 4, 5, 5, 0, 0, 3, 4, 0, 4, 0, 5, 4, 2, 2, 0, 4, 0, 0, 3, 2, 4, 3, 3, 3, 3, 3, 5, 1, 0, 3, 3, 2, 4, 1, 4, 3, 2, 4, 3, 2, 0, 3, 3, 2], [5, 4, 2, 2, 1, 5, 5, 3, 3, 2, 2, 2, 4, 4, 3, 3, 3, 3, 3, 3, 5, 5, 1, 5, 4, 3, 3, 3, 3, 3, 3, 3, 2, 4, 4, 4, 4, 4, 4, 3, 2, 2, 4, 4, 2, 4, 4, 3, 3, 3, 3, 2, 2, 0, 4, 3, 3, 2, 4, 0, 2, 2, 4, 0, 3, 2, 0, 5, 2, 2, 2], [5, 2, 2, 2, 5, 4, 1, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 4, 5, 2, 4, 4, 4, 1, 5, 2, 4, 4, 2, 2, 2, 2, 2, 2, 0, 0, 5, 1, 5, 4, 0, 2, 4, 1, 5, 4, 3, 2, 0, 3, 2, 4, 0, 4, 4, 3, 3, 2, 4, 4, 0, 2, 0, 3, 2], [5, 4, 4, 4, 4, 0, 2, 4, 3, 2, 2, 2, 2, 4, 1, 1, 5, 2, 1, 5, 4, 1, 5, 5, 0, 4, 3, 3, 3, 4, 4, 4, 3, 5, 4, 4, 3, 3, 3, 2, 2, 4, 0, 4, 5, 4, 3, 2, 4, 0, 4, 3, 5, 0, 0, 3, 3, 3, 3, 2, 5, 5, 4, 3, 3, 3, 2, 4, 0, 3, 2], [5, 2, 4, 0, 4, 4, 4, 4, 4, 3, 3, 2, 4, 4, 4, 4, 1, 5, 1, 1, 5, 4, 2, 4, 3, 5, 0, 2, 2, 4, 3, 3, 3, 5, 4, 1, 2, 4, 1, 5, 5, 1, 0, 4, 5, 0, 5, 4, 2, 2, 4, 0, 2, 4, 4, 4, 4, 4, 0, 1, 3, 3, 2, 0, 5, 5, 4, 2, 4, 3, 2], [5, 5, 1, 5, 2, 2, 4, 3, 2, 4, 4, 2, 2, 4, 4, 4, 1, 5, 4, 4, 3, 2, 4, 3, 2, 4, 1, 5, 2, 4, 3, 3, 3, 2, 2, 2, 4, 5, 5, 0, 2, 2, 4, 0, 3, 5, 4, 0, 3, 3, 3, 3, 3, 3, 2, 2, 4, 3, 3, 3, 3, 2, 5, 4, 4, 4, 3, 2, 4, 0, 2], [4, 3, 2, 2, 2, 2, 4, 0, 0, 5, 4, 4, 3, 3, 2, 2, 2, 0, 2, 4, 4, 0, 4, 1, 4, 3, 2, 2, 4, 4, 0, 0, 4, 0, 0, 4, 4, 4, 3, 3, 3, 2, 4, 0, 0, 4, 0, 4, 3, 3, 3, 3, 1, 5, 5, 4, 0, 5, 5, 1, 0, 0, 1, 5, 0, 0, 5, 4, 4, 0, 2], [4, 3, 3, 3, 3, 1, 0, 4, 4, 3, 2, 4, 4, 0, 4, 0, 0, 4, 4, 3, 3, 3, 3, 3, 3, 2, 2, 2, 4, 3, 3, 2, 4, 4, 1, 5, 4, 1, 0, 3, 3, 3, 4, 3, 3, 5, 3, 2, 4, 3, 3, 1, 0, 0, 3, 3, 1, 5, 1, 3, 3, 3, 3, 5, 3, 3, 2, 0, 3, 3, 2], [4, 4, 3, 3, 2, 4, 0, 2, 2, 2, 2, 2, 2, 4, 3, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 3, 2, 4, 1, 5, 2, 2, 4, 1, 5, 2, 4, 3, 3, 3, 2, 2, 0, 3, 3, 3, 3, 2, 4, 0, 4, 3, 4, 4, 1, 5, 0, 4, 4, 4, 0, 3, 3, 3, 2, 0, 3, 2, 5, 2, 2]]\n",
      "decoded_tags None <class 'NoneType'>\n",
      "logits tensor([[[-0.0536,  0.1498,  0.1446, -0.1542,  0.0847,  0.1254],\n",
      "         [-0.1353,  0.0661, -0.0677, -0.1022, -0.1225,  0.0804],\n",
      "         [-0.0962,  0.1232,  0.0978, -0.1322, -0.0427,  0.0197],\n",
      "         ...,\n",
      "         [ 0.0467, -0.0308,  0.0423,  0.1644, -0.0531, -0.0004],\n",
      "         [-0.0343, -0.1441, -0.0163,  0.0958,  0.0391, -0.0531],\n",
      "         [-0.0158, -0.0200,  0.0685, -0.0532, -0.0921, -0.0291]],\n",
      "\n",
      "        [[-0.1810,  0.1166, -0.0270, -0.0780, -0.0794,  0.1323],\n",
      "         [-0.0257, -0.0578,  0.0186, -0.1329,  0.1380, -0.0222],\n",
      "         [-0.1490, -0.1659,  0.1000, -0.0026, -0.0243, -0.0353],\n",
      "         ...,\n",
      "         [-0.0734, -0.2610, -0.0236, -0.0817, -0.1654,  0.0068],\n",
      "         [-0.0132, -0.0547,  0.1986,  0.0568, -0.0553, -0.0595],\n",
      "         [-0.0037, -0.0267,  0.0830, -0.0530, -0.0821, -0.0459]],\n",
      "\n",
      "        [[ 0.0709, -0.1194,  0.0222, -0.3121,  0.1222,  0.1081],\n",
      "         [-0.1172, -0.0211,  0.0798, -0.1685,  0.0260,  0.0097],\n",
      "         [ 0.0503, -0.0698,  0.0934, -0.2528, -0.1839,  0.0469],\n",
      "         ...,\n",
      "         [ 0.1419,  0.0952, -0.1556, -0.0116, -0.0848, -0.1325],\n",
      "         [-0.0919, -0.0831, -0.1719,  0.0678,  0.0517, -0.0620],\n",
      "         [-0.0347, -0.0360,  0.0599, -0.0874, -0.0908, -0.0682]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1313, -0.0120,  0.0226, -0.0369,  0.1053, -0.0529],\n",
      "         [ 0.0651,  0.1112,  0.0698,  0.0466,  0.1606, -0.0088],\n",
      "         [-0.1854, -0.0206,  0.2804,  0.1099, -0.0180,  0.0642],\n",
      "         ...,\n",
      "         [-0.0428,  0.0497, -0.1248,  0.0586,  0.1473,  0.0783],\n",
      "         [ 0.0662, -0.0677,  0.0084, -0.0384, -0.0301,  0.0941],\n",
      "         [-0.0503, -0.0249,  0.0595, -0.0626, -0.0934, -0.0146]],\n",
      "\n",
      "        [[-0.1022, -0.0026,  0.0257, -0.0637,  0.1103,  0.0187],\n",
      "         [-0.1629, -0.0271, -0.0618,  0.0549,  0.0174, -0.0422],\n",
      "         [-0.2067, -0.0824,  0.0464,  0.1287, -0.0916, -0.1214],\n",
      "         ...,\n",
      "         [-0.0033, -0.0373, -0.0539,  0.0182, -0.0596, -0.0689],\n",
      "         [ 0.0320,  0.0542, -0.0522,  0.0516,  0.0360, -0.0098],\n",
      "         [ 0.0031, -0.0729,  0.0741, -0.0599, -0.0748, -0.0689]],\n",
      "\n",
      "        [[-0.0160, -0.0418,  0.0310, -0.1170,  0.2128,  0.0382],\n",
      "         [-0.0587, -0.0077,  0.0167,  0.0580,  0.1913,  0.0108],\n",
      "         [-0.0282, -0.0316,  0.1023,  0.1636,  0.0504,  0.0960],\n",
      "         ...,\n",
      "         [-0.0421, -0.0975, -0.1437, -0.0806, -0.1305,  0.0251],\n",
      "         [ 0.1989, -0.0747,  0.1803, -0.0477,  0.0761,  0.0787],\n",
      "         [-0.0170, -0.0086,  0.0240, -0.0446, -0.0527, -0.0397]]],\n",
      "       device='mps:0', grad_fn=<LinearBackward0>) torch.Size([9, 71, 6])\n",
      "loss tensor(1133.2013, device='mps:0', grad_fn=<NegBackward0>)\n",
      "logic None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/75428952-3aa4-11ee-8b65-46d450270006/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x9x1x1xi1>'\n",
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/75428952-3aa4-11ee-8b65-46d450270006/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x9x1x1xi1>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1133.2013, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 83.5951, 'train_samples_per_second': 0.108, 'train_steps_per_second': 0.012, 'train_loss': 1133.2012939453125, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=1133.2012939453125, metrics={'train_runtime': 83.5951, 'train_samples_per_second': 0.108, 'train_steps_per_second': 0.012, 'train_loss': 1133.2012939453125, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b012aa09df04db4bfeb3dae4aa8d864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions [[[-0.18970905  0.18024121  0.06065112 -0.11171746  0.16198713\n",
      "   -0.02602383]\n",
      "  [-0.05944495  0.09248458 -0.17799744 -0.15251164 -0.10702872\n",
      "    0.13930583]\n",
      "  [-0.29510376  0.20085649  0.25969154  0.10611022  0.35970443\n",
      "   -0.13364089]\n",
      "  ...\n",
      "  [ 0.31057712 -0.00123025 -0.23006034 -0.10129994  0.27407625\n",
      "    0.19301315]\n",
      "  [ 0.13187079 -0.11723104 -0.19628857  0.02858959  0.11800069\n",
      "    0.01459896]\n",
      "  [-0.04808104 -0.0138107   0.08388251 -0.06492199 -0.06727362\n",
      "   -0.05280718]]\n",
      "\n",
      " [[-0.15894619  0.07284678  0.14737347 -0.1276831   0.25255606\n",
      "    0.09391508]\n",
      "  [ 0.00786223  0.0871888   0.18400078 -0.19992264  0.26098937\n",
      "    0.00664918]\n",
      "  [-0.0803908   0.03582398  0.14990774 -0.09686233  0.14528047\n",
      "    0.01309295]\n",
      "  ...\n",
      "  [ 0.15231846 -0.09140155 -0.04400397  0.13401577  0.05537358\n",
      "    0.1083131 ]\n",
      "  [ 0.05493654 -0.05631565  0.06448226  0.03003671 -0.03381815\n",
      "   -0.20454642]\n",
      "  [-0.02573016 -0.04879984  0.07133649 -0.05633134 -0.0750716\n",
      "   -0.02148213]]\n",
      "\n",
      " [[-0.12695628  0.01386457  0.04659856 -0.01238302  0.14942427\n",
      "    0.06496427]\n",
      "  [-0.08160642  0.05387551  0.06533185  0.03375872 -0.05284474\n",
      "   -0.03361593]\n",
      "  [-0.14552586 -0.0917469  -0.01051962  0.08451021  0.11369187\n",
      "    0.01078599]\n",
      "  ...\n",
      "  [ 0.02522668  0.0062766  -0.10254629 -0.06719357  0.04975696\n",
      "   -0.12523745]\n",
      "  [ 0.00046304  0.05157395  0.11419207 -0.01862753 -0.10510913\n",
      "   -0.2524298 ]\n",
      "  [ 0.01254023 -0.048389    0.03970297 -0.08745316 -0.06572326\n",
      "   -0.04814808]]]\n",
      "references [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  -100]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  -100]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  -100]]\n",
      "predictions [[[-0.18970905  0.18024121  0.06065112 -0.11171746  0.16198713\n",
      "   -0.02602383]\n",
      "  [-0.05944495  0.09248458 -0.17799744 -0.15251164 -0.10702872\n",
      "    0.13930583]\n",
      "  [-0.29510376  0.20085649  0.25969154  0.10611022  0.35970443\n",
      "   -0.13364089]\n",
      "  ...\n",
      "  [ 0.31057712 -0.00123025 -0.23006034 -0.10129994  0.27407625\n",
      "    0.19301315]\n",
      "  [ 0.13187079 -0.11723104 -0.19628857  0.02858959  0.11800069\n",
      "    0.01459896]\n",
      "  [-0.04808104 -0.0138107   0.08388251 -0.06492199 -0.06727362\n",
      "   -0.05280718]]\n",
      "\n",
      " [[-0.15894619  0.07284678  0.14737347 -0.1276831   0.25255606\n",
      "    0.09391508]\n",
      "  [ 0.00786223  0.0871888   0.18400078 -0.19992264  0.26098937\n",
      "    0.00664918]\n",
      "  [-0.0803908   0.03582398  0.14990774 -0.09686233  0.14528047\n",
      "    0.01309295]\n",
      "  ...\n",
      "  [ 0.15231846 -0.09140155 -0.04400397  0.13401577  0.05537358\n",
      "    0.1083131 ]\n",
      "  [ 0.05493654 -0.05631565  0.06448226  0.03003671 -0.03381815\n",
      "   -0.20454642]\n",
      "  [-0.02573016 -0.04879984  0.07133649 -0.05633134 -0.0750716\n",
      "   -0.02148213]]\n",
      "\n",
      " [[-0.12695628  0.01386457  0.04659856 -0.01238302  0.14942427\n",
      "    0.06496427]\n",
      "  [-0.08160642  0.05387551  0.06533185  0.03375872 -0.05284474\n",
      "   -0.03361593]\n",
      "  [-0.14552586 -0.0917469  -0.01051962  0.08451021  0.11369187\n",
      "    0.01078599]\n",
      "  ...\n",
      "  [ 0.02522668  0.0062766  -0.10254629 -0.06719357  0.04975696\n",
      "   -0.12523745]\n",
      "  [ 0.00046304  0.05157395  0.11419207 -0.01862753 -0.10510913\n",
      "   -0.2524298 ]\n",
      "  [ 0.01254023 -0.048389    0.03970297 -0.08745316 -0.06572326\n",
      "   -0.04814808]]] <class 'numpy.ndarray'>\n",
      "{'eval_loss': 382.0308532714844, 'eval_accuracy_metric': 0.14761904761904762, 'eval_precision_metric': 0.14761904761904762, 'eval_recall_metric': 0.14761904761904762, 'eval_f1_metric': 0.14761904761904762, 'eval_matthews_correlation': 0.0, 'eval_confusion_matrix': array([[31,  0,  0,  0,  0,  0],\n",
      "       [26,  0,  0,  0,  0,  0],\n",
      "       [39,  0,  0,  0,  0,  0],\n",
      "       [35,  0,  0,  0,  0,  0],\n",
      "       [41,  0,  0,  0,  0,  0],\n",
      "       [38,  0,  0,  0,  0,  0]]), 'eval_runtime': 41.76, 'eval_samples_per_second': 0.072, 'eval_steps_per_second': 0.024, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "metrics=trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CRF.decode of CRF(num_tags=6)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_lora_model.crf.modules_to_save.default.decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy_metric</th>\n",
       "      <th>eval_precision_metric</th>\n",
       "      <th>eval_recall_metric</th>\n",
       "      <th>eval_f1_metric</th>\n",
       "      <th>eval_matthews_correlation</th>\n",
       "      <th>eval_confusion_matrix</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1133.2013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>83.5951</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.012</td>\n",
       "      <td>4.646633e+12</td>\n",
       "      <td>1133.201294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>382.030853</td>\n",
       "      <td>0.147619</td>\n",
       "      <td>0.147619</td>\n",
       "      <td>0.147619</td>\n",
       "      <td>0.147619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[31, 0, 0, 0, 0, 0], [26, 0, 0, 0, 0, 0], [39...</td>\n",
       "      <td>31.3562</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>382.030853</td>\n",
       "      <td>0.147619</td>\n",
       "      <td>0.147619</td>\n",
       "      <td>0.147619</td>\n",
       "      <td>0.147619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[31, 0, 0, 0, 0, 0], [26, 0, 0, 0, 0, 0], [39...</td>\n",
       "      <td>32.5586</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>382.030853</td>\n",
       "      <td>0.147619</td>\n",
       "      <td>0.147619</td>\n",
       "      <td>0.147619</td>\n",
       "      <td>0.147619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[31, 0, 0, 0, 0, 0], [26, 0, 0, 0, 0, 0], [39...</td>\n",
       "      <td>41.7600</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  learning_rate  epoch  step  train_runtime  \\\n",
       "0  1133.2013            0.0    1.0     1            NaN   \n",
       "1        NaN            NaN    1.0     1        83.5951   \n",
       "2        NaN            NaN    1.0     1            NaN   \n",
       "3        NaN            NaN    1.0     1            NaN   \n",
       "4        NaN            NaN    1.0     1            NaN   \n",
       "\n",
       "   train_samples_per_second  train_steps_per_second    total_flos  \\\n",
       "0                       NaN                     NaN           NaN   \n",
       "1                     0.108                   0.012  4.646633e+12   \n",
       "2                       NaN                     NaN           NaN   \n",
       "3                       NaN                     NaN           NaN   \n",
       "4                       NaN                     NaN           NaN   \n",
       "\n",
       "    train_loss   eval_loss  eval_accuracy_metric  eval_precision_metric  \\\n",
       "0          NaN         NaN                   NaN                    NaN   \n",
       "1  1133.201294         NaN                   NaN                    NaN   \n",
       "2          NaN  382.030853              0.147619               0.147619   \n",
       "3          NaN  382.030853              0.147619               0.147619   \n",
       "4          NaN  382.030853              0.147619               0.147619   \n",
       "\n",
       "   eval_recall_metric  eval_f1_metric  eval_matthews_correlation  \\\n",
       "0                 NaN             NaN                        NaN   \n",
       "1                 NaN             NaN                        NaN   \n",
       "2            0.147619        0.147619                        0.0   \n",
       "3            0.147619        0.147619                        0.0   \n",
       "4            0.147619        0.147619                        0.0   \n",
       "\n",
       "                               eval_confusion_matrix  eval_runtime  \\\n",
       "0                                                NaN           NaN   \n",
       "1                                                NaN           NaN   \n",
       "2  [[31, 0, 0, 0, 0, 0], [26, 0, 0, 0, 0, 0], [39...       31.3562   \n",
       "3  [[31, 0, 0, 0, 0, 0], [26, 0, 0, 0, 0, 0], [39...       32.5586   \n",
       "4  [[31, 0, 0, 0, 0, 0], [26, 0, 0, 0, 0, 0], [39...       41.7600   \n",
       "\n",
       "   eval_samples_per_second  eval_steps_per_second  \n",
       "0                      NaN                    NaN  \n",
       "1                      NaN                    NaN  \n",
       "2                    0.096                  0.032  \n",
       "3                    0.092                  0.031  \n",
       "4                    0.072                  0.024  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_log = pd.DataFrame(trainer.state.log_history)\n",
    "display(training_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_location = '/models/testing_1'\n",
    "training_log['eval_confusion_matrix'] = training_log['eval_confusion_matrix'].apply(lambda x: x.tolist() if type(x)==np.ndarray else None)\n",
    "t5_lora_model.save_pretrained(ROOT + adapter_location)\n",
    "training_log.to_csv(ROOT + adapter_location + '/training_log.csv', index=False)\n",
    "training_log.to_parquet(ROOT + adapter_location + '/training_log.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iput IDs:\t M A A V I L E R L G A L W V Q N L R G K L A L G I L P Q S H I H T S A S L E I S R K W E K K N K I V Y P P Q L P G E P R R P A E I Y H C R R</s>\n",
      "Labels:\t\t 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -100\n",
      "Labels Decoded:\t I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I\n",
      "Attention Mask:\t 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "----\n",
      "Result: \t M S S O O L T M S O O O M T S S O O O M M S S O L T T M M M M M S S S S S S S S O O O O O M S I O M I T T I T I S I I O O O O O M S S O O O M\n"
     ]
    }
   ],
   "source": [
    "_ds_index = 2\n",
    "_ds_type = 'test'\n",
    "\n",
    "_input_ids_test = t5_tokenizer.decode(dataset_signalp[_ds_type][_ds_index]['input_ids'])\n",
    "_labels_test = torch.tensor([dataset_signalp[_ds_type][_ds_index]['labels'] + [-100]]).to(device)\n",
    "_attention_mask_test = torch.tensor([dataset_signalp[_ds_type][_ds_index]['attention_mask']]).to(device)\n",
    "\n",
    "_labels_test_decoded = [src.config.label_decoding[x] for x in _labels_test.tolist()[0][:-1]]\n",
    "\n",
    "print('Iput IDs:\\t', _input_ids_test)\n",
    "print('Labels:\\t\\t', *_labels_test.tolist()[0])\n",
    "print('Labels Decoded:\\t', *_labels_test_decoded)\n",
    "print('Attention Mask:\\t', *_attention_mask_test.tolist()[0])\n",
    "print('----')\n",
    "\n",
    "viterbi_decoding=True\n",
    "\n",
    "preds = src.model_new.predict_model(\n",
    "    sequence=_input_ids_test,\n",
    "    tokenizer=t5_tokenizer,\n",
    "    model=t5_lora_model,\n",
    "    labels=_labels_test,\n",
    "    attention_mask=_attention_mask_test,\n",
    "    device=device,\n",
    "    viterbi_decoding=viterbi_decoding,\n",
    "    )\n",
    "\n",
    "_result = src.model_new.translate_logits(\n",
    "    logits=preds.logits,\n",
    "    viterbi_decoding=viterbi_decoding,\n",
    "    )\n",
    "\n",
    "print('Result: \\t',* _result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_printoptions(threshold=10_000)\n",
    "# t5_lora_model.custom_classifier.modules_to_save.default.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
